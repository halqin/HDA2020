---
title: "Analysis of High Dimensional Data - Lab 5"
subtitle: "Large Scale Inference"
author: "Adapted by Milan Malfait"
date: "3 Dec 2020"
references:
- id: alon1999broad
  type: article-journal
  author:
  - family: Alon
    given: Uri
  - family: Barkai
    given: Naama
  - family: Notterman
    given: Daniel A
  - family: Gish
    given: Kurt
  - family: Ybarra
    given: Suzanne
  - family: Mack
    given: Daniel
  - family: Levine
    given: Arnold J
  issued:
  - year: 1999
  title: Broad patterns of gene expression revealed by clustering analysis of tumor
    and normal colon tissues probed by oligonucleotide arrays
  container-title: Proceedings of the National Academy of Sciences
  publisher: National Acad Sciences
  page: 6745-6750
  volume: '96'
  issue: '12'
---

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center",
  fig.width = 8,
  fig.asp = 0.618,
  out.width = "100%"
)
```

***

```{r libraries, warning=FALSE, message=FALSE}
## install packages with:
## install.packages("locfdr")
library(locfdr)
```


# Introduction

**In this lab session we will look at the following topics**

  - .......................................................
  
## The dataset {-}

In this practical session, we use the same dataset as in
[Lab 4](./Lab4-Sparse-PCA-LDA.html) by @alon1999broad on
gene expression levels in 40 tumour and 22 normal colon tissue samples.
They checked a total of 6500 human genes using the Affymetrix oligonucleotide array.
You can read the data in as follows:

```{r load-data}
Alon1999 <- read.csv("https://github.com/statOmics/HDA2020/raw/data/Alon1999.csv")
str(Alon1999[, 1:10])
table(Alon1999$Y)
```

The dataset contains one variable named `Y` with the values `t` and `n`.
This variable indicates whether the sample came from tumourous (`t`) or
normal (`n`) tissue.

The goal of this practical is to ...................................


## Testing many hypotheses

To demonstrate the ideas we will be working with, we will simulate artificial data.
Note that since we are doing simulations, we can control everything and also know
exactly what the underlying "true" distribution is (sometimes also called the "ground truth").
However, keep in mind that this is an unlikely representation of real-world data.

In particular, we will simulate data for multiple hypothesis tests where
__the null hypothesis is always true__. I.e. for the $i$th test, we assume that
$\mu_{1i}$ and $\mu_{2i}$ represent the means of the two populations of interest,
the null hypothesis for comparing the two means is
$H_{0i} : \mu_{1i} = \mu_{2i}$. Let the alternative hypothesis of this test be
$H_{1i} : \mu_{1i} \neq \mu_{2i}$, i.e. we perform a *two-tailed* test.
Suppose we have data collected from both populations, given by $X_{1i}$ and $X_{2i}$
of size $n_{1i}$ and $n_{2i}$, respectively, and assume that both populations
have the same knwon variance $\sigma_i^2$. Then we can test this hypothesis by using
__Z-scores__, given by

$$ Z_i=\frac{\bar X_{1i}-\bar X_{2i}}{\sigma_i\sqrt{1/n_{1i}+1/n_{2i}}} $$

Under the null hypothesis, the scores will be distributed as a *standard normal*
$Z_i\sim N(0,1)$. The null hypothesis is rejected in favor of the alternative if
$z_i < \phi_{\alpha_i/2}$ or $z_i > \phi_{1-\alpha_i/2}$, or equivalently if
$|z_i| > \phi_{1-\alpha_i/2}$, where $\phi_{\alpha_i/2}$ is the $\alpha_i/2$th
quantile of the standard normal distribution.

In a multiple hypothesis testing setting, we will perform $m$ tests using the
same test statistic. If the *null* were true for all hypotheses, we woul end up
with a sample $z_1,\ldots,z_m$ from a standard normal distribution.

In this setting, we can only make one type of error: wrongly rejecting the null
hypothesis, i.e. a __type 1 error__. The probability of making this error is given
by

$$ \alpha_i=\text{P}\{\text{ reject } H_{0i} | H_{0i}\} $$

(the $| H_{0i}$ part should be read as "given that the null hypothesis is true").

If we now perform $m_0$ such tests (using the 0 subscript to denote that they
are all hypotheses for which the *null* is true), we can summarise the possible
outcomes as in the table below:

$$
\begin{table}
   \begin{tabular}{ r|l|l|l }
      & Accept $H_{0}$ & Reject $H_{0}$ & Total \\
      & (True Negative) & (False Positive) \\
      \hline
	  Null True & U & V & $m_0$
   \end{tabular}
\end{table}
$$

Here, $U$ and $V$ represent the total number of true negative and false positive
results we get, respectively, out of $m_0$ total tests.

As an example, let's simulate the scenario above. In the code below, the `replicate`
function is used to repeat the same procedure a number of times.
Essentially, the following steps are performed :

1. Sample $N = 12$ observations for 2 hypothetical groups from normal distributions 
   with the same mean and known variance
2. Calculate Z-scores based on the 2 group means
3. Repeat steps 1 to 2 `m0` times

This mimics performing `m0` hypothesis tests on data for which we know the null
hypothesis is true.

For simplicity we assume the variance to be known and equal to 1 for both groups.
We simulate 10 observations for each group and calculate the denominator for the
Z-scores since it's the same for each test.

```{r}
## Set parameters for the simulation
N <- 10 # samples per group
m0 <- 1000 # number of hypothesis tests
mu_1 <- 3 # true mean group 1
mu_2 <- 3 # true mean group 2
sigma <- 1 # known variance
denom <- sigma * sqrt(2 / N) # denominator for Z-scores

set.seed(123) # seed for reproducibility
null_z_scores <- replicate(m0, {
  group1 <- rnorm(N, mean = mu_1, sd = sqrt(sigma))
  group2 <- rnorm(N, mean = mu_2, sd = sqrt(sigma))

  ## Calculate Z-score
  (mean(group2) - mean(group1)) / denom
})

## Visuzlize Z-scores
hist(null_z_scores, breaks = 50, freq = FALSE)
## Overlay theoretical standard normal
lines(x <- seq(-5, 5, length.out = 100), dnorm(x), col = "dodgerblue", lwd = 3)
## Draw vertical lines at 5 and 95th percentiles
abline(v = qnorm(c(0.05, 0.95)), col = "firebrick", lty = 2, lwd = 3)
```

We see that the Z-scores are nicely distributed as a standard normal.
The vertical dashed lines indicate the 5th and 95th percentiles of the standard
normal. The regions outside these lines indicate the Z-scores that we would call
significant if we used a cut-off of $\alpha = 0.05$. So, even though we simulated
data under the null hypothesis, our Z-test still returns "significant" results for
a number of cases just by chance!

Let's calculate the p-values for our hypothesis tests and see what the damage is.
To calculate the p-values, we use the `pnorm()` function in this case, which returns
the value of the standard normal CDF (i.e. `pnorm(x)` = $P(Z < x)$). Since we consider
a two-tailed test, we take the absolute values of the Z-scores and set the `lower.tail`
argument in `pnorm` to `FALSE` (by default it's `TRUE`), so that we get 
`pnorm(abs(x), lower.tail = FALSE)` = $P(Z > |x|)$ and multiply this value by 2.

```{r}
null_pvals <- 2 * pnorm(abs(null_z_scores), lower.tail = FALSE)

alpha <- 0.05 # significance cutoff

hist(null_pvals, breaks = seq(0, 1, by = 0.05))
abline(v = alpha, col = "firebrick", lwd = 3)

called <- (null_pvals < alpha)

## V = number of false positives, in this case: all significant tests
(V <- sum(called))
mean(called) # V / m0
```

So we get 48 significant tests (false positives) out of a total of 1000, which is, 
unsurprisingly, approximately equal to our significance cutoff $\alpha$.
Note also that the p-values are uniformly distibuted under the null hypothesis.

If we had carried out only a few tests (say 10) it would be very unlikely to observe
a false positive (on average: 0.05 * 10 = 0.5 false positives) at $\alpha = 0.05$,
but since now we're carrying out so many, we're almost guaranteed to get false positives.
This is what is known as the __multiple hypothesis testing problem__.

Note that in real-world data the quantities $U$ and $V$ are *unknown* (because
we don't know the truth! If we did, we wouldn't have to carry out any hypothesis
tests in the first place). However, by using simulated data, we do know the truth
and so we can explore these quantities.


## The family-wise error rate

If we carry out many tests, we're almost guaranteed to get type I errors, just by
chance. Therefore the type I error rate is no longer a relevant metric.
Instead, we consider the __Familywise Error Rate (FWER)__, given by

$$
\text{FWER}=\text{P}\{V > 0\}
  = \text{P}\{\text{rejecting at least one } H_{0i}| H_0\}
$$

where $H_0$ is the intersection of all  partial nulls ($H_{0i}$) $i=1,\ldots,m_0$.
In general, we would prefer testing procedures that keep the FWER under control,
i.e. correct for the multiplicity problem.
So instead of choosing $\alpha$ to control the probability of getting a false positive
in each test, we try to control the FWER, i.e. the probability of getting
*at least* one false positive in our *set* of hypothesis tests.

In the following exercises, we will explore this idea on some synthetic data.



# Exercises: FWER null data simulation {-}

We will simulate performing hypothesis tests for which the null distribution is
always true for different values of `m0`.

To make the code more simple, we will directly sample the Z-score statistics from
a standard normal (instead of sampling the individual groups and then performing
the test on them, as shown above). We can also skip the p-value calculation and
compare our Z-scores directly to the $(1 - \alpha) / 2$th quantile of the standard
normal distribution (using the `qnorm()` function).

#### 1. Set `m0 = 5` and generate `m0` Z-score statistics by sampling from the standard normal distribution. Check which ones are significant using a cutoff `alpha = 0.05` {-}

To check significance you can use `abs(z_score) > qnorm(1 - alpha / 2)`


#### 2. Calculate $V$, the number of significant tests. {-}

Note that to count a logical vector, you can just use `sum` on it, since `TRUE`
is numerically interpreted as 1 and `FALSE` as 0.


#### 3. Check that `V > 0` (at least one significant) and store the result in a variable (e.g. `at_least_one_signif <- V > 0`). {-}


#### 4. Repeat steps 1-3 1000 times and keep track of the result in step 3 by storing it in a vector. {-}

You can either use a `for` loop or `replicate`. If using a `for` loop, make sure to
create an empty vector of the desired length first! Growing a vector is technically
possible but is very inefficient. You can initialize an empty (logical) vector of 
a certain length with `vec <- logical(length = x)`


#### 5. Now compute the FWER as the proportion of times $V$ was greater than 0. {-}


#### 6. Repeat the same procedure for `m0 = 50` and `m0 = 1000`. Interpret the results. {-}


## Solutions {-}

<details><summary>Solution</summary>

I combined the exercises above into one code block, to avoid repeated code.
Essentially, I wrapped the simulation in a `for` loop that goes over the different
values of `m0`.

```{r}
## Repeat 1000 times for m0 = 5, 50 and 1000, directly simulate test statistic
## record minimal p-value and check if smaller than alpha
set.seed(1)
B <- 1000 # number of simulations
m0_vec <- c(5, 50, 1000)
alpha <- 0.05
## Initialize empty vector to store FWER results
fwer <- numeric(length = length(m0_vec))
names(fwer) <- paste("m0 =", m0_vec)

## Loop over m0 values and simulate test statistics
for (i in seq_along(m0_vec)) {
  ## Simulate B times, each time recording V > 0
  at_least_one_signif <- replicate(B, {
    null_tests <- rnorm(m0_vec[i], mean = 0, sd = 1) # z-score distribution
    signif <- abs(null_tests) > qnorm(1 - alpha / 2)
    sum(signif) > 0
  })
  fwer[i] <- mean(at_least_one_signif) # proportion of V >= 1
}
fwer
```

</details>



# Controlling the FWER

We saw in the previous exercises that when conducting a large number of hypothesis
tests the FWER becomes unacceptably large. So what can we do to lower it?
The most straightforward way would be to just lower $\alpha$ (so we get less false positives).

Note that we can write the FWER under the null hypothesis (so the p-values are uniformly distributed) as

$$
\text{FWER} = P(\text{at least one rejection}) &= 1 - P(\text{no rejections}) \\
  &= 1 - \prod_{i=1}^m_0 P(p_i > \alpha) \\
  &= 1 - \prod_{i=1}^m_0 (1 - \alpha) \\
  &= 1 - (1 - \alpha)^m_0
$$

where $p_i$ is the p-value for the $i$th test.

If we wanted to control the FWER at a certain level (e.g. 0.05), we can simply solve
the equation above for $\alpha$ and get, for a given FWER:

$$ \alpha = 1 - (1 - \text{FWER})^{1/m_0} $$


## Exercise

Using your simulation code from before, confirm that when setting; `alpha = 0.0102`
and `m0 = 5`, `alpha = 0.00102` and `m0 = 50`, and finally `alpha = 0.000051292`
and `m0 = 1000` all result in an FWER of approximately 0.05.

<details><summary>Solution</summary>

```{r}
## Repeat 1000 times for m0 = 5, 50 and 1000, directly simulate test statistic
## record minimal p-value and check if smaller than alpha
set.seed(1)
B <- 1000 # number of simulations
m0_vec <- c(5, 50, 1000)
alpha_vec <- c(0.0102, 0.00102, 0.000051292)
## Initialize empty vector to store FWER results
fwer <- numeric(length = length(m0_vec))
names(fwer) <- paste("m0 =", m0_vec)

## Loop over m0 values and simulate test statistics
for (i in seq_along(m0_vec)) {
  ## Simulate B times, each time recording V > 0
  at_least_one_signif <- replicate(B, {
    null_tests <- rnorm(m0_vec[i], mean = 0, sd = 1) # z-score distribution
    signif <- abs(null_tests) > qnorm(1 - alpha_vec[i] / 2)
    sum(signif) > 0
  })
  fwer[i] <- mean(at_least_one_signif) # proportion of V >= 1
}
fwer
```

</details>

The procedure described above is also known as Sidak's procedure.
Another widely used technique to control the FWER is the Bonferroni correction.
Note, however that in general, controlling the FWER at levels such as 0.05 is very
conservative, i.e. if there were ture alternative cases we might miss them.


# Simulating data under a mixture of null and alternative hypotheses



# Exercises: mixed data simulation


# Exercises: real data (Alon *et al.* (1999))



# References {-}



# Session info {-}

<details><summary>Session info</summary>

```{r session_info, echo=FALSE, cache=FALSE}
Sys.time()
sessioninfo::session_info()
```

</details>

# [Home](https://statomics.github.io/HDA2020/) {-}
