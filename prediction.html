<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Lieven Clement" />


<title>3. Prediction with High Dimensional Predictors</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<script src="site_libs/navigation-1.1/sourceembed.js"></script>
<script src="site_libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
#rmd-source-code {
  display: none;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">HDA2020</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-chalkboard-teacher"></span>
     
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="intro.html">1. Introduction</a>
    </li>
    <li>
      <a href="svd.html">2. Singular Value Decomposition</a>
    </li>
    <li>
      <a href="prediction.html">3. Prediction with High Dimensional Predictors</a>
    </li>
    <li>
      <a href="sparseSvd.html">4. Sparse Singular Value Decomposition</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-laptop"></span>
     
    Practicals
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Lab1-Intro-SVD.html">Lab 1</a>
    </li>
    <li>
      <a href="Lab2-PCA.html">Lab 2</a>
    </li>
    <li>
      <a href="Lab3-Penalized-Regression.html">Lab 3</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/statOmics/HDA2020">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="http://statomics.github.io/">statOmics</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-download-source" href="#">Download Rmd</a></li>
</ul>
</div>



<h1 class="title toc-ignore">3. Prediction with High Dimensional Predictors</h1>
<h4 class="author">Lieven Clement</h4>
<h4 class="date">statOmics, Ghent University (<a href="https://statomics.github.io" class="uri">https://statomics.github.io</a>)</h4>

</div>


<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<div id="prediction-with-high-dimensional-predictors" class="section level2">
<h2><span class="header-section-number">1.1</span> Prediction with High Dimensional Predictors</h2>
<p>General setting:</p>
<ul>
<li><p>Aim: build a <strong>prediction model</strong> that gives a prediction of an outcome for a given set of predictors.</p></li>
<li><p>We use <span class="math inline">\(X\)</span> to refer to the predictors and <span class="math inline">\(Y\)</span> to refer to the outcome.</p></li>
<li><p>A <strong>training data set</strong> is available, say <span class="math inline">\((\mathbf{X},\mathbf{Y})\)</span>. It contains <span class="math inline">\(n\)</span> observations on outcomes and on <span class="math inline">\(p\)</span> predictors.</p></li>
<li><p>Using the training data, a prediction model is build, say <span class="math inline">\(\hat{m}(\mathbf{X})\)</span>. This typically involves <strong>model building (feature selection)</strong> and parameter estimation.</p></li>
<li><p>During the model building, potential <strong>models need to be evaluated</strong> in terms of their prediction quality.</p></li>
</ul>
</div>
<div id="example-toxicogenomics-in-early-drug-development" class="section level2">
<h2><span class="header-section-number">1.2</span> Example: Toxicogenomics in early drug development</h2>
<div id="background" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Background</h3>
<ul>
<li><p>Effect of compound on gene expression.</p></li>
<li><p>Insight in action and toxicity of drug in early phase</p></li>
<li><p>Determine activity with bio-assay: e.g. binding affinity of compound to cell wall receptor (target, IC50).</p></li>
<li><p>Early phase: 20 to 50 compounds</p></li>
<li><p>Based on in vitro results one aims to get insight in how to build better compound (higher on-target activity less toxicity.</p></li>
<li><p>Small variations in molecular structure lead to variations in BA and gene expression.</p></li>
<li><p>Aim: Build model to predict bio-activity based on gene expression in liver cell line.</p></li>
</ul>
</div>
<div id="data" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Data</h3>
<ul>
<li><p>30 chemical compounds have been screened for toxicity</p></li>
<li><p>Bioassay data on toxicity screening</p></li>
<li><p>Gene expressions in a liver cell line are profiled for each compound (4000 genes)</p></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>toxData &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/statOmics/HDA2020/data/toxDataCentered.csv&quot;</span>)</span></code></pre></div>
<pre><code>## 
## [36m──[39m [1m[1mColumn specification[1m[22m [36m────────────────────────────────────────────────────────[39m
## cols(
##   .default = col_double()
## )
## [36mℹ[39m Use [30m[47m[30m[47m`spec()`[47m[30m[49m[39m for the full column specifications.</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>svdX &lt;-<span class="st"> </span><span class="kw">svd</span>(toxData[,<span class="op">-</span><span class="dv">1</span>])</span></code></pre></div>
<p>Data is already centered:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>toxData <span class="op">%&gt;%</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="st">  </span>colMeans <span class="op">%&gt;%</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="st">  </span>range</span></code></pre></div>
<pre><code>## [1] -1.434038e-17  2.567391e-17</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a> toxData <span class="op">%&gt;%</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="st">  </span>names <span class="op">%&gt;%</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="st">  </span>head</span></code></pre></div>
<pre><code>## [1] &quot;BA&quot; &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; &quot;X4&quot; &quot;X5&quot;</code></pre>
<ul>
<li>First column contains data on Bioassay.</li>
<li>The higher the score on Bioassay the more toxic the compound</li>
<li>Other columns contain data on gene expression X1, … , X4000</li>
</ul>
</div>
<div id="data-exploration" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Data exploration</h3>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>toxData <span class="op">%&gt;%</span></span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span><span class="st">&quot;&quot;</span>,<span class="dt">y=</span>BA)) <span class="op">+</span></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="st">  </span><span class="kw">geom_boxplot</span>(<span class="dt">outlier.shape=</span><span class="ot">NA</span>) <span class="op">+</span></span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">position=</span><span class="st">&quot;jitter&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>svdX &lt;-<span class="st"> </span>toxData[,<span class="op">-</span><span class="dv">1</span>] <span class="op">%&gt;%</span></span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="st">  </span>svd</span>
<span id="cb9-3"><a href="#cb9-3"></a></span>
<span id="cb9-4"><a href="#cb9-4"></a>k &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>Vk &lt;-<span class="st"> </span>svdX<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>k]</span>
<span id="cb9-6"><a href="#cb9-6"></a>Uk &lt;-<span class="st"> </span>svdX<span class="op">$</span>u[,<span class="dv">1</span><span class="op">:</span>k]</span>
<span id="cb9-7"><a href="#cb9-7"></a>Dk &lt;-<span class="st"> </span><span class="kw">diag</span>(svdX<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span>k])</span>
<span id="cb9-8"><a href="#cb9-8"></a>Zk &lt;-<span class="st"> </span>Uk<span class="op">%*%</span>Dk</span>
<span id="cb9-9"><a href="#cb9-9"></a><span class="kw">colnames</span>(Zk) &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;Z&quot;</span>,<span class="dv">1</span><span class="op">:</span>k)</span>
<span id="cb9-10"><a href="#cb9-10"></a><span class="kw">colnames</span>(Vk) &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;V&quot;</span>,<span class="dv">1</span><span class="op">:</span>k)</span>
<span id="cb9-11"><a href="#cb9-11"></a></span>
<span id="cb9-12"><a href="#cb9-12"></a>Zk <span class="op">%&gt;%</span></span>
<span id="cb9-13"><a href="#cb9-13"></a><span class="st">  </span>as.data.frame <span class="op">%&gt;%</span></span>
<span id="cb9-14"><a href="#cb9-14"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">BA =</span> toxData <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(BA)) <span class="op">%&gt;%</span></span>
<span id="cb9-15"><a href="#cb9-15"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span> Z1, <span class="dt">y =</span> Z2, <span class="dt">color =</span> BA)) <span class="op">+</span></span>
<span id="cb9-16"><a href="#cb9-16"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">3</span>) <span class="op">+</span></span>
<span id="cb9-17"><a href="#cb9-17"></a><span class="st">  </span><span class="kw">scale_colour_gradient2</span>(<span class="dt">low =</span> <span class="st">&quot;blue&quot;</span>,<span class="dt">mid=</span><span class="st">&quot;white&quot;</span>,<span class="dt">high=</span><span class="st">&quot;red&quot;</span>) <span class="op">+</span></span>
<span id="cb9-18"><a href="#cb9-18"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">3</span>, <span class="dt">pch =</span> <span class="dv">21</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<ul>
<li>Scores on the first two principal components (or MDS plot).</li>
<li>Each point corresponds to a compound.</li>
<li>Color code refers to the toxicity score (higher score more toxic).</li>
<li>Clear separation between compounds according to toxicity.</li>
</ul>
<hr />
<ul>
<li>Next logic step in a PCA is to interpret the principal components.</li>
<li>We thus have to assess the loadings.</li>
<li>We can add a vector for each gene to get a biplot, but this would require plotting 4000 vectors, which would render the plot unreadable.</li>
</ul>
<p>Alternative graph to look at the many loadings of the first two PCs.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">grid.arrange</span>(</span>
<span id="cb10-2"><a href="#cb10-2"></a>  Vk <span class="op">%&gt;%</span></span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="st">    </span>as.data.frame <span class="op">%&gt;%</span></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">geneID =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Vk)) <span class="op">%&gt;%</span></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> geneID, <span class="dt">y =</span> V1)) <span class="op">+</span></span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">pch=</span><span class="dv">21</span>) <span class="op">+</span></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="st">    </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">2</span>)<span class="op">*</span><span class="kw">sd</span>(Vk[,<span class="dv">1</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>) ,</span>
<span id="cb10-8"><a href="#cb10-8"></a>  Vk <span class="op">%&gt;%</span></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="st">    </span>as.data.frame <span class="op">%&gt;%</span></span>
<span id="cb10-10"><a href="#cb10-10"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">geneID =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Vk)) <span class="op">%&gt;%</span></span>
<span id="cb10-11"><a href="#cb10-11"></a><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> geneID, <span class="dt">y =</span> V2)) <span class="op">+</span></span>
<span id="cb10-12"><a href="#cb10-12"></a><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">pch=</span><span class="dv">21</span>) <span class="op">+</span></span>
<span id="cb10-13"><a href="#cb10-13"></a><span class="st">    </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">2</span>)<span class="op">*</span><span class="kw">sd</span>(Vk[,<span class="dv">2</span>]), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>),</span>
<span id="cb10-14"><a href="#cb10-14"></a>  <span class="dt">ncol=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<ul>
<li><p>It is almost impossible to interpret the PCs because there are 4000 genes contributing to each PC.</p></li>
<li><p>In an attempt to find the most important genes (in the sense that they drive the interpretation of the PCs), the plots show horizontal reference lines: the average of the loadings, and the average ± twice the standard deviation of the loadings. In between the lines we expects about 95% of the loadings (if they were normally distributed).</p></li>
<li><p>The points outside the band come from the genes that have rather large loadings (in absolute value) and hence are important for the interpretation of the PCs.</p></li>
<li><p>Note, that particularly for the first PC, only a few genes show a markedly large loadings that are negative. This means that an upregulation of these genes will lead to low scores on PC1.</p></li>
<li><p>These genes will very likely play an important role in the toxicity mechanism.</p></li>
<li><p>Indeed, low scores on PC1 are in the direction of more toxicity.</p></li>
<li><p>In the next chapter we will introduce a method to obtain sparse PCs.</p></li>
</ul>
</div>
<div id="prediction-model" class="section level3">
<h3><span class="header-section-number">1.2.4</span> Prediction model</h3>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a>m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(BA <span class="op">~</span><span class="st"> </span><span class="dv">-1</span> <span class="op">+</span><span class="st"> </span>., toxData)</span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a>m1 <span class="op">%&gt;%</span></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="st">  </span>coef <span class="op">%&gt;%</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="st">  </span><span class="kw">head</span>(<span class="dv">40</span>)</span></code></pre></div>
<pre><code>##          X1          X2          X3          X4          X5          X6 
##  -7.4569940   0.3571348  11.2492315  10.8354021 -13.7433891   5.6833874 
##          X7          X8          X9         X10         X11         X12 
##  65.5387777   4.3404555   7.9103924  37.0296057 -54.8368698 -55.5547845 
##         X13         X14         X15         X16         X17         X18 
##   5.7924667  23.1428002  -6.9610365 -28.5250571 -22.5509025 -97.9623731 
##         X19         X20         X21         X22         X23         X24 
## -30.4171782 -32.6991673 -14.2808834 -16.1431266 -22.7498681  73.1635178 
##         X25         X26         X27         X28         X29         X30 
##  -5.7065827  37.4745379 -20.1999102  14.9906821  99.6080955          NA 
##         X31         X32         X33         X34         X35         X36 
##          NA          NA          NA          NA          NA          NA 
##         X37         X38         X39         X40 
##          NA          NA          NA          NA</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>m1 <span class="op">%&gt;%</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="st">  </span>coef <span class="op">%&gt;%</span></span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="st">  </span>is.na <span class="op">%&gt;%</span></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="st">  </span>sum</span></code></pre></div>
<pre><code>## [1] 3971</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="kw">summary</span>(m1)<span class="op">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>Problem??</p>
</div>
</div>
<div id="brain-example" class="section level2">
<h2><span class="header-section-number">1.3</span> Brain example</h2>
<ul>
<li>Courtesy to Solomon Kurz. Statistical rethinking with brms, ggplot2, and the tidyverse version 1.2.0.</li>
</ul>
<p><a href="https://bookdown.org/content/3890/" class="uri">https://bookdown.org/content/3890/</a> <a href="https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse" class="uri">https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse</a></p>
<ul>
<li>Data with brain size and body size for seven species</li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a>brain &lt;-</span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="kw">tibble</span>(<span class="dt">species =</span> <span class="kw">c</span>(<span class="st">&quot;afarensis&quot;</span>, <span class="st">&quot;africanus&quot;</span>, <span class="st">&quot;habilis&quot;</span>, <span class="st">&quot;boisei&quot;</span>, <span class="st">&quot;rudolfensis&quot;</span>, <span class="st">&quot;ergaster&quot;</span>, <span class="st">&quot;sapiens&quot;</span>),</span>
<span id="cb17-3"><a href="#cb17-3"></a>       <span class="dt">brain   =</span> <span class="kw">c</span>(<span class="dv">438</span>, <span class="dv">452</span>, <span class="dv">612</span>, <span class="dv">521</span>, <span class="dv">752</span>, <span class="dv">871</span>, <span class="dv">1350</span>),</span>
<span id="cb17-4"><a href="#cb17-4"></a>       <span class="dt">mass    =</span> <span class="kw">c</span>(<span class="fl">37.0</span>, <span class="fl">35.5</span>, <span class="fl">34.5</span>, <span class="fl">41.5</span>, <span class="fl">55.5</span>, <span class="fl">61.0</span>, <span class="fl">53.5</span>))</span></code></pre></div>
<div id="data-exploration-1" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Data exploration</h3>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a>brain</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["species"],"name":[1],"type":["chr"],"align":["left"]},{"label":["brain"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["mass"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"afarensis","2":"438","3":"37.0"},{"1":"africanus","2":"452","3":"35.5"},{"1":"habilis","2":"612","3":"34.5"},{"1":"boisei","2":"521","3":"41.5"},{"1":"rudolfensis","2":"752","3":"55.5"},{"1":"ergaster","2":"871","3":"61.0"},{"1":"sapiens","2":"1350","3":"53.5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a>p &lt;-<span class="st"> </span>brain <span class="op">%&gt;%</span></span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span>  mass, <span class="dt">y =</span> brain, <span class="dt">label =</span> species)) <span class="op">+</span></span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="st">  </span><span class="kw">geom_point</span>()</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a>p <span class="op">+</span><span class="st"> </span><span class="kw">geom_text</span>(<span class="dt">nudge_y =</span> <span class="dv">40</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="models" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Models</h3>
<p>Six models range in complexity from the simple univariate model</p>
<p><span class="math display">\[\begin{align*}
\text{brain}_i &amp; \sim \operatorname{Normal} (\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_1 \text{mass}_i,
\end{align*}\]</span></p>
<p>to the dizzying sixth-degree polynomial model</p>
<p><span class="math display">\[\begin{align*}
\text{brain}_i &amp; \sim \operatorname{Normal} (\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_1 \text{mass}_i + \beta_2 \text{mass}_i^2 + \beta_3 \text{mass}_i^3 + \beta_4 \text{mass}_i^4 + \beta_5 \text{mass}_i^5 + \beta_6 \text{mass}_i^6.
\end{align*}\]</span></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a>formulas &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="cf">function</span>(i)</span>
<span id="cb20-2"><a href="#cb20-2"></a>  <span class="kw">return</span>(</span>
<span id="cb20-3"><a href="#cb20-3"></a>     <span class="kw">paste0</span>(<span class="st">&quot;I(mass^&quot;</span>,<span class="dv">1</span><span class="op">:</span>i,<span class="st">&quot;)&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">paste</span>(<span class="dt">collapse=</span><span class="st">&quot; + &quot;</span>)</span>
<span id="cb20-4"><a href="#cb20-4"></a>    )  </span>
<span id="cb20-5"><a href="#cb20-5"></a>)</span>
<span id="cb20-6"><a href="#cb20-6"></a></span>
<span id="cb20-7"><a href="#cb20-7"></a>formulas &lt;-<span class="st"> </span><span class="kw">sapply</span>(</span>
<span id="cb20-8"><a href="#cb20-8"></a>  <span class="kw">paste0</span>(<span class="st">&quot;brain ~ &quot;</span>, formulas),</span>
<span id="cb20-9"><a href="#cb20-9"></a>  as.formula)</span>
<span id="cb20-10"><a href="#cb20-10"></a></span>
<span id="cb20-11"><a href="#cb20-11"></a>models &lt;-<span class="st"> </span><span class="kw">lapply</span>(formulas, lm , <span class="dt">data =</span> brain)</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a><span class="kw">data.frame</span>(</span>
<span id="cb21-2"><a href="#cb21-2"></a>  <span class="dt">formula=</span>formulas <span class="op">%&gt;%</span></span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="st">    </span>as.character,</span>
<span id="cb21-4"><a href="#cb21-4"></a>  <span class="dt">r2 =</span> <span class="kw">sapply</span>(</span>
<span id="cb21-5"><a href="#cb21-5"></a>    models,</span>
<span id="cb21-6"><a href="#cb21-6"></a>    <span class="cf">function</span>(mod) <span class="kw">summary</span>(mod)<span class="op">$</span>r.squared)</span>
<span id="cb21-7"><a href="#cb21-7"></a>  )  <span class="op">%&gt;%</span></span>
<span id="cb21-8"><a href="#cb21-8"></a><span class="st">  </span><span class="kw">ggplot</span>(</span>
<span id="cb21-9"><a href="#cb21-9"></a>    <span class="kw">aes</span>(<span class="dt">x =</span> r2,</span>
<span id="cb21-10"><a href="#cb21-10"></a>      <span class="dt">y =</span> formula,</span>
<span id="cb21-11"><a href="#cb21-11"></a>      <span class="dt">label =</span> r2 <span class="op">%&gt;%</span></span>
<span id="cb21-12"><a href="#cb21-12"></a><span class="st">        </span><span class="kw">round</span>(<span class="dv">2</span>) <span class="op">%&gt;%</span></span>
<span id="cb21-13"><a href="#cb21-13"></a><span class="st">        </span>as.character)</span>
<span id="cb21-14"><a href="#cb21-14"></a>  ) <span class="op">+</span></span>
<span id="cb21-15"><a href="#cb21-15"></a><span class="st">  </span><span class="kw">geom_text</span>()</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>We plot the fit for each model individually and them arrange them together in one plot.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a>plots &lt;-<span class="st"> </span><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="cf">function</span>(i)</span>
<span id="cb22-2"><a href="#cb22-2"></a>{</span>
<span id="cb22-3"><a href="#cb22-3"></a>  p <span class="op">+</span></span>
<span id="cb22-4"><a href="#cb22-4"></a><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x,i)) <span class="op">+</span></span>
<span id="cb22-5"><a href="#cb22-5"></a><span class="st">  </span><span class="kw">ggtitle</span>(</span>
<span id="cb22-6"><a href="#cb22-6"></a>    <span class="kw">paste0</span>(</span>
<span id="cb22-7"><a href="#cb22-7"></a>      <span class="st">&quot;r2 = &quot;</span>,</span>
<span id="cb22-8"><a href="#cb22-8"></a>      <span class="kw">round</span>(<span class="kw">summary</span>(models[[i]])<span class="op">$</span>r.squared<span class="op">*</span><span class="dv">100</span>,<span class="dv">1</span>),</span>
<span id="cb22-9"><a href="#cb22-9"></a>      <span class="st">&quot;%&quot;</span>)</span>
<span id="cb22-10"><a href="#cb22-10"></a>    )</span>
<span id="cb22-11"><a href="#cb22-11"></a>})</span>
<span id="cb22-12"><a href="#cb22-12"></a></span>
<span id="cb22-13"><a href="#cb22-13"></a><span class="kw">do.call</span>(<span class="st">&quot;grid.arrange&quot;</span>,<span class="kw">c</span>(plots, <span class="dt">ncol =</span> <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## Warning in qt((1 - level)/2, df): NaNs produced</code></pre>
<pre><code>## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning -
## Inf</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<ul>
<li><p>We clearly see that increasing the model complexity always produces a fit with a smaller SSE.</p></li>
<li><p>The problem of overfitting is very obvious. The more complex polynomial models will not generalise well for prediction!</p></li>
<li><p>We even have a model that fits the data perfectly, but that will make very absurd preditions!</p></li>
<li><p>Too few parameters hurts, too. Fit the underfit intercept-only model.</p></li>
</ul>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a>m0 &lt;-<span class="st"> </span><span class="kw">lm</span>(brain <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, brain)</span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="kw">summary</span>(m0)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = brain ~ 1, data = brain)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -275.71 -227.21 -101.71   97.79  636.29 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)    713.7      121.8    5.86  0.00109 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 322.2 on 6 degrees of freedom</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a>p <span class="op">+</span></span>
<span id="cb27-2"><a href="#cb27-2"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="dv">1</span>) <span class="op">+</span></span>
<span id="cb27-3"><a href="#cb27-3"></a><span class="st">  </span><span class="kw">ggtitle</span>(</span>
<span id="cb27-4"><a href="#cb27-4"></a>    <span class="kw">paste0</span>(</span>
<span id="cb27-5"><a href="#cb27-5"></a>      <span class="st">&quot;r2 = &quot;</span>,</span>
<span id="cb27-6"><a href="#cb27-6"></a>      <span class="kw">round</span>(<span class="kw">summary</span>(m0)<span class="op">$</span>r.squared<span class="op">*</span><span class="dv">100</span>,<span class="dv">1</span>),</span>
<span id="cb27-7"><a href="#cb27-7"></a>      <span class="st">&quot;%&quot;</span>)</span>
<span id="cb27-8"><a href="#cb27-8"></a>    )</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>The underfit model did not learn anything about the relation between mass and brain. It would also do a very poor job for predicting new data.</p>
</div>
</div>
<div id="overview" class="section level2">
<h2><span class="header-section-number">1.4</span> Overview</h2>
<p>We will make a distinction between continuous and discrete outcomes. In this course we focus on</p>
<ul>
<li><p>Linear regression models for continous outcomes</p>
<ul>
<li>Penalised regression: Lasso and ridge</li>
<li>Principal component regression (PCR)</li>
</ul></li>
<li><p>Logistic regression models for binary outcomes</p>
<ul>
<li>Penalised regression: Lasso and ridge</li>
</ul></li>
</ul>
<p>For all types of model, we will discuss feature selection methods.</p>
</div>
</div>
<div id="linear-regression-for-high-dimensional-data" class="section level1">
<h1><span class="header-section-number">2</span> Linear Regression for High Dimensional Data</h1>
<p>Consider linear regression model (for double centered data) <span class="math display">\[
  Y_i = \beta_1X_{i1} + \beta_2 X_{i2} + \cdots + \beta_pX_{ip} + \epsilon_i ,
\]</span> with <span class="math inline">\(\text{E}\left[\epsilon \mid \mathbf{X}\right]=0\)</span> and <span class="math inline">\(\text{var}\left[\epsilon \mid \mathbf{X}\right]=\sigma^2\)</span>.</p>
<p>In matrix notation the model becomes <span class="math display">\[
  \mathbf{Y} = \mathbf{X}\mathbf\beta + \mathbf\epsilon.
\]</span> The least squares estimator of <span class="math inline">\(\mathbf\beta\)</span> is given by <span class="math display">\[
  \hat{\mathbf\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} ,
\]</span> and the variance of <span class="math inline">\(\hat{\mathbf\beta}\)</span> equals <span class="math display">\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X}^T\mathbf{X})^{-1}\sigma^2.
\]</span> <span class="math inline">\(\longrightarrow\)</span> the <span class="math inline">\(p \times p\)</span> matrix <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span> is crucial</p>
<p>Note, that</p>
<ul>
<li><p>with double centered data it is meant that both the responses are centered (mean of <span class="math inline">\(\mathbf{Y}\)</span> is zero) and that all predictors are centered (columns of <span class="math inline">\(\mathbf{X}\)</span> have zero mean). With double centered data the intercept in a linear regression model is always exactly equal to zero and hence the intercept must not be included in the model.</p></li>
<li><p>we do not assume that the residuals are normally distributed. For prediction purposes this is often not required (normality is particularly important for statistical inference in small samples).</p></li>
</ul>
<div id="linear-regression-for-multivariate-data-vs-high-dimensional-data" class="section level2">
<h2><span class="header-section-number">2.1</span> Linear Regression for multivariate data vs High Dimensional Data</h2>
<ul>
<li><p><span class="math inline">\(\mathbf{X^TX}\)</span> and <span class="math inline">\((\mathbf{X^TX})^{-1}\)</span> are <span class="math inline">\(p \times p\)</span> matrices</p></li>
<li><p><span class="math inline">\(\mathbf{X^TX}\)</span> can only be inverted if it has rank <span class="math inline">\(p\)</span></p></li>
<li><p>Rank of a matrix of form <span class="math inline">\(\mathbf{X^TX}\)</span>, with <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(n\times p\)</span> matrix, can never be larger than <span class="math inline">\(\min(n,p)\)</span>.</p></li>
<li><p>in most regression problems <span class="math inline">\(n&gt;p\)</span> and rank of <span class="math inline">\((\mathbf{X^TX})\)</span> equals <span class="math inline">\(p\)</span></p></li>
<li><p>in high dimensional regression problems <span class="math inline">\(p &gt;&gt;&gt; n\)</span> and rank of <span class="math inline">\((\mathbf{X^TX})\)</span> equals <span class="math inline">\(n&lt;p\)</span></p></li>
<li><p>in the toxicogenomics example <span class="math inline">\(n=30&lt;p=4000\)</span> and <span class="math inline">\(\text{rank}(\mathbf{X^TX})\leq n=30\)</span>. <span class="math inline">\(\longrightarrow\)</span> <span class="math inline">\((\mathbf{X^TX})^{-1}\)</span> does not exist, and neither does <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</p></li>
</ul>
</div>
<div id="can-svd-help" class="section level2">
<h2><span class="header-section-number">2.2</span> Can SVD help?</h2>
<ul>
<li><p>Since the columns of <span class="math inline">\(\mathbf{X}\)</span> are centered, <span class="math inline">\(\mathbf{X^TX} \propto \text{var}\left[\mathbf{X}\right]\)</span>.</p></li>
<li><p>if <span class="math inline">\(\text{rank}(\mathbf{X^TX})=n=30\)</span>, the PCA will give 30 components, each being a linear combination of <span class="math inline">\(p=4000\)</span> variables. These 30 PCs contain all information present in the original <span class="math inline">\(\mathbf{X}\)</span> data.</p></li>
<li><p>if <span class="math inline">\(\text{rank}(\mathbf{X})=n=30\)</span>, the SVD of <span class="math inline">\(\mathbf{X}\)</span> is given by <span class="math display">\[
   \mathbf{X} = \sum_{i=1}^n \delta_i \mathbf{u}_i \mathbf{v}_i^T = \mathbf{U} \boldsymbol{\Delta} \mathbf{V}^T = \mathbf{ZV}^T,
  \]</span> with <span class="math inline">\(\mathbf{Z}\)</span> the <span class="math inline">\(n\times n\)</span> matrix with the scores on the <span class="math inline">\(n\)</span> PCs.</p></li>
<li><p>Still problematic because if we use all PCs <span class="math inline">\(n=p\)</span>.</p></li>
</ul>
</div>
</div>
<div id="principal-component-regression" class="section level1">
<h1><span class="header-section-number">3</span> Principal Component Regression</h1>
<p>A principal component regression (PCR) consists of</p>
<ol style="list-style-type: decimal">
<li><p>transforming <span class="math inline">\(p=4000\)</span> dimensional <span class="math inline">\(X\)</span>-variable to the <span class="math inline">\(n=30\)</span> dimensional <span class="math inline">\(Z\)</span>-variable (PC scores). The <span class="math inline">\(n\)</span> PCs are mutually uncorrelated.</p></li>
<li><p>using the <span class="math inline">\(n\)</span> PC-variables as regressors in a linear regression model</p></li>
<li><p>performing feature selection to select the most important regressors (PC).</p></li>
</ol>
<p>Feature selection is key, because we don’t want to have as many regressors as there are observations in the data. This would result in zero residual degrees of freedom. (see later)</p>
<hr />
<p>To keep the exposition general so that we allow for a feature selection to have taken place, I use the notation <span class="math inline">\(\mathbf{U}_S\)</span> to denote a matrix with left-singular column vectors <span class="math inline">\(\mathbf{u}_i\)</span>, with <span class="math inline">\(i \in {\cal{S}}\)</span> (<span class="math inline">\({\cal{S}}\)</span> an index set referring to the PCs to be included in the regression model).</p>
<p>For example, suppose that a feature selection method has resulted in the selection of PCs 1, 3 and 12 for inclusion in the prediction model, then <span class="math inline">\({\cal{S}}=\{1,3,12\}\)</span> and <span class="math display">\[
 \mathbf{U}_S = \begin{pmatrix}
  \mathbf{u}_1 &amp; \mathbf{u}_3 &amp; \mathbf{u}_{12}
 \end{pmatrix}.
\]</span></p>
<hr />
<div id="example-model-based-on-first-4-pcs" class="section level3">
<h3><span class="header-section-number">3.0.1</span> Example model based on first 4 PCs</h3>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a>k &lt;-<span class="st"> </span><span class="dv">30</span></span>
<span id="cb28-2"><a href="#cb28-2"></a>Uk &lt;-<span class="st"> </span>svdX<span class="op">$</span>u[,<span class="dv">1</span><span class="op">:</span>k]</span>
<span id="cb28-3"><a href="#cb28-3"></a>Dk &lt;-<span class="st"> </span><span class="kw">diag</span>(svdX<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span>k])</span>
<span id="cb28-4"><a href="#cb28-4"></a>Zk &lt;-<span class="st"> </span>Uk<span class="op">%*%</span>Dk</span>
<span id="cb28-5"><a href="#cb28-5"></a>Y &lt;-<span class="st"> </span>toxData <span class="op">%&gt;%</span></span>
<span id="cb28-6"><a href="#cb28-6"></a><span class="st">  </span><span class="kw">pull</span>(BA)</span>
<span id="cb28-7"><a href="#cb28-7"></a></span>
<span id="cb28-8"><a href="#cb28-8"></a>m4 &lt;-<span class="st"> </span><span class="kw">lm</span>(Y<span class="op">~</span>Zk[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])</span>
<span id="cb28-9"><a href="#cb28-9"></a><span class="kw">summary</span>(m4)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ Zk[, 1:4])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.1438 -0.7033 -0.1222  0.7255  2.2997 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -2.068e-16  2.081e-01   0.000   1.0000    
## Zk[, 1:4]1  -5.275e-01  7.725e-02  -6.828 3.72e-07 ***
## Zk[, 1:4]2  -1.231e-02  8.262e-02  -0.149   0.8828    
## Zk[, 1:4]3  -1.759e-01  8.384e-02  -2.098   0.0461 *  
## Zk[, 1:4]4  -3.491e-02  8.396e-02  -0.416   0.6811    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.14 on 25 degrees of freedom
## Multiple R-squared:  0.672,  Adjusted R-squared:  0.6195 
## F-statistic:  12.8 on 4 and 25 DF,  p-value: 8.352e-06</code></pre>
<p>Note:</p>
<ul>
<li>the intercept is estimated as zero. (Why?) The model could have been fitted as</li>
</ul>
<pre><code>m4 &lt;- lm(Y~-1+Zk[,1:4])</code></pre>
<ul>
<li><p>the PC-predictors are uncorrelated (by construction)</p></li>
<li><p>first PC-predictors are not necessarily the most important predictors</p></li>
<li><p><span class="math inline">\(p\)</span>-values are not very meaningful when prediction is the objective</p></li>
</ul>
<p>Methods for feature selection will be discussed later.</p>
</div>
</div>
<div id="ridge-regression" class="section level1">
<h1><span class="header-section-number">4</span> Ridge Regression</h1>
<div id="penalty" class="section level2">
<h2><span class="header-section-number">4.1</span> Penalty</h2>
<p>The ridge parameter estimator is defined as the parameter <span class="math inline">\(\mathbf\beta\)</span> that minimises the <strong>penalised least squares criterion</strong></p>
<p><span class="math display">\[
 \text{SSE}_\text{pen}=\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 + \lambda \Vert \boldsymbol{\beta} \Vert_2^2
\]</span></p>
<ul>
<li><p><span class="math inline">\(\Vert \boldsymbol{\beta} \Vert_2^2=\sum_{j=1}^p \beta_j^2\)</span> is the <strong><span class="math inline">\(L_2\)</span> penalty term</strong></p></li>
<li><p><span class="math inline">\(\lambda&gt;0\)</span> is the penalty parameter (to be chosen by the user).</p></li>
</ul>
<p>Note, that that is equivalent to minimizing <span class="math display">\[
\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \boldsymbol{\beta}\Vert^2_2\leq s
\]</span></p>
<p>Note, that <span class="math inline">\(s\)</span> has a one-to-one correspondence with <span class="math inline">\(\lambda\)</span></p>
</div>
<div id="graphical-interpretation" class="section level2">
<h2><span class="header-section-number">4.2</span> Graphical interpretation</h2>
<p><img src="prediction_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="solution" class="section level2">
<h2><span class="header-section-number">4.3</span> Solution</h2>
<p>The solution is given by <span class="math display">\[
  \hat{\boldsymbol{\beta}} = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X^T Y}.
\]</span> It can be shown that <span class="math inline">\((\mathbf{X^TX}+\lambda \mathbf{I})\)</span> is always of rank <span class="math inline">\(p\)</span> if <span class="math inline">\(\lambda&gt;0\)</span>.</p>
<p>Hence, <span class="math inline">\((\mathbf{X^TX}+\lambda \mathbf{I})\)</span> is invertible and <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> exists even if <span class="math inline">\(p&gt;&gt;&gt;n\)</span>.</p>
<p>We also find <span class="math display">\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X}^T\mathbf{X} (\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\sigma^2
\]</span></p>
<p>However, it can be shown that improved intervals that also account for the bias can be constructed by using:</p>
<p><span class="math display">\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1}  \sigma^2.
\]</span></p>
<div id="proof" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Proof</h3>
<p>The criterion to be minimised is <span class="math display">\[
   \text{SSE}_\text{pen}=\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 + \lambda \Vert \boldsymbol{\beta} \Vert_2^2.
 \]</span> First we re-express SSE in matrix notation: <span class="math display">\[
   \text{SSE}_\text{pen} = (\mathbf{Y}-\mathbf{X\beta})^T(\mathbf{Y}-\mathbf{X\beta}) + \lambda \boldsymbol{\beta}^T\boldsymbol{\beta}.
 \]</span> The partial derivative w.r.t. <span class="math inline">\(\boldsymbol{\beta}\)</span> is <span class="math display">\[
   \frac{\partial}{\partial \boldsymbol{\beta}}\text{SSE}_\text{pen} = -2\mathbf{X}^T(\mathbf{Y}-\mathbf{X\beta})+2\lambda\boldsymbol{\beta}.
 \]</span> Solving <span class="math inline">\(\frac{\partial}{\partial \boldsymbol{\beta}}\text{SSE}_\text{pen}=0\)</span> gives <span class="math display">\[
   \hat{\boldsymbol{\beta}} = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X^T Y}.
 \]</span> (assumption: <span class="math inline">\((\mathbf{X^TX}+\lambda \mathbf{I})\)</span> is of rank <span class="math inline">\(p\)</span>. This is always true if <span class="math inline">\(\lambda&gt;0\)</span>)</p>
</div>
</div>
<div id="link-with-svd" class="section level2">
<h2><span class="header-section-number">4.4</span> Link with SVD</h2>
<div id="svd-and-inverse" class="section level3">
<h3><span class="header-section-number">4.4.1</span> SVD and inverse</h3>
<p>Write the SVD of <span class="math inline">\(\mathbf{X}\)</span> (<span class="math inline">\(p&gt;n\)</span>) as <span class="math display">\[
   \mathbf{X} = \sum_{i=1}^n \delta_i \mathbf{u}_i \mathbf{v}_i^T = \sum_{i=1}^p \delta_i \mathbf{u}_i \mathbf{v}_i^T  = \mathbf{U}\boldsymbol{\Delta} \mathbf{V}^T ,
\]</span> with</p>
<ul>
<li><p><span class="math inline">\(\delta_{n+1}=\delta_{n+2}= \cdots = \delta_p=0\)</span></p></li>
<li><p><span class="math inline">\(\boldsymbol{\Delta}\)</span> a <span class="math inline">\(p\times p\)</span> diagonal matrix of the <span class="math inline">\(\delta_1,\ldots, \delta_p\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{U}\)</span> an <span class="math inline">\(n\times p\)</span> matrix and <span class="math inline">\(\mathbf{V}\)</span> a <span class="math inline">\(p \times p\)</span> matrix. Note that only the first <span class="math inline">\(n\)</span> columns of <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are informative.</p></li>
</ul>
<p>With the SVD of <span class="math inline">\(\mathbf{X}\)</span> we write <span class="math display">\[
   \mathbf{X}^T\mathbf{X} = \mathbf{V}\boldsymbol{\Delta
     }^2\mathbf{V}^T.
 \]</span> The inverse of <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is then given by <span class="math display">\[
   (\mathbf{X}^T\mathbf{X})^{-1} = \mathbf{V}\boldsymbol{\Delta}^{-2}\mathbf{V}^T.
 \]</span> Since <span class="math inline">\(\boldsymbol{\Delta}\)</span> has <span class="math inline">\(\delta_{n+1}=\delta_{n+2}= \cdots = \delta_p=0\)</span>, it is not invertible.</p>
</div>
<div id="svd-of-penalised-matrix-mathbfxtxlambda-mathbfi" class="section level3">
<h3><span class="header-section-number">4.4.2</span> SVD of penalised matrix <span class="math inline">\(\mathbf{X^TX}+\lambda \mathbf{I}\)</span></h3>
<p>It can be shown that <span class="math display">\[
  \mathbf{X^TX}+\lambda \mathbf{I} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T ,
\]</span> i.e. adding a constant to the diagonal elements does not affect the eigenvectors, and all eigenvalues are increased by this constant. <span class="math inline">\(\longrightarrow\)</span> zero eigenvalues become <span class="math inline">\(\lambda\)</span>.</p>
<p>Hence, <span class="math display">\[
  (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \mathbf{V}^T ,
\]</span> which can be computed even when some eigenvalues in <span class="math inline">\(\boldsymbol{\Delta}^2\)</span> are zero.</p>
<p>Note, that for high dimensional data (<span class="math inline">\(p&gt;&gt;&gt;n\)</span>) many eigenvalues are zero because <span class="math inline">\(\mathbf{X^TX}\)</span> is a <span class="math inline">\(p \times p\)</span> matrix and has rank <span class="math inline">\(n\)</span>.</p>
<p>The identity <span class="math inline">\(\mathbf{X^TX}+\lambda \mathbf{I} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T\)</span> is easily checked: <span class="math display">\[
  \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T = \mathbf{V}\boldsymbol{\Delta}^2\mathbf{V}^T + \lambda \mathbf{VV}^T  = \mathbf{V}\boldsymbol{\Delta}^2\mathbf{V}^T + \lambda \mathbf{I} = \mathbf{X^TX}+\lambda \mathbf{I}.
\]</span></p>
</div>
</div>
<div id="properties" class="section level2">
<h2><span class="header-section-number">4.5</span> Properties</h2>
<ul>
<li><p>The Ridge estimator is biased! The <span class="math inline">\(\boldsymbol{\beta}\)</span> are shrunken to zero! <span class="math display">\[\begin{eqnarray}
 \text{E}[\hat{\boldsymbol{\beta}}] &amp;=&amp; (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X}^T \text{E}[\mathbf{Y}]\\
&amp;=&amp; (\mathbf{X}^T\mathbf{X}+\lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{X}\boldsymbol{\beta}\\
\end{eqnarray}\]</span></p></li>
<li><p>Note, that the shrinkage is larger in the direction of the smaller eigenvalues.</p></li>
</ul>
<p><span class="math display">\[\begin{eqnarray}
\text{E}[\hat{\boldsymbol{\beta}}]&amp;=&amp;\mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \mathbf{V}^T \mathbf{V} \boldsymbol{\Delta}^2 \mathbf{V}^T\boldsymbol{\beta}\\
&amp;=&amp;\mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \boldsymbol{\Delta}^2 \mathbf{V}^T\boldsymbol{\beta}\\
&amp;=&amp; \mathbf{V}
\left[\begin{array}{ccc}
\frac{\delta_1^2}{\delta_1^2+\lambda}&amp;\ldots&amp;0 \\
&amp;\vdots&amp;\\
0&amp;\ldots&amp;\frac{\delta_r^2}{\delta_r^2+\lambda}
\end{array}\right]
\mathbf{V}^T\boldsymbol{\beta}
\end{eqnarray}\]</span></p>
<ul>
<li><p>the variance of the prediction <span class="math inline">\(\hat{{Y}}(\mathbf{x})=\mathbf{x}^T\hat\beta\)</span>, <span class="math display">\[
 \text{var}\left[\hat{{Y}}(\mathbf{x})\mid \mathbf{x}\right] = \mathbf{x}^T(\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\mathbf{x}
  \]</span> is smaller than with the least-squares estimator.</p></li>
<li><p>through the bias-variance trade-off it is hoped that better predictions in terms of expected conditional test error can be obtained, for an appropriate choice of <span class="math inline">\(\lambda\)</span>.</p></li>
</ul>
<p>Recall the expression of the expected conditional test error <span class="math display">\[\begin{eqnarray}
  Err(\mathbf{x}) &amp;=&amp; \text{E}\left[(\hat{Y} - Y^*)^2\mid \mathbf{x}\right]\\
  &amp;=&amp;
  \text{var}\left[\hat{Y}\mid \mathbf{x}\right] + \text{bias}^2(\mathbf{x})+
  \text{var}\left[Y^*\mid \mathbf{x}\right]
\end{eqnarray}\]</span> where</p>
<ul>
<li><span class="math inline">\(\hat{Y}=\hat{Y}(\mathbf{x})=\mathbf{x}^T\hat{\boldsymbol{\beta}}\)</span> is the prediction at <span class="math inline">\(\mathbf{x}\)</span></li>
<li><span class="math inline">\(Y^*\)</span> is an outcome at predictor <span class="math inline">\(\mathbf{x}\)</span></li>
<li><span class="math inline">\(\mu(\mathbf{x}) = \text{E}\left[\hat{Y}\mid \mathbf{x}\right] \text{ and } \mu^*(x)=\text{E}\left[Y^*\mid \mathbf{x}\right]\)</span></li>
<li><span class="math inline">\(\text{bias}(\mathbf{x})=\mu(\mathbf{x})-\mu^*(\mathbf{x})\)</span></li>
<li><span class="math inline">\(\text{var}\left[Y^*\mid \mathbf{x}\right]\)</span> the irreducible error that does not depend on the model. It simply originates from observations that randomly fluctuate around the true mean <span class="math inline">\(\mu^*(x)\)</span>.</li>
</ul>
</div>
<div id="toxicogenomics-example" class="section level2">
<h2><span class="header-section-number">4.6</span> Toxicogenomics example</h2>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="kw">library</span>(glmnet)</span>
<span id="cb31-2"><a href="#cb31-2"></a>mRidge &lt;-<span class="st"> </span><span class="kw">glmnet</span>(</span>
<span id="cb31-3"><a href="#cb31-3"></a>  <span class="dt">x =</span> toxData[,<span class="op">-</span><span class="dv">1</span>] <span class="op">%&gt;%</span></span>
<span id="cb31-4"><a href="#cb31-4"></a><span class="st">    </span>as.matrix,</span>
<span id="cb31-5"><a href="#cb31-5"></a>  <span class="dt">y =</span> toxData <span class="op">%&gt;%</span></span>
<span id="cb31-6"><a href="#cb31-6"></a><span class="st">    </span><span class="kw">pull</span>(BA),</span>
<span id="cb31-7"><a href="#cb31-7"></a>  <span class="dt">alpha =</span> <span class="dv">0</span>) <span class="co"># ridge: alpha = 0  </span></span>
<span id="cb31-8"><a href="#cb31-8"></a></span>
<span id="cb31-9"><a href="#cb31-9"></a><span class="kw">plot</span>(mRidge, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The R function  uses  to refer to the penalty parameter. In this course we use <span class="math inline">\(\lambda\)</span>, because <span class="math inline">\(\lambda\)</span> is often used as eigenvalues.</p>
<p>The graph shows that with increasing penalty parameter, the parameter estimates are shrunken towards zero. The estimates will only reach zero for <span class="math inline">\(\lambda \rightarrow \infty\)</span>. The stronger the shrinkage, the larger the bias (towards zero) and the smaller the variance of the parameter estimators (and hence also smaller variance of the predictions).</p>
<p>Another (informal) viewpoint is the following. By shrinking the estimates towards zero, the estimates loose some of their ``degrees of freedom’’ so that the parameters become estimable with only <span class="math inline">\(n&lt;p\)</span> data points. Even with a very small <span class="math inline">\(\lambda&gt;0\)</span>, the parameters regain their estimability. However, note that the variance of the estimator is given by <span class="math display">\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \sigma^2 = \mathbf{V}(\boldsymbol{\Delta}^2+\lambda\mathbf{I})^{-1}\mathbf{V}^T\sigma^2.
\]</span> Hence, a small <span class="math inline">\(\lambda\)</span> will result in large variances of the parameter estimators. The larger <span class="math inline">\(\lambda\)</span>, the smaller the variances become. In the limit, as <span class="math inline">\(\lambda\rightarrow\infty\)</span>, the estimates are converged to zero and show no variability any longer.</p>
</div>
</div>
<div id="lasso-regression" class="section level1">
<h1><span class="header-section-number">5</span> Lasso Regression</h1>
<ul>
<li><p>The Lasso is another example of penalised regression.</p></li>
<li><p>The lasso estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> is the solution to minimising the penalised SSE <span class="math display">\[
 \text{SSE}_\text{pen} = \sum_{i=1}^n (Y_i - \mathbf{x}_i^T\boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \vert \beta_j\vert.
\]</span></p></li>
</ul>
<p>or, equivalently, minimising</p>
<p><span class="math display">\[
\text{SSE}  = \Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert_1 \leq c
\]</span> with</p>
<ul>
<li><p><span class="math inline">\(\Vert \mathbf\beta\Vert_1 = \sum\limits_{j=1}^p \vert \beta_j \vert\)</span></p></li>
<li><p>Despite strong similarity between ridge and lasso regression (<span class="math inline">\(L_2\)</span> versus <span class="math inline">\(L_1\)</span> norm in penalty term), there is no analytical solution of the lasso parameter estimate of <span class="math inline">\(\mathbf\beta\)</span>.</p></li>
<li><p>Fortunately, computational efficient algorithms have been implemented in statistical software</p></li>
<li><p>The Lasso estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> is biased and generally has a smaller variance then the least-squares estimator.</p></li>
<li><p>Hence, the bias-variance trade-off may here also help in finding better predictions with biased estimators.</p></li>
<li><p>In contrast to ridge regression, however, the lasso estimator can give at most <span class="math inline">\(\min(p,n)\)</span> non-zero <span class="math inline">\(\beta\)</span>-estimates.</p></li>
<li><p>Hence, at first sight the lasso is not directly appropriate for high-dimensional settings.</p></li>
<li><p>An important advantage of the lasso is that choosing an appropriate value for <span class="math inline">\(\lambda\)</span> is a kind a model building or feature selection procedure (see further).</p></li>
</ul>
<div id="graphical-interpretation-of-lasso-vs-ridge" class="section level2">
<h2><span class="header-section-number">5.1</span> Graphical interpretation of Lasso vs ridge</h2>
<p>Note that the lasso is a constrained regression problem with</p>
<p><span class="math display">\[
\Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert_1 \leq c
\]</span> and ridge <span class="math display">\[
\Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert^2_2 \leq c
\]</span></p>
<p><img src="prediction_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Note, that</p>
<ul>
<li>parameters for the lasso can never switch sign, they are set at zero! Selection!</li>
<li>ridge regression can lead to parameters that switch sign.</li>
</ul>
</div>
<div id="toxicogenomics-example-1" class="section level2">
<h2><span class="header-section-number">5.2</span> Toxicogenomics example</h2>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a>mLasso &lt;-<span class="st"> </span><span class="kw">glmnet</span>(</span>
<span id="cb32-2"><a href="#cb32-2"></a>  <span class="dt">x =</span> toxData[,<span class="op">-</span><span class="dv">1</span>] <span class="op">%&gt;%</span></span>
<span id="cb32-3"><a href="#cb32-3"></a><span class="st">    </span>as.matrix,</span>
<span id="cb32-4"><a href="#cb32-4"></a>  <span class="dt">y =</span> toxData <span class="op">%&gt;%</span></span>
<span id="cb32-5"><a href="#cb32-5"></a><span class="st">    </span><span class="kw">pull</span>(BA),</span>
<span id="cb32-6"><a href="#cb32-6"></a><span class="dt">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb32-7"><a href="#cb32-7"></a><span class="kw">plot</span>(mLasso, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<ul>
<li><p>The graph with the paths of the parameter estimates nicely illustrates the typical behaviour of the lasso estimates as a function of <span class="math inline">\(\lambda\)</span>: when <span class="math inline">\(\lambda\)</span> increases the estimates are shrunken towards zero.</p></li>
<li><p>When an estimate hits zero, it remains exactly equal to zero when <span class="math inline">\(\gamma\)</span> further increases. A parameter estimate equal to zero, say <span class="math inline">\(\hat\beta_j=0\)</span>, implies that the corresponding predictor <span class="math inline">\(x_j\)</span> is no longer included in the model (i.e. <span class="math inline">\(\beta_jx_j=0\)</span>).</p></li>
<li><p>The model fit is known as a sparse model fit (many zeroes). Hence, choosing a appropriate value for <span class="math inline">\(\gamma\)</span> is like choosing the important predictors in the model (feature selection).</p></li>
</ul>
</div>
</div>
<div id="splines-and-the-connection-to-ridge-regression." class="section level1">
<h1><span class="header-section-number">6</span> Splines and the connection to ridge regression.</h1>
<div id="lidar-dataset" class="section level2">
<h2><span class="header-section-number">6.1</span> Lidar dataset</h2>
<ul>
<li><p>LIDAR (light detection and ranging) uses the reflection of laser-emitted light to detect chemical compounds in the atmosphere.</p></li>
<li><p>The LIDAR technique has proven to be an efficient tool for monitoring the distribution of several atmospheric pollutants of importance; see Sigrist (1994).</p></li>
<li><p>The range is the distance traveled before the light is reflected back to its source.</p></li>
<li><p>The logratio is the logarithm of the ratio of received light from two laser sources.</p>
<ul>
<li><p>One source had a frequency equal to the resonance frequency of the compound of interest, which was mercury in this study.</p></li>
<li><p>The other source had a frequency off this resonance frequency.</p></li>
<li><p>The concentration of mercury can be derived from a regression model of the logratio in function of the range for each range x.</p></li>
</ul></li>
</ul>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="kw">library</span>(<span class="st">&quot;SemiPar&quot;</span>)</span>
<span id="cb33-2"><a href="#cb33-2"></a><span class="kw">data</span>(lidar)</span>
<span id="cb33-3"><a href="#cb33-3"></a>pLidar &lt;-<span class="st"> </span>lidar <span class="op">%&gt;%</span></span>
<span id="cb33-4"><a href="#cb33-4"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> range, <span class="dt">y =</span> logratio)) <span class="op">+</span></span>
<span id="cb33-5"><a href="#cb33-5"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb33-6"><a href="#cb33-6"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;range (m)&quot;</span>)</span>
<span id="cb33-7"><a href="#cb33-7"></a></span>
<span id="cb33-8"><a href="#cb33-8"></a>pLidar <span class="op">+</span></span>
<span id="cb33-9"><a href="#cb33-9"></a><span class="st">  </span><span class="kw">geom_smooth</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<ul>
<li>The data is non-linear</li>
<li>Linear regression will not work!</li>
<li>The data shows a smooth relation between the logratio and the range</li>
</ul>
</div>
<div id="basis-expansion" class="section level2">
<h2><span class="header-section-number">6.2</span> Basis expansion</h2>
<p><span class="math display">\[y_i=f(x_i)+\epsilon_i,\]</span> with <span class="math display">\[f(x)=\sum\limits_{k=1}^K \theta_k b_k(x)\]</span></p>
<ul>
<li><p>Select set of basis functions <span class="math inline">\(b_k(x)\)</span></p></li>
<li><p>Select number of basis functions <span class="math inline">\(K\)</span></p></li>
<li><p>Examples</p>
<ul>
<li>Polynomial model: <span class="math inline">\(x^k\)</span></li>
<li>Orthogonal series: Fourier, Legendre polynomials, Wavelets</li>
<li>Polynomial splines: <span class="math inline">\(1, x, (x-t_m)_+\)</span> with <span class="math inline">\(m=1, \ldots, K-2\)</span> knots <span class="math inline">\(t_m\)</span></li>
<li>…</li>
</ul></li>
</ul>
<div id="trunctated-line-basis" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Trunctated line basis</h3>
<p><span class="math display">\[y_i=f(x_i)+\epsilon_i,\]</span></p>
<ul>
<li>One of the most simple basis expansions</li>
<li><span class="math inline">\(f(x_i)=\beta_0+\beta_1x_i+\sum\limits_{m=1}^{K-2}\theta_m(x_i-t_m)_+\)</span> with <span class="math inline">\((.)_+\)</span> the operator that takes the positive part.</li>
<li>Note, that better basis expansions exist, which are orthogonal, computational more stable and/or continuous derivative beyond first order</li>
<li>We will use this basis for didactical purposes</li>
<li>We can use OLS to fit y w.r.t. the basis.</li>
</ul>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a>knots &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">400</span>,<span class="dv">700</span>,<span class="fl">12.5</span>)</span>
<span id="cb35-2"><a href="#cb35-2"></a></span>
<span id="cb35-3"><a href="#cb35-3"></a>basis &lt;-<span class="st"> </span><span class="kw">sapply</span>(knots,</span>
<span id="cb35-4"><a href="#cb35-4"></a>  <span class="cf">function</span>(k,y) (y<span class="op">-</span>k)<span class="op">*</span>(y<span class="op">&gt;</span>k),</span>
<span id="cb35-5"><a href="#cb35-5"></a>  <span class="dt">y=</span> lidar <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(range)</span>
<span id="cb35-6"><a href="#cb35-6"></a>  )</span>
<span id="cb35-7"><a href="#cb35-7"></a></span>
<span id="cb35-8"><a href="#cb35-8"></a>basisExp &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="dt">range =</span> lidar <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(range), basis)</span>
<span id="cb35-9"><a href="#cb35-9"></a></span>
<span id="cb35-10"><a href="#cb35-10"></a>splineFitLs &lt;-<span class="st"> </span><span class="kw">lm</span>(logratio <span class="op">~</span><span class="st"> </span><span class="dv">-1</span> <span class="op">+</span><span class="st"> </span>basisExp, lidar)</span>
<span id="cb35-11"><a href="#cb35-11"></a></span>
<span id="cb35-12"><a href="#cb35-12"></a>pBasis &lt;-<span class="st"> </span>basisExp[,<span class="op">-</span><span class="dv">1</span>] <span class="op">%&gt;%</span></span>
<span id="cb35-13"><a href="#cb35-13"></a><span class="st">  </span>data.frame <span class="op">%&gt;%</span></span>
<span id="cb35-14"><a href="#cb35-14"></a><span class="st">  </span><span class="kw">gather</span>(<span class="st">&quot;basis&quot;</span>,<span class="st">&quot;values&quot;</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">%&gt;%</span></span>
<span id="cb35-15"><a href="#cb35-15"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> range, <span class="dt">y =</span> values, <span class="dt">color =</span> basis)) <span class="op">+</span></span>
<span id="cb35-16"><a href="#cb35-16"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb35-17"><a href="#cb35-17"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&quot;none&quot;</span>) <span class="op">+</span></span>
<span id="cb35-18"><a href="#cb35-18"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;basis&quot;</span>)</span>
<span id="cb35-19"><a href="#cb35-19"></a></span>
<span id="cb35-20"><a href="#cb35-20"></a><span class="kw">grid.arrange</span>(</span>
<span id="cb35-21"><a href="#cb35-21"></a>  pLidar <span class="op">+</span></span>
<span id="cb35-22"><a href="#cb35-22"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x =</span> lidar<span class="op">$</span>range, <span class="dt">y =</span> splineFitLs<span class="op">$</span>fitted), <span class="dt">lwd =</span> <span class="dv">2</span>),</span>
<span id="cb35-23"><a href="#cb35-23"></a>  pBasis,</span>
<span id="cb35-24"><a href="#cb35-24"></a>  <span class="dt">ncol=</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## Warning: Use of `lidar$range` is discouraged. Use `range` instead.</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<ul>
<li>Note, that the model is overfitting!</li>
<li>The fit is very wiggly and is tuned too much to the data.</li>
<li>The fit has a large variance and low bias.</li>
<li>It will therefore not generalise well to predict the logratio of future observations.</li>
</ul>
<div id="solution-for-overfitting" class="section level4">
<h4><span class="header-section-number">6.2.1.1</span> Solution for overfitting?</h4>
<ul>
<li><p>We could perform model selection on the basis to select the important basis functions to model the signal. But, this will have the undesired property that the fit will no longer be smooth.</p></li>
<li><p>We can also adopt a ridge penalty!</p></li>
<li><p>However, we do not want to penalise the intercept and the linear term.<br />
</p></li>
<li><p>Ridge criterion</p></li>
</ul>
<p><span class="math display">\[\Vert\mathbf{Y}-\mathbf{X\beta}\Vert^2+\lambda\boldsymbol{\beta}^T\mathbf{D}\boldsymbol{\beta}
\]</span></p>
<p>With <span class="math inline">\(\mathbf{D}\)</span> with dimensions (K,K): <span class="math inline">\(\mathbf{D}=\left[\begin{array}{cc}\mathbf{0}_{2\times2}&amp; \mathbf{0}_{2\times K-2}\\ \mathbf{0}_{K-2\times2}&amp;\mathbf{I}_{K-2\times K-2}\end{array}\right]\)</span></p>
<ul>
<li>Here we will set the penalty at 900.</li>
</ul>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a>D &lt;-<span class="st"> </span><span class="kw">diag</span>(<span class="kw">ncol</span>(basisExp))</span>
<span id="cb37-2"><a href="#cb37-2"></a>D[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>] &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb37-3"><a href="#cb37-3"></a>lambda &lt;-<span class="st"> </span><span class="dv">900</span></span>
<span id="cb37-4"><a href="#cb37-4"></a>betaRidge &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(basisExp)<span class="op">%*%</span>basisExp<span class="op">+</span>(lambda<span class="op">*</span>D))<span class="op">%*%</span><span class="kw">t</span>(basisExp)<span class="op">%*%</span>lidar<span class="op">$</span>logratio</span>
<span id="cb37-5"><a href="#cb37-5"></a><span class="kw">grid.arrange</span>(</span>
<span id="cb37-6"><a href="#cb37-6"></a>  pLidar <span class="op">+</span></span>
<span id="cb37-7"><a href="#cb37-7"></a><span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x =</span> lidar<span class="op">$</span>range, <span class="dt">y =</span> <span class="kw">c</span>(basisExp <span class="op">%*%</span><span class="st"> </span>betaRidge)), <span class="dt">lwd =</span> <span class="dv">2</span>),</span>
<span id="cb37-8"><a href="#cb37-8"></a>  pBasis,</span>
<span id="cb37-9"><a href="#cb37-9"></a>  <span class="dt">ncol=</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## Warning: Use of `lidar$range` is discouraged. Use `range` instead.</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>How do we choose <span class="math inline">\(\lambda\)</span>?</p>
<hr />
</div>
</div>
</div>
</div>
<div id="evaluation-of-prediction-models" class="section level1">
<h1><span class="header-section-number">7</span> Evaluation of Prediction Models</h1>
<p>Predictions are calculated with the fitted model <span class="math display">\[
   \hat{Y}(\mathbf{x}) = \hat{m}(\mathbf{x})=\mathbf{x}^T\hat{\beta}
 \]</span> when focussing on prediction, we want the prediction error to be as small as possible.</p>
<p>The <strong>prediction error</strong> for a prediction at covariate pattern <span class="math inline">\(\mathbf{x}\)</span> is given by <span class="math display">\[
     \hat{Y}(\mathbf{x}) - Y^*,
  \]</span> where</p>
<ul>
<li><p><span class="math inline">\(\hat{Y}(\mathbf{x})=\mathbf{x}^T\hat{\boldsymbol{\beta}}\)</span> is the prediction at <span class="math inline">\(\mathbf{x}\)</span></p></li>
<li><p><span class="math inline">\(Y^*\)</span> is an outcome at covariate pattern <span class="math inline">\(\mathbf{x}\)</span></p></li>
</ul>
<p>Prediction is typically used to predict an outcome before it is observed.</p>
<ul>
<li>Hence, the outcome <span class="math inline">\(Y^*\)</span> is not observed yet, and</li>
<li>the prediction error cannot be computed.</li>
</ul>
<hr />
<ul>
<li><p>Recall that the prediction model <span class="math inline">\(\hat{Y}(\mathbf{x})\)</span> is estimated by using data in the training data set <span class="math inline">\((\mathbf{X},\mathbf{Y})\)</span>, and</p></li>
<li><p>that the outcome <span class="math inline">\(Y^*\)</span> is an outcome at <span class="math inline">\(\mathbf{x}\)</span> which is assumed to be independent of the training data.</p></li>
<li><p>Goal is to use prediction model for predicting a future observation (<span class="math inline">\(Y^*\)</span>), i.e. an observation that still has to be realised/observed (otherwise prediction seems rather useless).</p></li>
<li><p>Hence, <span class="math inline">\(Y^*\)</span> can never be part of the training data set.</p></li>
</ul>
<hr />
<p>Here we provide definitions and we show how the prediction performance of a prediction model can be evaluated from data.</p>
<p>Let <span class="math inline">\({\cal{T}}=(\mathbf{Y},\mathbf{X})\)</span> denote the training data, from which the prediction model <span class="math inline">\(\hat{Y}(\cdot)\)</span> is build. This building process typically involves feature selection and parameter estimation.</p>
<p>We will use a more general notation for the prediction model: <span class="math inline">\(\hat{m}(\mathbf{x})=\hat{Y}(\mathbf{x})\)</span>.</p>
<hr />
<div id="test-or-generalisation-error" class="section level2">
<h2><span class="header-section-number">7.1</span> Test or Generalisation Error</h2>
<p>The test or generalisation error for prediction model <span class="math inline">\(\hat{m}(\cdot)\)</span> is given by <span class="math display">\[
    \text{Err}_{\cal{T}} = \text{E}_{Y^*,X^*}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\mid {\cal{T}}\right]
  \]</span> where <span class="math inline">\((Y^*,X^*)\)</span> is independent of the training data.</p>
<hr />
<ul>
<li>Note that the test error is conditional on the training data <span class="math inline">\({\cal{T}}\)</span>.</li>
<li>Hence, the test error evaluates the performance of the single model build from the observed training data.</li>
<li>This is the ultimate target of the model assessment, because it is exactly this prediction model that will be used in practice and applied to future predictors <span class="math inline">\(\mathbf{X}^*\)</span> to predict <span class="math inline">\(Y^*\)</span>.</li>
<li>The test error is defined as an average over all such future observations <span class="math inline">\((Y^*,\mathbf{X}^*)\)</span>.</li>
</ul>
<hr />
</div>
<div id="conditional-test-error" class="section level2">
<h2><span class="header-section-number">7.2</span> Conditional test error</h2>
<p>Sometimes the conditional test error is used:</p>
<p>The conditional test error in <span class="math inline">\(\mathbf{x}\)</span> for prediction model <span class="math inline">\(\hat{m}(\mathbf{x})\)</span> is given by <span class="math display">\[
   \text{Err}_{\cal{T}}(\mathbf{x}) = \text{E}_{Y^*}\left[(\hat{m}(\mathbf{x}) - Y^*)^2\mid {\cal{T}}, \mathbf{x}\right]
 \]</span> where <span class="math inline">\(Y^*\)</span> is an outcome at predictor <span class="math inline">\(\mathbf{x}\)</span>, independent of the training data.</p>
<p>Hence, <span class="math display">\[
   \text{Err}_{\cal{T}} = \text{E}_{X^*}\left[\text{Err}_{\cal{T}}(\mathbf{X}^*)\right].
 \]</span></p>
<p>A closely related error is the <strong>insample error</strong>.</p>
<hr />
</div>
<div id="insample-error" class="section level2">
<h2><span class="header-section-number">7.3</span> Insample Error</h2>
<p>The insample error for prediction model <span class="math inline">\(\hat{m}(\mathbf{x})\)</span> is given by <span class="math display">\[
   \text{Err}_{\text{in} \cal{T}} = \frac{1}{n}\sum_{i=1}^n \text{Err}_{\cal{T}}(\mathbf{x}_i),
 \]</span></p>
<p>i.e. the insample error is the sample average of the conditional test errors evaluated in the <span class="math inline">\(n\)</span> training dataset predictors <span class="math inline">\(\mathbf{x}_i\)</span>.</p>
<p>Since <span class="math inline">\(\text{Err}_{\cal{T}}\)</span> is an average over all <span class="math inline">\(\mathbf{X}\)</span>, even over those predictors not observed in the training dataset, it is sometimes referred to as the <strong>outsample error</strong>.</p>
<hr />
</div>
<div id="estimation-of-the-insample-error" class="section level2">
<h2><span class="header-section-number">7.4</span> Estimation of the insample error</h2>
<p>We start with introducing the training error rate, which is closely related to the MSE in linear models.</p>
<div id="training-error" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Training error</h3>
<p>The training error is given by <span class="math display">\[
   \overline{\text{err}} = \frac{1}{n}\sum_{i=1}^n (Y_i - \hat{m}(\mathbf{x}_i))^2 ,
 \]</span> where the <span class="math inline">\((Y_i,\mathbf{x}_i)\)</span> from the training dataset which is also used for the calculation of <span class="math inline">\(\hat{m}\)</span>.</p>
<ul>
<li><p>The training error is an overly optimistic estimate of the test error <span class="math inline">\(\text{Err}_{\cal{T}}\)</span>.</p></li>
<li><p>The training error will never increases when the model becomes more complex. <span class="math inline">\(\longrightarrow\)</span> cannot be used directly as a model selection criterion.</p></li>
</ul>
<p>Indeed, model parameters are often estimated by minimising the training error (cfr. SSE).</p>
<ul>
<li>Hence the fitted model adapts to the training data, and</li>
<li>training error will be an overly optimistic estimate of the test error <span class="math inline">\(\text{Err}_{\cal{T}}\)</span>.</li>
</ul>
<hr />
<p>It can be shown that the training error is related to the insample test error via</p>
<p><span class="math display">\[
\text{E}_\mathbf{Y}
\left[\text{Err}_{\text{in}{\cal{T}}}\right] = \text{E}_\mathbf{Y}\left[\overline{\text{err}}\right] + \frac{2}{n}\sum_{i=1}^n \text{cov}_\mathbf{Y}\left[\hat{m}(\mathbf{x}_i),Y_i\right],
\]</span></p>
<p>Note, that for linear models <span class="math display">\[ \hat{m}(\mathbf{x}_i) = \mathbf{X}\hat{\boldsymbol{\beta}}= \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} = \mathbf{HY}
\]</span> with</p>
<ul>
<li><span class="math inline">\(\mathbf{H}\)</span> the hat matrix and</li>
<li>all <span class="math inline">\(Y_i\)</span> are assumed to be independently distributed <span class="math inline">\(N(\mathbf{X}\boldsymbol{\beta},\sigma^2)\)</span></li>
</ul>
<p>Hence, for linear models with independent observations</p>
<p><span class="math display">\[\begin{eqnarray}
\text{cov}_\mathbf{Y}\left[\hat{m}(\mathbf{x}_i),Y_i)\right] &amp;=&amp;
\text{cov}_\mathbf{Y}\left[\mathbf{H}_{i}^T\mathbf{Y},Y_i)\right]\\
&amp;=&amp; \text{cov}_\mathbf{Y}\left[h_{ii} Y_i,Y_i\right]\\
&amp;=&amp; h_{ii} \text{cov}_\mathbf{Y}\left[Y_i,Y_i\right]\\
&amp;=&amp; h_{ii} \sigma^2\\
\end{eqnarray}\]</span></p>
<p>And we can thus estimate the insample error by Mallow’s <span class="math inline">\(C_p\)</span></p>
<p><span class="math display">\[\begin{eqnarray}
C_p &amp;=&amp; \overline{\text{err}} + \frac{2\sigma^2}{n}\text{tr}(\mathbf{H})\\
&amp;=&amp; \overline{\text{err}} + \frac{2\sigma^2p}{n}
\end{eqnarray}\]</span></p>
<p>with <span class="math inline">\(p\)</span> the number of predictors.</p>
<ul>
<li>Mallow’s <span class="math inline">\(C_p\)</span> is often used for model selection.</li>
<li>Note, that we can also consider it as a kind of penalized least squares:</li>
</ul>
<p><span class="math display">\[
n \times C_p = \Vert \mathbf{Y} - \mathbf{X}\boldsymbol{\beta}\Vert_2^2 + 2\sigma^2 \Vert \boldsymbol{\beta} \Vert_0
\]</span> with <span class="math inline">\(L_0\)</span> norm <span class="math inline">\(\Vert \boldsymbol{\beta} \Vert_0 = \sum_{j=1}^p \beta_p^0 = p\)</span>.</p>
<hr />
</div>
</div>
<div id="expected-test-error" class="section level2">
<h2><span class="header-section-number">7.5</span> Expected test error</h2>
<p>The test or generalisation error was defined conditionally on the training data. By averaging over the distribution of training datasets, the expected test error arises.</p>
<p><span class="math display">\[\begin{eqnarray*}
   \text{E}_{\cal{T}}\left[\text{Err}_{{\cal{T}}}\right]
     &amp;=&amp; \text{E}_{\cal{T}}\left[\text{E}_{Y^*,X^*}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\mid {\cal{T}}\right]\right] \\
     &amp;=&amp; \text{E}_{Y^*,X^*,{\cal{T}}}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\right].
 \end{eqnarray*}\]</span></p>
<ul>
<li><p>The expected test error may not be of direct interest when the goal is to assess the prediction performance of a single prediction model <span class="math inline">\(\hat{m}(\cdot)\)</span>.</p></li>
<li><p>The expected test error averages the test errors of all models that can be build from all training datasets, and hence this may be less relevant when the interest is in evaluating one particular model that resulted from a single observed training dataset.</p></li>
<li><p>Also note that building a prediction model involves both parameter estimation and feature selection.</p></li>
<li><p>Hence the expected test error also evaluates the feature selection procedure (on average).</p></li>
<li><p>If the expected test error is small, it is an indication that the model building process gives good predictions for future observations <span class="math inline">\((Y^*,\mathbf{X}^*)\)</span> on average.</p></li>
</ul>
<div id="estimating-the-expected-test-error" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Estimating the Expected test error</h3>
<p>The expected test error may be estimated by cross validation (CV).</p>
<div id="leave-one-out-cross-validation-loocv" class="section level4">
<h4><span class="header-section-number">7.5.1.1</span> Leave one out cross validation (LOOCV)}</h4>
<p>The LOOCV estimator of the expected test error (or expected outsample error) is given by <span class="math display">\[
     \text{CV} = \frac{1}{n} \sum_{i=1}^n \left(Y_i - \hat{m}^{-i}(\mathbf{x}_i)\right)^2 ,
  \]</span> where</p>
<ul>
<li>the <span class="math inline">\((Y_i,\mathbf{x}_i)\)</span> form the training dataset</li>
<li><span class="math inline">\(\hat{m}^{-i}\)</span> is the fitted model based on all training data, except observation <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(\hat{m}^{-i}(\mathbf{x}_i)\)</span> is the prediction at <span class="math inline">\(\mathbf{x}_i\)</span>, which is the observation left out the training data before building model <span class="math inline">\(m\)</span>.</li>
</ul>
<p>Some rationale as to why LOOCV offers a good estimator of the outsample error:</p>
<ul>
<li><p>the prediction error <span class="math inline">\(Y^*-\hat{m}(\mathbf{x})\)</span> is mimicked by not using one of the training outcomes <span class="math inline">\(Y_i\)</span> for the estimation of the model so that this <span class="math inline">\(Y_i\)</span> plays the role of <span class="math inline">\(Y^*\)</span>, and, consequently, the fitted model <span class="math inline">\(\hat{m}^{-i}\)</span> is independent of <span class="math inline">\(Y_i\)</span></p></li>
<li><p>the sum in <span class="math inline">\(CV\)</span> is over all <span class="math inline">\(\mathbf{x}_i\)</span> in the training dataset, but each term <span class="math inline">\(\mathbf{x}_i\)</span> was left out once for the calculation of <span class="math inline">\(\hat{m}^{-i}\)</span>. Hence, <span class="math inline">\(\hat{m}^{-i}(\mathbf{x}_i)\)</span> mimics an outsample prediction.</p></li>
<li><p>the sum in CV is over <span class="math inline">\(n\)</span> different training datasets (each one with a different observation removed), and hence CV is an estimator of the <em>expected</em> test error.</p></li>
<li><p>For linear models the LOOCV can be readily obtained from the fitted model: i.e.</p></li>
</ul>
<p><span class="math display">\[\text{CV} = \frac{1}{n}\sum\limits_{i=1}^n \frac{e_i^2}{(1-h_{ii})^2}\]</span></p>
<p>with <span class="math inline">\(e_i\)</span> the residuals from the model that is fitted based on all training data.</p>
<hr />
<p>An alternative to LOOCV is the <span class="math inline">\(k\)</span>-fold cross validation procedure. It also gives an estimate of the expected outsample error.</p>
</div>
<div id="k-fold-cross-validation" class="section level4">
<h4><span class="header-section-number">7.5.1.2</span> <span class="math inline">\(k\)</span>-fold cross validation</h4>
<ul>
<li><p>Randomly divide the training dataset into <span class="math inline">\(k\)</span> approximately equal subsets . Let <span class="math inline">\(S_j\)</span> denote the index set of the <span class="math inline">\(j\)</span>th subset (referred to as a <strong>fold</strong>). Let <span class="math inline">\(n_j\)</span> denote the number of observations in fold <span class="math inline">\(j\)</span>.</p></li>
<li><p>The <span class="math inline">\(k\)</span>-fold cross validation estimator of the expected outsample error is given by <span class="math display">\[
   \text{CV}_k = \frac{1}{k}\sum_{j=1}^k \frac{1}{n_j} \sum_{i\in S_j} \left(Y_i - \hat{m}^{-S_j}(\mathbf{x}_i)\right)^2
 \]</span> where <span class="math inline">\(\hat{m}^{-S_j}\)</span> is the model fitted using all training data, except observations in fold <span class="math inline">\(j\)</span> (i.e. observations <span class="math inline">\(i \in S_j\)</span>).</p></li>
</ul>
<hr />
<p>The cross validation estimators of the expected outsample error are nearly unbiased. One argument that helps to understand where the bias comes from is the fact that e.g. in de LOOCV estimator the model is fit on only <span class="math inline">\(n-1\)</span> observations, whereas we are aiming at estimating the outsample error of a model fit on all <span class="math inline">\(n\)</span> training observations. Fortunately, the bias is often small and is in practice hardly a concern.</p>
<p><span class="math inline">\(k\)</span>-fold CV is computationally more complex.</p>
<p>Since CV and CV<span class="math inline">\(_k\)</span> are estimators, they also show sampling variability. Standard errors of the CV or CV<span class="math inline">\(_k\)</span> can be computed. We don’t show the details, but in the example this is illustrated.</p>
</div>
</div>
<div id="bias-variance-trade-off" class="section level3">
<h3><span class="header-section-number">7.5.2</span> Bias Variance trade-off</h3>
<p>For the expected conditional test error in <span class="math inline">\(\mathbf{x}\)</span>, it holds that <span class="math display">\[\begin{eqnarray*}
  \text{E}_{\cal{T}}\left[\text{Err}_{\cal{T}}(\mathbf{x})\right]
    &amp;=&amp; \text{E}_{Y^*,{\cal{T}}}\left[(\hat{m}(\mathbf{x})-Y^*)^2 \mid \mathbf{x}\right] \\
    &amp;=&amp;  \text{var}_{\mathbf{Y}}\left[\hat{Y}(\mathbf{x})\mid \mathbf{x}\right] +(\mu(\mathbf{x})-\mu^*(\mathbf{x}))^2+\text{var}_{Y^*}\left[Y^*\mid \mathbf{x}\right]
\end{eqnarray*}\]</span> where <span class="math inline">\(\mu(\mathbf{x}) = \text{E}_{\mathbf{Y}}\left[\hat{Y}(\mathbf{x})\mid \mathbf{x}\right] \text{ and } \mu^*(\mathbf{x})=\text{E}_{Y^*}\left[Y^*\mid \mathbf{x}\right]\)</span>.</p>
<ul>
<li><p><strong>bias</strong>: <span class="math inline">\(\text{bias}(\mathbf{x})=\mu(\mathbf{x})-\mu^*(\mathbf{x})\)</span></p></li>
<li><p><span class="math inline">\(\text{var}_{Y^*}\left[Y^*\mid \mathbf{x}\right]\)</span> does not depend on the model, and is referred to as the <strong>irreducible variance</strong>.</p></li>
</ul>
<hr />
<p>The importance of the bias-variance trade-off can be seen from a model selection perspective. When we agree that a good model is a model that has a small expected conditional test error at some point <span class="math inline">\(\mathbf{x}\)</span>, then the bias-variance trade-off shows us that a model may be biased as long as it has a small variance to compensate for the bias. It often happens that a biased model has a substantial smaller variance. When these two are combined, a small expected test error may occur.</p>
<p>Also note that the model <span class="math inline">\(m\)</span> which forms the basis of the prediction model <span class="math inline">\(\hat{m}(\mathbf{x})\)</span> does NOT need to satisfy <span class="math inline">\(m(\mathbf{x})=\mu(\mathbf{x})\)</span> or <span class="math inline">\(m(\mathbf{x})=\mu^*(\mathbf{x})\)</span>. The model <span class="math inline">\(m\)</span> is known by the data-analyst (its the basis of the prediction model), whereas <span class="math inline">\(\mu(\mathbf{x})\)</span> and <span class="math inline">\(\mu^*(\mathbf{x})\)</span> are generally unknown to the data-analyst. We only hope that <span class="math inline">\(m\)</span> serves well as a prediction model.</p>
<hr />
</div>
<div id="in-practice" class="section level3">
<h3><span class="header-section-number">7.5.3</span> In practice</h3>
<p>We use cross validation to estimate the lambda penalty for penalised regression:</p>
<ul>
<li>Ridge Regression</li>
<li>Lasso</li>
<li>Build models, e.g. select the number of PCs for PCA regression</li>
<li>Splines</li>
</ul>
</div>
<div id="toxicogenomics-example-2" class="section level3">
<h3><span class="header-section-number">7.5.4</span> Toxicogenomics example</h3>
<div id="lasso" class="section level4">
<h4><span class="header-section-number">7.5.4.1</span> Lasso</h4>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a><span class="kw">set.seed</span>(<span class="dv">15</span>)</span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="kw">library</span>(glmnet)</span>
<span id="cb39-3"><a href="#cb39-3"></a>mCvLasso &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(</span>
<span id="cb39-4"><a href="#cb39-4"></a>  <span class="dt">x =</span> toxData[,<span class="op">-</span><span class="dv">1</span>] <span class="op">%&gt;%</span></span>
<span id="cb39-5"><a href="#cb39-5"></a><span class="st">    </span>as.matrix,</span>
<span id="cb39-6"><a href="#cb39-6"></a>  <span class="dt">y =</span> toxData <span class="op">%&gt;%</span></span>
<span id="cb39-7"><a href="#cb39-7"></a><span class="st">    </span><span class="kw">pull</span>(BA),</span>
<span id="cb39-8"><a href="#cb39-8"></a>  <span class="dt">alpha =</span> <span class="dv">1</span>)  <span class="co"># lasso alpha=1</span></span>
<span id="cb39-9"><a href="#cb39-9"></a></span>
<span id="cb39-10"><a href="#cb39-10"></a><span class="kw">plot</span>(mCvLasso)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Default CV procedure in  is <span class="math inline">\(k=10\)</span>-fold CV.</p>
<p>The Graphs shows</p>
<ul>
<li>10-fold CV estimates of the extra-sample error as a function of the lasso penalty parameter <span class="math inline">\(\lambda\)</span>.</li>
<li>estimate plus and minus once the estimated standard error of the CV estimate (grey bars)</li>
<li>On top the number of non-zero regression parameter estimates are shown.</li>
</ul>
<p>Two vertical reference lines are added to the graph. They correspond to</p>
<ul>
<li>the <span class="math inline">\(\log(\lambda)\)</span> that gives the smallest CV estimate of the extra-sample error, and</li>
<li>the largest <span class="math inline">\(\log(\lambda)\)</span> that gives a CV estimate of the extra-sample error that is within one standard error from the smallest error estimate.</li>
<li>The latter choice of <span class="math inline">\(\lambda\)</span> has no firm theoretical basis, except that it somehow accounts for the imprecision of the error estimate. One could loosely say that this <span class="math inline">\(\gamma\)</span> corresponds to the smallest model (i.e. least number of predictors) that gives an error that is within margin of error of the error of the best model.</li>
</ul>
<hr />
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a>mLassoOpt &lt;-<span class="st"> </span><span class="kw">glmnet</span>(</span>
<span id="cb40-2"><a href="#cb40-2"></a>  <span class="dt">x =</span> toxData[,<span class="op">-</span><span class="dv">1</span>] <span class="op">%&gt;%</span></span>
<span id="cb40-3"><a href="#cb40-3"></a><span class="st">    </span>as.matrix,</span>
<span id="cb40-4"><a href="#cb40-4"></a>  <span class="dt">y =</span> toxData <span class="op">%&gt;%</span></span>
<span id="cb40-5"><a href="#cb40-5"></a><span class="st">    </span><span class="kw">pull</span>(BA),</span>
<span id="cb40-6"><a href="#cb40-6"></a>    <span class="dt">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb40-7"><a href="#cb40-7"></a>    <span class="dt">lambda =</span> mCvLasso<span class="op">$</span>lambda.min)</span>
<span id="cb40-8"><a href="#cb40-8"></a></span>
<span id="cb40-9"><a href="#cb40-9"></a><span class="kw">summary</span>(<span class="kw">coef</span>(mLassoOpt))</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["i"],"name":[1],"type":["int"],"align":["right"]},{"label":["j"],"name":[2],"type":["int"],"align":["right"]},{"label":["x"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1","3":"1.780154e-16"},{"1":"7","2":"1","3":"7.723008e-01"},{"1":"105","2":"1","3":"6.745130e-01"},{"1":"147","2":"1","3":"-7.480144e-01"},{"1":"420","2":"1","3":"1.275626e+00"},{"1":"453","2":"1","3":"4.271345e-02"},{"1":"1720","2":"1","3":"-4.548228e-01"},{"1":"1952","2":"1","3":"3.650587e-01"},{"1":"2032","2":"1","3":"1.233149e-04"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>With the optimal <span class="math inline">\(\lambda\)</span> (smallest error estimate) the output shows the 9 non-zero estimated regression coefficients (sparse solution).</p>
<hr />
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1"></a>mLasso1se &lt;-<span class="st"> </span><span class="kw">glmnet</span>(</span>
<span id="cb41-2"><a href="#cb41-2"></a>  <span class="dt">x =</span> toxData[,<span class="op">-</span><span class="dv">1</span>] <span class="op">%&gt;%</span></span>
<span id="cb41-3"><a href="#cb41-3"></a><span class="st">    </span>as.matrix,</span>
<span id="cb41-4"><a href="#cb41-4"></a>    <span class="dt">y=</span> toxData <span class="op">%&gt;%</span></span>
<span id="cb41-5"><a href="#cb41-5"></a><span class="st">      </span><span class="kw">pull</span>(BA),</span>
<span id="cb41-6"><a href="#cb41-6"></a>    <span class="dt">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb41-7"><a href="#cb41-7"></a>    <span class="dt">lambda =</span> mCvLasso<span class="op">$</span>lambda<span class="fl">.1</span>se)</span>
<span id="cb41-8"><a href="#cb41-8"></a></span>
<span id="cb41-9"><a href="#cb41-9"></a>mLasso1se <span class="op">%&gt;%</span></span>
<span id="cb41-10"><a href="#cb41-10"></a><span class="st">  </span>coef <span class="op">%&gt;%</span></span>
<span id="cb41-11"><a href="#cb41-11"></a><span class="st">  </span>summary</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["i"],"name":[1],"type":["int"],"align":["right"]},{"label":["j"],"name":[2],"type":["int"],"align":["right"]},{"label":["x"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1","3":"1.324855e-16"},{"1":"7","2":"1","3":"6.255918e-01"},{"1":"147","2":"1","3":"-1.767770e-02"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>This shows the solution for the largest <span class="math inline">\(\lambda\)</span> within one standard error of the optimal model. Now only 3 non-zero estimates result.</p>
<hr />
</div>
<div id="ridge" class="section level4">
<h4><span class="header-section-number">7.5.4.2</span> Ridge</h4>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1"></a>mCvRidge &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(</span>
<span id="cb42-2"><a href="#cb42-2"></a>  <span class="dt">x =</span> toxData[,<span class="op">-</span><span class="dv">1</span>] <span class="op">%&gt;%</span></span>
<span id="cb42-3"><a href="#cb42-3"></a><span class="st">    </span>as.matrix,</span>
<span id="cb42-4"><a href="#cb42-4"></a>    <span class="dt">y =</span> toxData <span class="op">%&gt;%</span></span>
<span id="cb42-5"><a href="#cb42-5"></a><span class="st">      </span><span class="kw">pull</span>(BA),</span>
<span id="cb42-6"><a href="#cb42-6"></a>      <span class="dt">alpha =</span> <span class="dv">0</span>)  <span class="co"># ridge alpha=0</span></span>
<span id="cb42-7"><a href="#cb42-7"></a></span>
<span id="cb42-8"><a href="#cb42-8"></a><span class="kw">plot</span>(mCvRidge)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<ul>
<li>Ridge does not seem to have optimal solution.</li>
<li>10-fold CV is also larger than for lasso.</li>
</ul>
<hr />
</div>
<div id="pca-regression" class="section level4">
<h4><span class="header-section-number">7.5.4.3</span> PCA regression</h4>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1"></a><span class="kw">set.seed</span>(<span class="dv">1264</span>)</span>
<span id="cb43-2"><a href="#cb43-2"></a><span class="kw">library</span>(DAAG)</span></code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;lattice&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:boot&#39;:
## 
##     melanoma</code></pre>
<pre><code>## The following object is masked from &#39;package:fda&#39;:
## 
##     melanoma</code></pre>
<pre><code>## 
## Attaching package: &#39;DAAG&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:plyr&#39;:
## 
##     ozone</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1"></a>tox &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb50-2"><a href="#cb50-2"></a>  <span class="dt">Y =</span> toxData <span class="op">%&gt;%</span></span>
<span id="cb50-3"><a href="#cb50-3"></a><span class="st">    </span><span class="kw">pull</span>(BA),</span>
<span id="cb50-4"><a href="#cb50-4"></a>  <span class="dt">PC =</span> Zk)</span>
<span id="cb50-5"><a href="#cb50-5"></a></span>
<span id="cb50-6"><a href="#cb50-6"></a>PC.seq &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">25</span></span>
<span id="cb50-7"><a href="#cb50-7"></a>Err &lt;-<span class="st"> </span><span class="kw">numeric</span>(<span class="dv">25</span>)</span>
<span id="cb50-8"><a href="#cb50-8"></a></span>
<span id="cb50-9"><a href="#cb50-9"></a>mCvPca &lt;-<span class="st"> </span><span class="kw">cv.lm</span>(</span>
<span id="cb50-10"><a href="#cb50-10"></a>  Y<span class="op">~</span>PC<span class="fl">.1</span>,</span>
<span id="cb50-11"><a href="#cb50-11"></a>  <span class="dt">data =</span> tox,</span>
<span id="cb50-12"><a href="#cb50-12"></a>  <span class="dt">m =</span> <span class="dv">5</span>,</span>
<span id="cb50-13"><a href="#cb50-13"></a>  <span class="dt">printit =</span> <span class="ot">FALSE</span>)</span>
<span id="cb50-14"><a href="#cb50-14"></a></span>
<span id="cb50-15"><a href="#cb50-15"></a>Err[<span class="dv">1</span>]&lt;-<span class="kw">attr</span>(mCvPca,<span class="st">&quot;ms&quot;</span>)</span>
<span id="cb50-16"><a href="#cb50-16"></a></span>
<span id="cb50-17"><a href="#cb50-17"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="dv">25</span>) {</span>
<span id="cb50-18"><a href="#cb50-18"></a>  mCvPca &lt;-<span class="st"> </span><span class="kw">cv.lm</span>(</span>
<span id="cb50-19"><a href="#cb50-19"></a>    <span class="kw">as.formula</span>(</span>
<span id="cb50-20"><a href="#cb50-20"></a>      <span class="kw">paste</span>(<span class="st">&quot;Y ~ PC.1 + &quot;</span>,</span>
<span id="cb50-21"><a href="#cb50-21"></a>        <span class="kw">paste</span>(<span class="st">&quot;PC.&quot;</span>, <span class="dv">2</span><span class="op">:</span>i, <span class="dt">collapse =</span> <span class="st">&quot;+&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),</span>
<span id="cb50-22"><a href="#cb50-22"></a>        <span class="dt">sep=</span><span class="st">&quot;&quot;</span></span>
<span id="cb50-23"><a href="#cb50-23"></a>      )</span>
<span id="cb50-24"><a href="#cb50-24"></a>    ),</span>
<span id="cb50-25"><a href="#cb50-25"></a>    <span class="dt">data =</span> tox,</span>
<span id="cb50-26"><a href="#cb50-26"></a>    <span class="dt">m =</span> <span class="dv">5</span>,</span>
<span id="cb50-27"><a href="#cb50-27"></a>    <span class="dt">printit =</span> <span class="ot">FALSE</span>)</span>
<span id="cb50-28"><a href="#cb50-28"></a>  Err[i]&lt;-<span class="kw">attr</span>(mCvPca,<span class="st">&quot;ms&quot;</span>)</span>
<span id="cb50-29"><a href="#cb50-29"></a>}</span></code></pre></div>
<ul>
<li><p>Here we illustrate principal component regression.</p></li>
<li><p>The most important PCs are selected in a forward model selection procedure.</p></li>
<li><p>Within the model selection procedure the models are evaluated with 5-fold CV estimates of the outsample error.</p></li>
<li><p>It is important to realise that a forward model selection procedure will not necessarily result in the best prediction model, particularly because the order of the PCs is generally not related to the importance of the PCs for predicting the outcome.</p></li>
<li><p>A supervised PC would be better.</p></li>
</ul>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1"></a>pPCreg &lt;-<span class="st"> </span><span class="kw">data.frame</span>(PC.seq, Err) <span class="op">%&gt;%</span></span>
<span id="cb51-2"><a href="#cb51-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> PC.seq, <span class="dt">y =</span> Err)) <span class="op">+</span></span>
<span id="cb51-3"><a href="#cb51-3"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb51-4"><a href="#cb51-4"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb51-5"><a href="#cb51-5"></a><span class="st">  </span><span class="kw">geom_hline</span>(</span>
<span id="cb51-6"><a href="#cb51-6"></a>    <span class="dt">yintercept =</span> <span class="kw">c</span>(</span>
<span id="cb51-7"><a href="#cb51-7"></a>      mCvLasso<span class="op">$</span>cvm[mCvLasso<span class="op">$</span>lambda<span class="op">==</span>mCvLasso<span class="op">$</span>lambda.min],</span>
<span id="cb51-8"><a href="#cb51-8"></a>      mCvLasso<span class="op">$</span>cvm[mCvLasso<span class="op">$</span>lambda<span class="op">==</span>mCvLasso<span class="op">$</span>lambda<span class="fl">.1</span>se]),</span>
<span id="cb51-9"><a href="#cb51-9"></a>    <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></span>
<span id="cb51-10"><a href="#cb51-10"></a><span class="st">  </span><span class="kw">xlim</span>(<span class="dv">1</span>,<span class="dv">26</span>)</span>
<span id="cb51-11"><a href="#cb51-11"></a></span>
<span id="cb51-12"><a href="#cb51-12"></a><span class="kw">grid.arrange</span>(</span>
<span id="cb51-13"><a href="#cb51-13"></a>  pPCreg,</span>
<span id="cb51-14"><a href="#cb51-14"></a>  pPCreg <span class="op">+</span><span class="st"> </span><span class="kw">ylim</span>(<span class="dv">0</span>,<span class="dv">5</span>),</span>
<span id="cb51-15"><a href="#cb51-15"></a>  <span class="dt">ncol=</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## Warning: Removed 6 row(s) containing missing values (geom_path).</code></pre>
<pre><code>## Warning: Removed 6 rows containing missing values (geom_point).</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<ul>
<li><p>The graph shows the CV estimate of the outsample error as a function of the number of sparse PCs included in the model.</p></li>
<li><p>A very small error is obtained with the model with only the first PC. The best model with 3 PCs.</p></li>
<li><p>The two vertical reference lines correspond to the error estimates obtained with lasso (optimal <span class="math inline">\(\lambda\)</span> and largest <span class="math inline">\(\lambda\)</span> within one standard error).</p></li>
<li><p>Thus although there was a priori no guarantee that the first PCs are the most predictive, it seems to be the case here (we were lucky!).</p></li>
<li><p>Moreover, the first PC resulted in a small outsample error.</p></li>
<li><p>Note that the graph does not indicate the variability of the error estimates (no error bars).</p></li>
<li><p>Also note that the graph clearly illustrates the effect of overfitting: including too many PCs causes a large outsample error.</p></li>
</ul>
</div>
</div>
<div id="lidar-example-splines" class="section level3">
<h3><span class="header-section-number">7.5.5</span> Lidar Example: splines</h3>
<ul>
<li>We use the mgcv package to fit the spline model to the lidar data.</li>
<li>A better basis is used than the truncated spline basis</li>
<li>Thin plate splines are also linear smoothers, i.e.<br />
<span class="math inline">\(\hat{Y} = \hat{m}(\mathbf{X}) = \mathbf{SY}\)</span></li>
<li>So their variance can be easily calculated.</li>
<li>The ridge/smoothness penalty is chosen by generalized cross validation.</li>
</ul>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1"></a><span class="kw">library</span>(mgcv)</span></code></pre></div>
<pre><code>## Loading required package: nlme</code></pre>
<pre><code>## 
## Attaching package: &#39;nlme&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:IRanges&#39;:
## 
##     collapse</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     collapse</code></pre>
<pre><code>## This is mgcv 1.8-33. For overview type &#39;help(&quot;mgcv-package&quot;)&#39;.</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1"></a>gamfit &lt;-<span class="st"> </span><span class="kw">gam</span>(logratio <span class="op">~</span><span class="st"> </span><span class="kw">s</span>(range), <span class="dt">data =</span> lidar)</span>
<span id="cb60-2"><a href="#cb60-2"></a>gamfit<span class="op">$</span>sp</span></code></pre></div>
<pre><code>##    s(range) 
## 0.006114634</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1"></a>pLidar <span class="op">+</span></span>
<span id="cb62-2"><a href="#cb62-2"></a><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">x =</span> lidar<span class="op">$</span>range, <span class="dt">y =</span> gamfit<span class="op">$</span>fitted), <span class="dt">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## Warning: Use of `lidar$range` is discouraged. Use `range` instead.</code></pre>
<p><img src="prediction_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
</div>
</div>
<div id="more-general-error-definitions" class="section level2">
<h2><span class="header-section-number">7.6</span> More general error definitions</h2>
<p>So far we only looked at continuous outcomes <span class="math inline">\(Y\)</span> and errors defined in terms of the squared loss <span class="math inline">\((\hat{m}(\mathbf{x})-Y^*)^2\)</span>.</p>
<p>More generally, a <strong>loss function</strong> measures an discrepancy between the prediction <span class="math inline">\(\hat{m}(\mathbf{x})\)</span> and an independent outcome <span class="math inline">\(Y^*\)</span> that corresponds to <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>Some examples for continuous <span class="math inline">\(Y\)</span>: <span class="math display">\[\begin{eqnarray*}
  L(Y^*,\hat{m}(\mathbf{x}))
    &amp;=&amp; (\hat{m}(\mathbf{x})-Y^*)^2 \;\;\text{(squared error)} \\
  L(Y^*,\hat{m}(\mathbf{x}))
    &amp;=&amp; \vert\hat{m}(\mathbf{x})-Y^*\vert \;\;\text{(absolute error)} \\
   L(Y^*,\hat{m}(\mathbf{x}))
    &amp;=&amp; 2 \int_{\cal{Y}} f_y(y) \log\frac{f_y(y)}{f_{\hat{m}}(y)} dy \;\;\text{(deviance)}.
\end{eqnarray*}\]</span></p>
<p>In the expression of the deviance</p>
<ul>
<li><span class="math inline">\(f_y\)</span> denotes the density function of a distribution with mean set to <span class="math inline">\(y\)</span> (cfr. perfect fit), and</li>
<li><span class="math inline">\(f_{\hat{m}}\)</span> is the density function of the same distribution but with mean set to the predicted outcome <span class="math inline">\(\hat{m}(\mathbf{x})\)</span>.</li>
</ul>
<hr />
<p>With a given loss function, the errors are defined as follows: - Test or generalisation or outsample error <span class="math display">\[
      \text{Err}_{\cal{T}} = \text{E}_{Y^*,X^*}\left[L(Y^*,\hat{m}(\mathbf{X}^*))\right]
    \]</span></p>
<ul>
<li><p>Training error <span class="math display">\[
  \overline{\text{err}} = \frac{1}{n}\sum_{i=1}^n L(Y_i,\hat{m}(\mathbf{x}_i))
\]</span></p></li>
<li><p><span class="math inline">\(\ldots\)</span></p></li>
</ul>
<hr />
<p>When an exponential family distribution is assumed for the outcome distribution, and when the deviance loss is used, the insample error can be estimated by means of the AIC and BIC.</p>
<div id="akaikes-information-criterion-aic" class="section level3">
<h3><span class="header-section-number">7.6.1</span> Akaike’s Information Criterion (AIC)</h3>
<p>The AIC for a model <span class="math inline">\(m\)</span> is given by <span class="math display">\[
\text{AIC} = -2 \ln \hat{L}(m) +2p
\]</span> where <span class="math inline">\(\hat{L}(m)\)</span> is the maximised likelihood for model <span class="math inline">\(m\)</span>.</p>
<p>When assuming normally distributed error terms and homoscedasticity, the AIC becomes <span class="math display">\[
\text{AIC} = n\ln \text{SSE}(m) +2p = n\ln(n\overline{\text{err}}(m)) + 2p
\]</span> with <span class="math inline">\(\text{SSE}(m)\)</span> the residual sum of squares of model <span class="math inline">\(m\)</span>.</p>
<p>In linear models with normal error terms, Mallow’s <span class="math inline">\(C_p\)</span> criterion (statistic) is a linearised version of AIC and it is an unbiased estimator of the in-sample error.</p>
<hr />
</div>
<div id="bayesian-information-criterion-bic" class="section level3">
<h3><span class="header-section-number">7.6.2</span> Bayesian Information Criterion (BIC)}</h3>
<p>The BIC for a model <span class="math inline">\(m\)</span> is given by <span class="math display">\[
\text{BIC} = -2 \ln \hat{L}(m) +p\ln(n)
\]</span> where <span class="math inline">\(\hat{L}(m)\)</span> is the maximised likelihood for model <span class="math inline">\(m\)</span>.</p>
<p>When assuming normally distributed error terms and homoscedasticity, the BIC becomes <span class="math display">\[
\text{BIC} = n\ln \text{SSE}(m) +p\ln(n) = n\ln(n\overline{\text{err}}(m)) + p\ln(n)
\]</span> with <span class="math inline">\(\text{SSE}(m)\)</span> the residual sum of squares of model <span class="math inline">\(m\)</span>.</p>
<p>When large datasets are used, the BIC will favour smaller models than the AIC.</p>
<hr />
</div>
</div>
<div id="training-and-test-sets" class="section level2">
<h2><span class="header-section-number">7.7</span> Training and test sets</h2>
<p>Sometimes, when a large (training) dataset is available, one may decide the split the dataset randomly in a</p>
<ul>
<li><p><strong>training dataset</strong>: data are used for model fitting and for model building or feature selection (this may require e.g. cross validation)</p></li>
<li><p><strong>test dataset</strong>: this data are used to evaluate the final model (result of model building). An unbiased estimate of the outsample error (i.e. test or generalisation error) based on this test data is <span class="math display">\[
   \frac{1}{m} \sum_{i=1}^m \left(\hat{m}(\mathbf{x}_i)-Y_i\right)^2,
\]</span> where</p>
<ul>
<li><p><span class="math inline">\((Y_1,\mathbf{x}_1), \ldots, (Y_m,\mathbf{x}_m)\)</span> denote the <span class="math inline">\(m\)</span> observations in the test dataset</p></li>
<li><p><span class="math inline">\(\hat{m}\)</span> is estimated from using the training data (this may also be the result from model building, using only the training data).</p></li>
</ul></li>
</ul>
<hr />
<p>Note that the training dataset is used for model building or feature selection. This also requires the evaluation of models. For these evaluations the methods from the previous slides can be used (e.g. cross validation, <span class="math inline">\(k\)</span>-fold CV, Mallow’s <span class="math inline">\(C_p\)</span>). The test dataset is only used for the evaluation of the final model (estimated and build from using only the training data). The estimate of the outsample error based on the test dataset is the best possible estimate in the sense that it is unbiased. The observations used for this estimation are independent of the observations in the training data. However, if the number of data points in the test dataset (<span class="math inline">\(m\)</span>) is small, the estimate of the outsample error may show large variance and hence is not reliable.</p>
</div>
</div>
<div id="logistic-regression-analysis-for-high-dimensional-data" class="section level1">
<h1><span class="header-section-number">8</span> Logistic Regression Analysis for High Dimensional Data</h1>
<div id="cancer-example" class="section level2">
<h2><span class="header-section-number">8.1</span> Cancer Example</h2>
<ul>
<li><p>Schmidt <em>et al.</em>, 2008, Cancer Research, {}, 5405-5413</p></li>
<li><p>Gene expression patterns in <span class="math inline">\(n=200\)</span> breast tumors were investigated (<span class="math inline">\(p=22283\)</span> genes)</p></li>
<li><p>After surgery the tumors were graded by a pathologist (stage 1,2,3)</p></li>
<li><p>Here the objective is to predict stage 3 from the gene expression data (prediction of binary outcome)</p></li>
<li><p>If the prediction model works well, it can be used to predict the stage from a biopsy sample.</p></li>
</ul>
<hr />
</div>
<div id="logistic-regression-models" class="section level2">
<h2><span class="header-section-number">8.2</span> Logistic regression models</h2>
<p>Binary outcomes are often analysed with <strong>logistic regression models</strong>.</p>
<p>Let <span class="math inline">\(Y\)</span> denote the binary (1/0, case/control, positive/negative) outcome, and <span class="math inline">\(\mathbf{x}\)</span> the <span class="math inline">\(p\)</span>-dimensional predictor.</p>
<p>Logistic regression assumes <span class="math display">\[
   Y \mid \mathbf{x} \sim \text{Bernoulli}(\pi(\mathbf{x}))
\]</span> with <span class="math inline">\(\pi(\mathbf{x}) = \text{P}\left[Y=1\mid \mathbf{x}\right]\)</span> and <span class="math display">\[
   \ln \frac{\pi(\mathbf{x})}{1-\pi(\mathbf{x})}=\beta_0 + \boldsymbol{\beta}^T\mathbf{x}.
\]</span></p>
<p>The parameters are typically estimated by maximising the log-likelihood, which is denoted by <span class="math inline">\(l(\mathbf{ \beta})\)</span>, i.e. <span class="math display">\[
   \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}).
\]</span></p>
<ul>
<li><p>Maximum likelihood is only applicable when <span class="math inline">\(n&gt;p\)</span>.</p></li>
<li><p>When <span class="math inline">\(p&gt;n\)</span> penalised maximum likelihood methods are applicable.</p></li>
</ul>
<hr />
</div>
<div id="penalized-maximum-likelihood" class="section level2">
<h2><span class="header-section-number">8.3</span> Penalized maximum likelihood</h2>
<p>Penalised estimation methods (e.g. lasso and ridge) can als be applied to maximum likelihood, resulting in the <strong>penalised maximum likelihood estimate</strong>.</p>
<p>Lasso: <span class="math display">\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda \Vert \boldsymbol{\beta}\Vert_1.
\]</span></p>
<p>Ridge: <span class="math display">\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda \Vert \boldsymbol{\beta}\Vert_2^2.
\]</span></p>
<p>Once the parameters are estimated, the model may be used to compute <span class="math display">\[
  \hat{\pi}(\mathbf{x}) = \hat{\text{P}}\left[Y=1\mid \mathbf{x}\right].
\]</span> With these estimated probabilities the prediction rule becomes <span class="math display">\[\begin{eqnarray*}
  \hat{\pi}(\mathbf{x}) &amp;\leq c&amp; \text{predict } Y=0 \\
  \hat{\pi}(\mathbf{x}) &amp;&gt;c &amp; \text{predict } Y=1
\end{eqnarray*}\]</span> with <span class="math inline">\(0&lt;c&lt;1\)</span> a threshold that either is fixed (e.g. <span class="math inline">\(c=1/2\)</span>), depends on prior probabilities, or is empirically determined by optimising e.g. the Area Under the ROC Curve (AUC) or by finding a good compromise between sensitivity and specificity.</p>
<p>Note that logistic regression directly models the <strong>Posterior probability</strong> that an observation belongs to class <span class="math inline">\(Y=1\)</span>, given the predictor <span class="math inline">\(\mathbf{x}\)</span>.</p>
</div>
<div id="model-evaluation" class="section level2">
<h2><span class="header-section-number">8.4</span> Model evaluation</h2>
<p>Common model evaluation criteria for binary prediction models are:</p>
<ul>
<li><p>sensitivity = true positive rate (TPR)</p></li>
<li><p>specificity = true negative rate (TNR)</p></li>
<li><p>misclassification error</p></li>
<li><p>area under the ROC curve (AUC)</p></li>
</ul>
<p>These criteria can again be estimated via cross validation or via splitting of the data into training and test/validation data.</p>
<div id="sensitivity-of-a-model-pi-with-threshold-c" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Sensitivity of a model <span class="math inline">\(\pi\)</span> with threshold <span class="math inline">\(c\)</span></h3>
<p>Sensitivity is the probability to correctly predict a positive outcome: <span class="math display">\[
\text{sens}(\pi,c)=\text{P}_{X^*}\left[\hat\pi(\mathbf{X}^*)&gt;c \mid Y^*=1 \mid {\cal{T}}\right].
\]</span></p>
<p>It is also known as the true positive rate (TPR).</p>
</div>
<div id="specificity-of-a-model-pi-with-threshold-c" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Specificity of a model <span class="math inline">\(\pi\)</span> with threshold <span class="math inline">\(c\)</span></h3>
<p>Specificity is the probability to correctly predict a negative outcome: <span class="math display">\[
\text{spec}(\pi,c)=\text{P}_{X^*}\left[\hat\pi(\mathbf{X}^*)\leq c \mid Y^*=0 \mid {\cal{T}}\right].
\]</span></p>
<p>It is also known as the true negative rate (TNR).</p>
<hr />
</div>
<div id="misclassification-error-of-a-model-pi-with-threshold-c" class="section level3">
<h3><span class="header-section-number">8.4.3</span> Misclassification error of a model <span class="math inline">\(\pi\)</span> with threshold <span class="math inline">\(c\)</span></h3>
<p>The misclassification error is the probability to incorrectly predict an outcome: <span class="math display">\[\begin{eqnarray*}
\text{mce}(\pi,c) &amp;=&amp;\text{P}_{X^*,Y^*}\left[\hat\pi(\mathbf{X})\leq c \text{ and } Y^*=1 \mid {\cal{T}}\right] \\
&amp;  &amp; + \text{P}_{X^*,Y^*}\left[\hat\pi(\mathbf{X})&gt; c \text{ and } Y^*=0 \mid {\cal{T}}\right].
\end{eqnarray*}\]</span></p>
<p>Note that in the definitions of sensitivity, specificity and the misclassification error, the probabilities refer to the distribution of the <span class="math inline">\((\mathbf{X}^*,Y^*)\)</span>, which is independent of the training data, conditional on the training data. This is in line with the test or generalisation error. The misclassification error is actually the test error when a 0/1 loss function is used. Just as before, the sensitivity, specificity and the misclassification error can also be averaged over the distribution of the training data set, which is in line with the expected test error which has been discussed earlier.</p>
<hr />
</div>
<div id="roc-curve-of-a-model-pi" class="section level3">
<h3><span class="header-section-number">8.4.4</span> ROC curve of a model <span class="math inline">\(\pi\)</span></h3>
<p>The Receiver Operating Characteristic (ROC) curve for model <span class="math inline">\(\pi\)</span> is given by the function</p>
<p><span class="math display">\[
\text{ROC}: [0,1] \rightarrow [0,1]\times [0,1]: c \mapsto (1-\text{spec}(\pi,c), \text{sens}(\pi,c)).
\]</span></p>
<p>For when <span class="math inline">\(c\)</span> moves from 1 to 0, the ROC function defines a curve in the plane <span class="math inline">\([0,1]\times [0,1]\)</span>, moving from <span class="math inline">\((0,0)\)</span> for <span class="math inline">\(c=1\)</span> to <span class="math inline">\((1,1)\)</span> for <span class="math inline">\(c=0\)</span>.</p>
<p>The horizontal axis of the ROC curve shows 1-specificity. This is also known as the False Positive Rate (FPR).</p>
<hr />
</div>
<div id="area-under-the-curve-auc-of-a-model-pi" class="section level3">
<h3><span class="header-section-number">8.4.5</span> Area under the curve (AUC) of a model <span class="math inline">\(\pi\)</span></h3>
<p>The area under the curve (AUC) for model <span class="math inline">\(\pi\)</span> is area under the ROC curve and is given by <span class="math display">\[
\int_0^1 \text{ROC}(c) dc.
\]</span></p>
<p>Some notes about the AUC:</p>
<ul>
<li><p>AUC=0.5 results when the ROC curve is the diagonal. This corresponds to flipping a coin, i.e. a complete random prediction.</p></li>
<li><p>AUC=1 results from the perfect ROC curve, which is the ROC curve through the points <span class="math inline">\((0,0)\)</span>, <span class="math inline">\((0,1)\)</span> and <span class="math inline">\((1,1)\)</span>. This ROC curve includes a threshold <span class="math inline">\(c\)</span> such that sensitivity and specificity are equal to one.</p></li>
</ul>
</div>
</div>
<div id="breast-cancer-example" class="section level2">
<h2><span class="header-section-number">8.5</span> Breast cancer example</h2>
<div id="data-1" class="section level3">
<h3><span class="header-section-number">8.5.1</span> Data</h3>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1"></a><span class="kw">library</span>(glmnet)</span>
<span id="cb64-2"><a href="#cb64-2"></a></span>
<span id="cb64-3"><a href="#cb64-3"></a><span class="co">#BiocManager::install(&quot;genefu&quot;)</span></span>
<span id="cb64-4"><a href="#cb64-4"></a><span class="co">#BiocManager::install(&quot;breastCancerMAINZ&quot;)</span></span>
<span id="cb64-5"><a href="#cb64-5"></a></span>
<span id="cb64-6"><a href="#cb64-6"></a><span class="kw">library</span>(genefu)</span>
<span id="cb64-7"><a href="#cb64-7"></a><span class="kw">library</span>(breastCancerMAINZ)</span>
<span id="cb64-8"><a href="#cb64-8"></a><span class="kw">data</span>(mainz)</span>
<span id="cb64-9"><a href="#cb64-9"></a></span>
<span id="cb64-10"><a href="#cb64-10"></a>X &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">exprs</span>(mainz)) <span class="co"># gene expressions</span></span>
<span id="cb64-11"><a href="#cb64-11"></a>n &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span id="cb64-12"><a href="#cb64-12"></a>H &lt;-<span class="st"> </span><span class="kw">diag</span>(n)<span class="op">-</span><span class="dv">1</span><span class="op">/</span>n<span class="op">*</span><span class="kw">matrix</span>(<span class="dv">1</span>,<span class="dt">ncol=</span>n,<span class="dt">nrow=</span>n)</span>
<span id="cb64-13"><a href="#cb64-13"></a>X &lt;-<span class="st"> </span>H<span class="op">%*%</span>X</span>
<span id="cb64-14"><a href="#cb64-14"></a>Y &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">pData</span>(mainz)<span class="op">$</span>grade<span class="op">==</span><span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb64-15"><a href="#cb64-15"></a><span class="kw">table</span>(Y)</span></code></pre></div>
<pre><code>## Y
##   0   1 
## 165  35</code></pre>
<hr />
<p>From the table of the outcomes in Y we read that</p>
<ul>
<li>35 tumors were graded as stage 3 and</li>
<li>165 tumors were graded as stage 1 or 2.</li>
</ul>
<p>In this the stage 3 tumors are referred to as cases or postives and the stage 1 and 2 tumors as controls or negatives.</p>
<hr />
</div>
<div id="training-and-test-dataset" class="section level3">
<h3><span class="header-section-number">8.5.2</span> Training and test dataset</h3>
<p>The use of the lasso logistic regression for the prediction of stage 3 breast cancer is illustrated here by</p>
<ul>
<li><p>randomly splitting the dataset into a training dataset (<span class="math inline">\(80\%\)</span> of data = 160 tumors) and a test dataset (40 tumors)</p></li>
<li><p>using the training data to select a good <span class="math inline">\(\lambda\)</span> value in the lasso logistic regression model (through 10-fold CV)</p></li>
<li><p>evaluating the final model by means of the test dataset (ROC Curve, AUC).</p></li>
</ul>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1"></a><span class="co">## Used to provide same results as in previous R version</span></span>
<span id="cb66-2"><a href="#cb66-2"></a><span class="kw">RNGkind</span>(<span class="dt">sample.kind =</span> <span class="st">&quot;Rounding&quot;</span>)</span></code></pre></div>
<pre><code>## Warning in RNGkind(sample.kind = &quot;Rounding&quot;): non-uniform &#39;Rounding&#39; sampler
## used</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1"></a><span class="kw">set.seed</span>(<span class="dv">6977326</span>)</span>
<span id="cb68-2"><a href="#cb68-2"></a><span class="co">####</span></span>
<span id="cb68-3"><a href="#cb68-3"></a></span>
<span id="cb68-4"><a href="#cb68-4"></a>n &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span id="cb68-5"><a href="#cb68-5"></a>nTrain &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="fl">0.8</span><span class="op">*</span>n)</span>
<span id="cb68-6"><a href="#cb68-6"></a>nTrain</span></code></pre></div>
<pre><code>## [1] 160</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1"></a>indTrain &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>n,nTrain)</span>
<span id="cb70-2"><a href="#cb70-2"></a>XTrain &lt;-<span class="st"> </span>X[indTrain,]</span>
<span id="cb70-3"><a href="#cb70-3"></a>YTrain &lt;-<span class="st"> </span>Y[indTrain]</span>
<span id="cb70-4"><a href="#cb70-4"></a>XTest &lt;-<span class="st"> </span>X[<span class="op">-</span>indTrain,]</span>
<span id="cb70-5"><a href="#cb70-5"></a>YTest &lt;-<span class="st"> </span>Y[<span class="op">-</span>indTrain]</span>
<span id="cb70-6"><a href="#cb70-6"></a><span class="kw">table</span>(YTest)</span></code></pre></div>
<pre><code>## YTest
##  0  1 
## 32  8</code></pre>
<p>Note that the randomly selected test data has 20% cases of stage 3 tumors. This is a bit higher than the 17.5% in the complete data.</p>
<p>One could also perform the random splitting among the positives and the negatives separately (stratified splitting).</p>
</div>
<div id="model-fitting-based-on-training-data" class="section level3">
<h3><span class="header-section-number">8.5.3</span> Model fitting based on training data</h3>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1"></a>mLasso &lt;-<span class="st"> </span><span class="kw">glmnet</span>(</span>
<span id="cb72-2"><a href="#cb72-2"></a>  <span class="dt">x =</span> XTrain,</span>
<span id="cb72-3"><a href="#cb72-3"></a>  <span class="dt">y =</span> YTrain,</span>
<span id="cb72-4"><a href="#cb72-4"></a>  <span class="dt">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb72-5"><a href="#cb72-5"></a>  <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)  <span class="co"># lasso: alpha = 1</span></span>
<span id="cb72-6"><a href="#cb72-6"></a></span>
<span id="cb72-7"><a href="#cb72-7"></a><span class="kw">plot</span>(mLasso, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">6</span>,<span class="op">-</span><span class="fl">1.5</span>))</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<hr />
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1"></a>mCvLasso &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(</span>
<span id="cb73-2"><a href="#cb73-2"></a>  <span class="dt">x =</span> XTrain,</span>
<span id="cb73-3"><a href="#cb73-3"></a>  <span class="dt">y =</span> YTrain,</span>
<span id="cb73-4"><a href="#cb73-4"></a>  <span class="dt">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb73-5"><a href="#cb73-5"></a>  <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb73-6"><a href="#cb73-6"></a>    <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)  <span class="co"># lasso alpha = 1</span></span>
<span id="cb73-7"><a href="#cb73-7"></a></span>
<span id="cb73-8"><a href="#cb73-8"></a><span class="kw">plot</span>(mCvLasso)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1"></a>mCvLasso</span></code></pre></div>
<pre><code>## 
## Call:  cv.glmnet(x = XTrain, y = YTrain, type.measure = &quot;class&quot;, alpha = 1,      family = &quot;binomial&quot;) 
## 
## Measure: Misclassification Error 
## 
##     Lambda Measure      SE Nonzero
## min 0.1044  0.1437 0.03366      18
## 1se 0.1911  0.1688 0.03492       0</code></pre>
<p>The total misclassification error is used here to select a good value for <span class="math inline">\(\lambda\)</span>.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1"></a><span class="co"># BiocManager::install(&quot;plotROC&quot;)</span></span>
<span id="cb76-2"><a href="#cb76-2"></a><span class="kw">library</span>(plotROC)</span>
<span id="cb76-3"><a href="#cb76-3"></a></span>
<span id="cb76-4"><a href="#cb76-4"></a>dfLassoOpt &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb76-5"><a href="#cb76-5"></a>  <span class="dt">pi =</span> <span class="kw">predict</span>(mCvLasso,</span>
<span id="cb76-6"><a href="#cb76-6"></a>    <span class="dt">newx =</span> XTest,</span>
<span id="cb76-7"><a href="#cb76-7"></a>    <span class="dt">s =</span> mCvLasso<span class="op">$</span>lambda.min,</span>
<span id="cb76-8"><a href="#cb76-8"></a>    <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">c</span>(.),</span>
<span id="cb76-9"><a href="#cb76-9"></a>  <span class="dt">known.truth =</span> YTest)</span>
<span id="cb76-10"><a href="#cb76-10"></a></span>
<span id="cb76-11"><a href="#cb76-11"></a>roc &lt;-</span>
<span id="cb76-12"><a href="#cb76-12"></a><span class="st">  </span>dfLassoOpt  <span class="op">%&gt;%</span></span>
<span id="cb76-13"><a href="#cb76-13"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">d =</span> known.truth, <span class="dt">m =</span> pi)) <span class="op">+</span></span>
<span id="cb76-14"><a href="#cb76-14"></a><span class="st">  </span><span class="kw">geom_roc</span>(<span class="dt">n.cuts =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span id="cb76-15"><a href="#cb76-15"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;1-specificity (FPR)&quot;</span>) <span class="op">+</span></span>
<span id="cb76-16"><a href="#cb76-16"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;sensitivity (TPR)&quot;</span>)</span>
<span id="cb76-17"><a href="#cb76-17"></a></span>
<span id="cb76-18"><a href="#cb76-18"></a>roc</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1"></a><span class="kw">calc_auc</span>(roc)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["PANEL"],"name":[1],"type":["fct"],"align":["left"]},{"label":["group"],"name":[2],"type":["int"],"align":["right"]},{"label":["AUC"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"-1","3":"0.8320312"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<ul>
<li><p>The ROC curve is shown for the model based on <span class="math inline">\(\lambda\)</span> with the smallest misclassification error. The model has an AUC of 0.83.</p></li>
<li><p>Based on this ROC curve an appropriate threshold <span class="math inline">\(c\)</span> can be chosen. For example, from the ROC curve we see that it is possible to attain a specificity and a sensitivity of 75%.</p></li>
<li><p>The sensitivities and specificities in the ROC curve are unbiased (independent test dataset) for the prediction model build from the training data. The estimates of sensitivity and specificity, however, are based on only 40 observations.</p></li>
</ul>
<hr />
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1"></a>mLambdaOpt &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x =</span> XTrain,</span>
<span id="cb78-2"><a href="#cb78-2"></a>  <span class="dt">y =</span> YTrain,</span>
<span id="cb78-3"><a href="#cb78-3"></a>  <span class="dt">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb78-4"><a href="#cb78-4"></a>  <span class="dt">lambda =</span> mCvLasso<span class="op">$</span>lambda.min,</span>
<span id="cb78-5"><a href="#cb78-5"></a>  <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb78-6"><a href="#cb78-6"></a></span>
<span id="cb78-7"><a href="#cb78-7"></a><span class="kw">qplot</span>(</span>
<span id="cb78-8"><a href="#cb78-8"></a>  <span class="kw">summary</span>(<span class="kw">coef</span>(mLambdaOpt))[<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb78-9"><a href="#cb78-9"></a>  <span class="kw">summary</span>(<span class="kw">coef</span>(mLambdaOpt))[<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>]) <span class="op">+</span></span>
<span id="cb78-10"><a href="#cb78-10"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;gene ID&quot;</span>) <span class="op">+</span></span>
<span id="cb78-11"><a href="#cb78-11"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;beta-hat&quot;</span>) <span class="op">+</span></span>
<span id="cb78-12"><a href="#cb78-12"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<ul>
<li>The model with the optimal <span class="math inline">\(\lambda\)</span> has only 19 non-zero parameter estimates.</li>
<li>Thus only 19 genes are involved in the prediction model.</li>
<li>These 19 parameter estimates are plotting in the graph. A listing of the model output would show the names of the genes.</li>
</ul>
<hr />
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1"></a>dfLasso1se &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb79-2"><a href="#cb79-2"></a>  <span class="dt">pi =</span> <span class="kw">predict</span>(mCvLasso,</span>
<span id="cb79-3"><a href="#cb79-3"></a>    <span class="dt">newx =</span> XTest,</span>
<span id="cb79-4"><a href="#cb79-4"></a>    <span class="dt">s =</span> mCvLasso<span class="op">$</span>lambda<span class="fl">.1</span>se,</span>
<span id="cb79-5"><a href="#cb79-5"></a>    <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">c</span>(.),</span>
<span id="cb79-6"><a href="#cb79-6"></a>  <span class="dt">known.truth =</span> YTest)</span>
<span id="cb79-7"><a href="#cb79-7"></a></span>
<span id="cb79-8"><a href="#cb79-8"></a>roc &lt;-</span>
<span id="cb79-9"><a href="#cb79-9"></a><span class="st">  </span><span class="kw">rbind</span>(</span>
<span id="cb79-10"><a href="#cb79-10"></a>    dfLassoOpt <span class="op">%&gt;%</span></span>
<span id="cb79-11"><a href="#cb79-11"></a><span class="st">      </span><span class="kw">mutate</span>(<span class="dt">method =</span> <span class="st">&quot;min&quot;</span>),</span>
<span id="cb79-12"><a href="#cb79-12"></a>    dfLasso1se <span class="op">%&gt;%</span></span>
<span id="cb79-13"><a href="#cb79-13"></a><span class="st">      </span><span class="kw">mutate</span>(<span class="dt">method =</span> <span class="st">&quot;1se&quot;</span>)</span>
<span id="cb79-14"><a href="#cb79-14"></a>  ) <span class="op">%&gt;%</span></span>
<span id="cb79-15"><a href="#cb79-15"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">d =</span> known.truth, <span class="dt">m =</span> pi, <span class="dt">color =</span> method)) <span class="op">+</span></span>
<span id="cb79-16"><a href="#cb79-16"></a><span class="st">  </span><span class="kw">geom_roc</span>(<span class="dt">n.cuts =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span id="cb79-17"><a href="#cb79-17"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;1-specificity (FPR)&quot;</span>) <span class="op">+</span></span>
<span id="cb79-18"><a href="#cb79-18"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;sensitivity (TPR)&quot;</span>)</span>
<span id="cb79-19"><a href="#cb79-19"></a></span>
<span id="cb79-20"><a href="#cb79-20"></a>roc</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1"></a><span class="kw">calc_auc</span>(roc)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["PANEL"],"name":[1],"type":["fct"],"align":["left"]},{"label":["group"],"name":[2],"type":["int"],"align":["right"]},{"label":["AUC"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1","3":"0.5000000"},{"1":"1","2":"2","3":"0.8320312"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<ul>
<li><p>When using the <span class="math inline">\(\lambda\)</span> of the optimal model up to 1 standard deviation, a diagonal ROC curve is obtained and hence AUC is <span class="math inline">\(0.5\)</span>.</p></li>
<li><p>This prediction model is thus equivalent to flipping a coin for making the prediction.</p></li>
<li><p>The reason is that with this choice of <span class="math inline">\(\lambda\)</span> (strong penalisation) almost all predictors are removed from the model.</p></li>
<li><p>Therefore, do never blindly choose for the ``optimal’’ <span class="math inline">\(\lambda\)</span> as defined here, but assess the performance of the model first.</p></li>
</ul>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1"></a>mLambda1se &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x =</span> XTrain,</span>
<span id="cb81-2"><a href="#cb81-2"></a>  <span class="dt">y =</span> YTrain,</span>
<span id="cb81-3"><a href="#cb81-3"></a>  <span class="dt">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb81-4"><a href="#cb81-4"></a>  <span class="dt">lambda =</span> mCvLasso<span class="op">$</span>lambda<span class="fl">.1</span>se,</span>
<span id="cb81-5"><a href="#cb81-5"></a>  <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb81-6"><a href="#cb81-6"></a></span>
<span id="cb81-7"><a href="#cb81-7"></a>mLambda1se <span class="op">%&gt;%</span></span>
<span id="cb81-8"><a href="#cb81-8"></a><span class="st">  </span>coef <span class="op">%&gt;%</span></span>
<span id="cb81-9"><a href="#cb81-9"></a><span class="st">  </span>summary</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["i"],"name":[1],"type":["int"],"align":["right"]},{"label":["j"],"name":[2],"type":["int"],"align":["right"]},{"label":["x"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1","3":"-1.594512"},{"1":"2","2":"1","3":"0.000000"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<hr />
</div>
</div>
<div id="the-elastic-net" class="section level2">
<h2><span class="header-section-number">8.6</span> The Elastic Net</h2>
<p>The lasso and ridge regression have positive and negative properties.</p>
<ul>
<li><p>Lasso</p>
<ul>
<li><p>positive: sparse solution</p></li>
<li><p>negative: at most <span class="math inline">\(\min(n,p)\)</span> predictors can be selected</p></li>
<li><p>negative: tend to select one predictor among a group of highly correlated predictors</p></li>
</ul></li>
<li><p>Ridge</p>
<ul>
<li>negative: no sparse solution</li>
<li>positive: more than <span class="math inline">\(\min(n,p)\)</span> predictors can be selected</li>
</ul></li>
</ul>
<p>A compromise between lasso and ridge: the <strong>elastic net</strong>: <span class="math display">\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\gamma_1 \vert \boldsymbol\beta\vert -\gamma_2 \Vert \boldsymbol\beta\Vert_2^2.
\]</span></p>
<p>The elastic gives a sparse solution with potentially more than <span class="math inline">\(\min(n,p)\)</span> predictors.</p>
<hr />
<p>The <code>glmnet</code> R function uses the following parameterisation, <span class="math display">\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda\alpha \Vert \boldsymbol\beta\Vert_0 -\lambda(1-\alpha) \Vert \boldsymbol\beta\Vert_2^2.
\]</span></p>
<ul>
<li><p><span class="math inline">\(\alpha\)</span> parameter gives weight to <span class="math inline">\(L_1\)</span> penalty term (hence <span class="math inline">\(\alpha=1\)</span> gives the lasso, and <span class="math inline">\(\alpha=0\)</span> gives ridge).</p></li>
<li><p>a <span class="math inline">\(\lambda\)</span> parameter to give weight to the penalisation</p></li>
<li><p>Note that the combination of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\alpha\)</span> gives the same flexibility as the combination of the parameters <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>.</p></li>
</ul>
<hr />
<div id="breast-cancer-example-1" class="section level3">
<h3><span class="header-section-number">8.6.1</span> Breast cancer example</h3>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1"></a>mElastic &lt;-<span class="st"> </span><span class="kw">glmnet</span>(</span>
<span id="cb82-2"><a href="#cb82-2"></a>  <span class="dt">x =</span> XTrain,</span>
<span id="cb82-3"><a href="#cb82-3"></a>  <span class="dt">y =</span> YTrain,</span>
<span id="cb82-4"><a href="#cb82-4"></a>  <span class="dt">alpha =</span> <span class="fl">0.5</span>,</span>
<span id="cb82-5"><a href="#cb82-5"></a>  <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)  <span class="co"># elastic net</span></span>
<span id="cb82-6"><a href="#cb82-6"></a></span>
<span id="cb82-7"><a href="#cb82-7"></a><span class="kw">plot</span>(mElastic, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">5.5</span>,<span class="op">-</span><span class="dv">1</span>))</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1"></a>mCvElastic &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="dt">x =</span> XTrain,</span>
<span id="cb83-2"><a href="#cb83-2"></a>  <span class="dt">y =</span> YTrain,</span>
<span id="cb83-3"><a href="#cb83-3"></a>  <span class="dt">alpha =</span> <span class="fl">0.5</span>,</span>
<span id="cb83-4"><a href="#cb83-4"></a>  <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>,</span>
<span id="cb83-5"><a href="#cb83-5"></a>    <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>)  <span class="co"># elastic net</span></span>
<span id="cb83-6"><a href="#cb83-6"></a></span>
<span id="cb83-7"><a href="#cb83-7"></a><span class="kw">plot</span>(mCvElastic)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1"></a>mCvElastic</span></code></pre></div>
<pre><code>## 
## Call:  cv.glmnet(x = XTrain, y = YTrain, type.measure = &quot;class&quot;, alpha = 0.5,      family = &quot;binomial&quot;) 
## 
## Measure: Misclassification Error 
## 
##      Lambda Measure      SE Nonzero
## min 0.01859  0.1313 0.02708     148
## 1se 0.21876  0.1562 0.03391      26</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1"></a>dfElast &lt;-<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb86-2"><a href="#cb86-2"></a>  <span class="dt">pi =</span> <span class="kw">predict</span>(mElastic,</span>
<span id="cb86-3"><a href="#cb86-3"></a>    <span class="dt">newx =</span> XTest,</span>
<span id="cb86-4"><a href="#cb86-4"></a>    <span class="dt">s =</span> mCvElastic<span class="op">$</span>lambda.min,</span>
<span id="cb86-5"><a href="#cb86-5"></a>    <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">c</span>(.),</span>
<span id="cb86-6"><a href="#cb86-6"></a>  <span class="dt">known.truth =</span> YTest)</span>
<span id="cb86-7"><a href="#cb86-7"></a></span>
<span id="cb86-8"><a href="#cb86-8"></a>roc &lt;-<span class="st"> </span><span class="kw">rbind</span>(</span>
<span id="cb86-9"><a href="#cb86-9"></a>  dfLassoOpt <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">method =</span> <span class="st">&quot;lasso&quot;</span>),</span>
<span id="cb86-10"><a href="#cb86-10"></a>  dfElast <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">method =</span> <span class="st">&quot;elast. net&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb86-11"><a href="#cb86-11"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">d =</span> known.truth, <span class="dt">m =</span> pi, <span class="dt">color =</span> method)) <span class="op">+</span></span>
<span id="cb86-12"><a href="#cb86-12"></a><span class="st">  </span><span class="kw">geom_roc</span>(<span class="dt">n.cuts =</span> <span class="dv">0</span>) <span class="op">+</span></span>
<span id="cb86-13"><a href="#cb86-13"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;1-specificity (FPR)&quot;</span>) <span class="op">+</span></span>
<span id="cb86-14"><a href="#cb86-14"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;sensitivity (TPR)&quot;</span>)</span>
<span id="cb86-15"><a href="#cb86-15"></a></span>
<span id="cb86-16"><a href="#cb86-16"></a>roc</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1"></a><span class="kw">calc_auc</span>(roc)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["PANEL"],"name":[1],"type":["fct"],"align":["left"]},{"label":["group"],"name":[2],"type":["int"],"align":["right"]},{"label":["AUC"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1","3":"0.8398438"},{"1":"1","2":"2","3":"0.8320312"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<ul>
<li>More parameters are used than for the lasso, but the performance does not improve.</li>
</ul>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1"></a>mElasticOpt &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x =</span> XTrain,</span>
<span id="cb88-2"><a href="#cb88-2"></a>  <span class="dt">y =</span> YTrain,</span>
<span id="cb88-3"><a href="#cb88-3"></a>  <span class="dt">alpha =</span> <span class="fl">0.5</span>,</span>
<span id="cb88-4"><a href="#cb88-4"></a>  <span class="dt">lambda =</span> mCvElastic<span class="op">$</span>lambda.min,</span>
<span id="cb88-5"><a href="#cb88-5"></a>  <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb88-6"><a href="#cb88-6"></a></span>
<span id="cb88-7"><a href="#cb88-7"></a><span class="kw">qplot</span>(</span>
<span id="cb88-8"><a href="#cb88-8"></a>  <span class="kw">summary</span>(<span class="kw">coef</span>(mElasticOpt))[<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb88-9"><a href="#cb88-9"></a>  <span class="kw">summary</span>(<span class="kw">coef</span>(mElasticOpt))[<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>]) <span class="op">+</span></span>
<span id="cb88-10"><a href="#cb88-10"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;gene ID&quot;</span>) <span class="op">+</span></span>
<span id="cb88-11"><a href="#cb88-11"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;beta-hat&quot;</span>) <span class="op">+</span></span>
<span id="cb88-12"><a href="#cb88-12"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="prediction_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
</div>
</div>
</div>

<div id="rmd-source-code">---
title: "3. Prediction with High Dimensional Predictors"
author: "Lieven Clement"
date: "statOmics, Ghent University (https://statomics.github.io)"
output:
    html_document:
      code_download: true
      theme: cosmo
      toc: true
      toc_float: true
      highlight: tango
      number_sections: true
---

```{r echo=FALSE, message= FALSE}
library(tidyverse)
library(gridExtra)
```

# Introduction

## Prediction with High Dimensional Predictors

General setting:

-   Aim: build a **prediction model** that gives a prediction of an outcome for a given set of predictors.

- We use $X$ to refer to the predictors and $Y$ to refer to the outcome.


- A **training data set** is available, say $(\mathbf{X},\mathbf{Y})$. It contains $n$ observations on outcomes and on $p$ predictors.

- Using the training data, a prediction model is build, say $\hat{m}(\mathbf{X})$. This typically involves **model building (feature selection)** and parameter estimation.


-   During the model building, potential **models need to be evaluated** in terms of their prediction quality.

## Example: Toxicogenomics in early drug development

### Background

- Effect of compound on gene expression.

- Insight in action and toxicity of drug in early phase
- Determine activity with bio-assay: e.g. binding affinity of compound to cell wall receptor (target, IC50).
- Early phase:  20 to 50 compounds
- Based on in vitro results one aims to get insight in how to build better compound (higher on-target activity less toxicity.
- Small variations in molecular structure lead to variations in BA and gene expression.
- Aim: Build model to predict bio-activity based on gene expression in liver cell line.

### Data

- 30 chemical compounds have been screened for toxicity

- Bioassay data on toxicity screening

- Gene expressions in a liver cell line are profiled for each compound (4000 genes)


```{r}
toxData <- read_csv("https://raw.githubusercontent.com/statOmics/HDA2020/data/toxDataCentered.csv")
svdX <- svd(toxData[,-1])
```

Data is already centered:

```{r}
toxData %>%
  colMeans %>%
  range
```

```{r}
 toxData %>%
  names %>%
  head
```

- First column contains data on Bioassay.
- The higher the score on Bioassay the more toxic the compound
- Other columns contain data on gene expression X1, ... , X4000

### Data exploration

```{r}
toxData %>%
  ggplot(aes(x="",y=BA)) +
  geom_boxplot(outlier.shape=NA) +
  geom_point(position="jitter")
```

```{r}
svdX <- toxData[,-1] %>%
  svd

k <- 2
Vk <- svdX$v[,1:k]
Uk <- svdX$u[,1:k]
Dk <- diag(svdX$d[1:k])
Zk <- Uk%*%Dk
colnames(Zk) <- paste0("Z",1:k)
colnames(Vk) <- paste0("V",1:k)

Zk %>%
  as.data.frame %>%
  mutate(BA = toxData %>% pull(BA)) %>%
  ggplot(aes(x= Z1, y = Z2, color = BA)) +
  geom_point(size = 3) +
  scale_colour_gradient2(low = "blue",mid="white",high="red") +
  geom_point(size = 3, pch = 21, color = "black")
```

- Scores on the first two principal components (or MDS plot).
- Each point corresponds to a compound.
- Color code refers to the toxicity score (higher score more toxic).
- Clear separation between compounds according to toxicity.

---

- Next logic step in a PCA is to interpret the principal components.
- We thus have to assess the loadings.
- We can add a vector for each gene to get a biplot, but this would require plotting 4000 vectors, which would render the plot unreadable.

Alternative graph to look at the many loadings of the first two PCs.

```{r}
grid.arrange(
  Vk %>%
    as.data.frame %>%
    mutate(geneID = 1:nrow(Vk)) %>%
    ggplot(aes(x = geneID, y = V1)) +
    geom_point(pch=21) +
    geom_hline(yintercept = c(-2,0,2)*sd(Vk[,1]), col = "red") ,
  Vk %>%
    as.data.frame %>%
    mutate(geneID = 1:nrow(Vk)) %>%
    ggplot(aes(x = geneID, y = V2)) +
    geom_point(pch=21) +
    geom_hline(yintercept = c(-2,0,2)*sd(Vk[,2]), col = "red"),
  ncol=2)
```

- It is almost impossible to interpret the PCs because there are 4000 genes contributing to each PC.

- In an attempt to find the most important genes (in the sense that they drive the interpretation of the PCs), the plots show horizontal reference lines: the average of the loadings, and the average ± twice the standard deviation of the loadings. In between the lines we expects about 95% of the loadings (if they were normally distributed).

- The points outside the band come from the genes that have rather large loadings (in absolute value) and hence are important for the interpretation of the PCs.

- Note, that particularly for the first PC, only a few genes show a markedly large loadings that are negative. This means that an upregulation of these genes will lead to low scores on PC1.
- These genes will very likely play an important role in the toxicity mechanism.
- Indeed, low scores on PC1 are in the direction of more toxicity.
- In the next chapter we will introduce a method to obtain sparse PCs.

### Prediction model

```{r}
m1 <- lm(BA ~ -1 + ., toxData)

m1 %>%
  coef %>%
  head(40)

m1 %>%
  coef %>%
  is.na %>%
  sum

summary(m1)$r.squared
```

Problem??

## Brain example

- Courtesy to Solomon Kurz. Statistical rethinking with brms, ggplot2, and the tidyverse version 1.2.0.

https://bookdown.org/content/3890/
https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse

- Data with brain size and body size for seven species

```{r}
brain <-
tibble(species = c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens"),
       brain   = c(438, 452, 612, 521, 752, 871, 1350),
       mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))
```

### Data exploration

```{r}
brain

p <- brain %>%
  ggplot(aes(x =  mass, y = brain, label = species)) +
  geom_point()

p + geom_text(nudge_y = 40)
```

### Models

Six models range in complexity from the simple univariate model

\begin{align*}
\text{brain}_i & \sim \operatorname{Normal} (\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_1 \text{mass}_i,
\end{align*}

to the dizzying sixth-degree polynomial model

\begin{align*}
\text{brain}_i & \sim \operatorname{Normal} (\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_1 \text{mass}_i + \beta_2 \text{mass}_i^2 + \beta_3 \text{mass}_i^3 + \beta_4 \text{mass}_i^4 + \beta_5 \text{mass}_i^5 + \beta_6 \text{mass}_i^6.
\end{align*}

```{r, message = F, warning = F}
formulas <- sapply(1:6, function(i)
  return(
     paste0("I(mass^",1:i,")") %>% paste(collapse=" + ")
    )  
)

formulas <- sapply(
  paste0("brain ~ ", formulas),
  as.formula)

models <- lapply(formulas, lm , data = brain)
```

```{r}
data.frame(
  formula=formulas %>%
    as.character,
  r2 = sapply(
    models,
    function(mod) summary(mod)$r.squared)
  )  %>%
  ggplot(
    aes(x = r2,
      y = formula,
      label = r2 %>%
        round(2) %>%
        as.character)
  ) +
  geom_text()
```

We plot the fit for each model individually and them arrange them together in one plot.

```{r}
plots <- lapply(1:6, function(i)
{
  p +
  geom_smooth(method = "lm", formula = y ~ poly(x,i)) +
  ggtitle(
    paste0(
      "r2 = ",
      round(summary(models[[i]])$r.squared*100,1),
      "%")
    )
})

do.call("grid.arrange",c(plots, ncol = 3))
```

- We clearly see that increasing the model complexity always produces a fit with a smaller SSE.
- The problem of overfitting is very obvious. The more complex polynomial models will not generalise well for prediction!
- We even have a model that fits the data perfectly, but that will make very absurd preditions!

- Too few parameters hurts, too. Fit the underfit intercept-only model.

```{r}
m0 <- lm(brain ~ 1, brain)
summary(m0)

p +
  stat_smooth(method = "lm", formula = y ~ 1) +
  ggtitle(
    paste0(
      "r2 = ",
      round(summary(m0)$r.squared*100,1),
      "%")
    )
```

The underfit model did not learn anything about the relation between mass and brain. It would also do a very poor job for predicting new data.

## Overview

We will make a distinction between continuous and discrete outcomes. In this course we focus on

- Linear regression models for continous outcomes

  - Penalised regression: Lasso and ridge
  - Principal component regression (PCR)

- Logistic regression models for binary outcomes

  - Penalised regression: Lasso and ridge

For all types of model, we will discuss feature selection methods.

# Linear Regression for High Dimensional Data

Consider linear regression model (for double centered data)
\[
  Y_i = \beta_1X_{i1} + \beta_2 X_{i2} + \cdots + \beta_pX_{ip} + \epsilon_i ,
\]
with $\text{E}\left[\epsilon \mid \mathbf{X}\right]=0$ and $\text{var}\left[\epsilon \mid \mathbf{X}\right]=\sigma^2$.

In matrix notation the model becomes
\[
  \mathbf{Y} = \mathbf{X}\mathbf\beta + \mathbf\epsilon.
\]
The least squares estimator of $\mathbf\beta$ is given by
\[
  \hat{\mathbf\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} ,
\]
and the variance of $\hat{\mathbf\beta}$ equals
\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X}^T\mathbf{X})^{-1}\sigma^2.
\]
$\longrightarrow$ the $p \times p$ matrix $(\mathbf{X}^T\mathbf{X})^{-1}$ is crucial

Note, that

- with double centered data it is meant that both the responses are centered (mean of $\mathbf{Y}$ is zero) and that all predictors are centered (columns of $\mathbf{X}$ have zero mean). With double centered data the intercept in a linear regression model is always exactly equal to zero and hence the intercept must not be included in the model.

- we do not assume that the residuals are normally distributed. For prediction purposes this is often not required (normality is particularly important for statistical inference in small samples).

## Linear Regression for multivariate data vs High Dimensional Data

- $\mathbf{X^TX}$ and $(\mathbf{X^TX})^{-1}$ are $p \times p$ matrices

- $\mathbf{X^TX}$ can only be inverted if it has rank $p$

- Rank of a matrix of form $\mathbf{X^TX}$, with $\mathbf{X}$ and $n\times p$ matrix, can never be larger than $\min(n,p)$.

- in most regression problems $n>p$ and rank of $(\mathbf{X^TX})$ equals $p$

- in high dimensional regression problems $p >>> n$ and rank of $(\mathbf{X^TX})$ equals $n<p$

- in the toxicogenomics example $n=30<p=4000$ and $\text{rank}(\mathbf{X^TX})\leq n=30$.
  $\longrightarrow$ $(\mathbf{X^TX})^{-1}$ does not exist, and neither does $\hat{\boldsymbol{\beta}}$.

## Can SVD help?
  - Since the columns of $\mathbf{X}$ are centered, $\mathbf{X^TX} \propto \text{var}\left[\mathbf{X}\right]$.

  - if $\text{rank}(\mathbf{X^TX})=n=30$, the PCA will give 30 components, each being a linear combination of $p=4000$ variables. These 30 PCs contain all information present in the original $\mathbf{X}$ data.

  - if $\text{rank}(\mathbf{X})=n=30$, the SVD of $\mathbf{X}$ is given by
  \[
   \mathbf{X} = \sum_{i=1}^n \delta_i \mathbf{u}_i \mathbf{v}_i^T = \mathbf{U} \boldsymbol{\Delta} \mathbf{V}^T = \mathbf{ZV}^T,
  \]
  with $\mathbf{Z}$ the $n\times n$ matrix with the scores on the $n$ PCs.

  - Still problematic because if we use all PCs $n=p$.


# Principal Component Regression

A principal component regression (PCR) consists of

1. transforming $p=4000$ dimensional $X$-variable to the $n=30$ dimensional $Z$-variable (PC scores). The $n$ PCs are mutually uncorrelated.

2. using the $n$ PC-variables as regressors in a linear regression model

3. performing feature selection to select the most important regressors (PC).

Feature selection is key, because we don't want to have as many regressors as there are observations in the data. This would result in zero residual degrees of freedom. (see later)

---

To keep the exposition general so that we allow for a feature selection to have taken place, I use the notation $\mathbf{U}_S$ to denote a matrix with left-singular column vectors $\mathbf{u}_i$, with $i \in {\cal{S}}$ (${\cal{S}}$ an index set referring to the PCs to be included in the regression model).

For example, suppose that a feature selection method has resulted in the selection of PCs 1, 3 and 12 for inclusion in the prediction model, then ${\cal{S}}=\{1,3,12\}$ and
\[
 \mathbf{U}_S = \begin{pmatrix}
  \mathbf{u}_1 & \mathbf{u}_3 & \mathbf{u}_{12}
 \end{pmatrix}.
\]

---

### Example model based on first 4 PCs

```{r}
k <- 30
Uk <- svdX$u[,1:k]
Dk <- diag(svdX$d[1:k])
Zk <- Uk%*%Dk
Y <- toxData %>%
  pull(BA)

m4 <- lm(Y~Zk[,1:4])
summary(m4)
```

Note:

- the intercept is estimated as zero. (Why?) The model could have been fitted as

```
m4 <- lm(Y~-1+Zk[,1:4])
```

- the PC-predictors are uncorrelated (by construction)

- first PC-predictors are not necessarily the most important predictors

- $p$-values are not very meaningful when prediction is the objective

Methods for feature selection will be discussed later.

# Ridge Regression

## Penalty

 The ridge parameter estimator is defined as the parameter $\mathbf\beta$ that minimises the **penalised least squares criterion**

 \[
 \text{SSE}_\text{pen}=\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 + \lambda \Vert \boldsymbol{\beta} \Vert_2^2
\]

- $\Vert \boldsymbol{\beta} \Vert_2^2=\sum_{j=1}^p \beta_j^2$ is the **$L_2$ penalty term**

- $\lambda>0$ is the penalty parameter (to be chosen by the user).

Note, that that is equivalent to minimizing
\[
\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \boldsymbol{\beta}\Vert^2_2\leq s
\]

Note, that $s$ has a one-to-one correspondence with $\lambda$

## Graphical interpretation

```{r echo = FALSE, warning = FALSE, message = FALSE}
library(ggforce)
library(latex2exp)
library(gridExtra)

p1 <- ggplot() +
  geom_ellipse(aes(x0 = 4, y0 = 11, a = 10, b = 3, angle = pi / 4)) +
  geom_ellipse(aes(x0 = 4, y0 = 11, a = 5, b = 1.5, angle = pi / 4)) +
  xlim(-12.5, 12.5) +
  ylim(-5, 20) +
  geom_point(aes(x = 4, y = 11)) +
  annotate("text", label = TeX("$(\\hat{\\beta}_1^{ols}, \\hat{\\beta}_2^{ols})$"), x = -5, y = 15, size = 6, parse = TRUE) +
  xlab(TeX("$\\beta_1$")) +
  ylab(TeX("$\\beta_2$")) +
  geom_segment(
    aes(x = -5, y = 12.5, xend = 3.7, yend = 11.3),
    arrow = arrow(length = unit(0.25, "cm"))
    ) +   
  coord_fixed()

pRidge <- p1 +
  geom_circle(aes(x0 = 0, y0 = 0, r = 3.9) , color = "red") +
  geom_point(aes(x = -1.1, y = 3.75), color = "red") +
  annotate("text", label = TeX("$(\\hat{\\beta}_1^{ridge}, \\hat{\\beta}_2^{ridge})$"), x = -8.1, y = 4.45, size = 6, parse = TRUE, color = "red") +
  ggtitle("Ridge") +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  theme_minimal()

pRidge
```

## Solution

The solution is given by
\[
  \hat{\boldsymbol{\beta}} = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X^T Y}.
\]
It can be shown that $(\mathbf{X^TX}+\lambda \mathbf{I})$ is always of rank $p$ if $\lambda>0$.

Hence, $(\mathbf{X^TX}+\lambda \mathbf{I})$ is invertible and $\hat{\boldsymbol{\beta}}$ exists even if $p>>>n$.

We also find
\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X}^T\mathbf{X} (\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\sigma^2
\]

However, it can be shown that improved intervals that also account for the bias can be constructed by using:

\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1}  \sigma^2.
\]

### Proof

The criterion to be minimised is
  \[
   \text{SSE}_\text{pen}=\Vert\mathbf{Y} - \mathbf{X\beta}\Vert_2^2 + \lambda \Vert \boldsymbol{\beta} \Vert_2^2.
 \]
 First we re-express SSE in matrix notation:
 \[
   \text{SSE}_\text{pen} = (\mathbf{Y}-\mathbf{X\beta})^T(\mathbf{Y}-\mathbf{X\beta}) + \lambda \boldsymbol{\beta}^T\boldsymbol{\beta}.
 \]
 The partial derivative w.r.t. $\boldsymbol{\beta}$ is
 \[
   \frac{\partial}{\partial \boldsymbol{\beta}}\text{SSE}_\text{pen} = -2\mathbf{X}^T(\mathbf{Y}-\mathbf{X\beta})+2\lambda\boldsymbol{\beta}.
 \]
 Solving $\frac{\partial}{\partial \boldsymbol{\beta}}\text{SSE}_\text{pen}=0$ gives
 \[
   \hat{\boldsymbol{\beta}} = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X^T Y}.
 \]
 (assumption: $(\mathbf{X^TX}+\lambda \mathbf{I})$ is of rank $p$. This is always true if $\lambda>0$)

## Link with SVD

### SVD and inverse
Write the SVD of $\mathbf{X}$ ($p>n$) as
\[
   \mathbf{X} = \sum_{i=1}^n \delta_i \mathbf{u}_i \mathbf{v}_i^T = \sum_{i=1}^p \delta_i \mathbf{u}_i \mathbf{v}_i^T  = \mathbf{U}\boldsymbol{\Delta} \mathbf{V}^T ,
\]
with

- $\delta_{n+1}=\delta_{n+2}= \cdots = \delta_p=0$

- $\boldsymbol{\Delta}$ a $p\times p$ diagonal matrix of the $\delta_1,\ldots, \delta_p$

-  $\mathbf{U}$ an $n\times p$ matrix and $\mathbf{V}$ a $p \times p$ matrix. Note that only the first $n$ columns of $\mathbf{U}$ and $\mathbf{V}$ are informative.

With the SVD of $\mathbf{X}$ we write
 \[
   \mathbf{X}^T\mathbf{X} = \mathbf{V}\boldsymbol{\Delta
     }^2\mathbf{V}^T.
 \]
 The inverse of $\mathbf{X}^T\mathbf{X}$ is then given by
 \[
   (\mathbf{X}^T\mathbf{X})^{-1} = \mathbf{V}\boldsymbol{\Delta}^{-2}\mathbf{V}^T.
 \]
 Since $\boldsymbol{\Delta}$ has $\delta_{n+1}=\delta_{n+2}= \cdots = \delta_p=0$, it is not invertible.

### SVD of penalised matrix $\mathbf{X^TX}+\lambda \mathbf{I}$

It can be shown that
\[
  \mathbf{X^TX}+\lambda \mathbf{I} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T ,
\]
i.e. adding a constant to the diagonal elements does not affect the eigenvectors, and all eigenvalues are increased by this constant.
$\longrightarrow$ zero eigenvalues become $\lambda$.

Hence,
\[
  (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \mathbf{V}^T ,
\]
which can be computed even when some eigenvalues in $\boldsymbol{\Delta}^2$ are zero.

Note, that for high dimensional data ($p>>>n$) many eigenvalues are zero because $\mathbf{X^TX}$ is a $p \times p$ matrix and has rank $n$.  

The identity $\mathbf{X^TX}+\lambda \mathbf{I} = \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T$ is easily checked:
\[
  \mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I}) \mathbf{V}^T = \mathbf{V}\boldsymbol{\Delta}^2\mathbf{V}^T + \lambda \mathbf{VV}^T  = \mathbf{V}\boldsymbol{\Delta}^2\mathbf{V}^T + \lambda \mathbf{I} = \mathbf{X^TX}+\lambda \mathbf{I}.
\]


## Properties

- The Ridge estimator is biased! The $\boldsymbol{\beta}$ are shrunken to zero!
\begin{eqnarray}
 \text{E}[\hat{\boldsymbol{\beta}}] &=& (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \mathbf{X}^T \text{E}[\mathbf{Y}]\\
&=& (\mathbf{X}^T\mathbf{X}+\lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{X}\boldsymbol{\beta}\\
\end{eqnarray}

- Note, that the shrinkage is larger in the direction of the smaller eigenvalues.

\begin{eqnarray}
\text{E}[\hat{\boldsymbol{\beta}}]&=&\mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \mathbf{V}^T \mathbf{V} \boldsymbol{\Delta}^2 \mathbf{V}^T\boldsymbol{\beta}\\
&=&\mathbf{V} (\boldsymbol{\Delta}^2+\lambda \mathbf{I})^{-1} \boldsymbol{\Delta}^2 \mathbf{V}^T\boldsymbol{\beta}\\
&=& \mathbf{V}
\left[\begin{array}{ccc}
\frac{\delta_1^2}{\delta_1^2+\lambda}&\ldots&0 \\
&\vdots&\\
0&\ldots&\frac{\delta_r^2}{\delta_r^2+\lambda}
\end{array}\right]
\mathbf{V}^T\boldsymbol{\beta}
\end{eqnarray}

-  the variance of the prediction $\hat{{Y}}(\mathbf{x})=\mathbf{x}^T\hat\beta$,
  \[
    \text{var}\left[\hat{{Y}}(\mathbf{x})\mid \mathbf{x}\right] = \mathbf{x}^T(\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\mathbf{x}
  \]
  is smaller than with the least-squares estimator.

-  through the bias-variance trade-off it is hoped that better predictions in terms of expected conditional test error can be obtained, for an appropriate choice of $\lambda$.


Recall the expression of the expected conditional test error
\begin{eqnarray}
  Err(\mathbf{x}) &=& \text{E}\left[(\hat{Y} - Y^*)^2\mid \mathbf{x}\right]\\
  &=&
  \text{var}\left[\hat{Y}\mid \mathbf{x}\right] + \text{bias}^2(\mathbf{x})+
  \text{var}\left[Y^*\mid \mathbf{x}\right]
\end{eqnarray}
where

- $\hat{Y}=\hat{Y}(\mathbf{x})=\mathbf{x}^T\hat{\boldsymbol{\beta}}$ is the prediction at $\mathbf{x}$
- $Y^*$ is an outcome at predictor $\mathbf{x}$
- $\mu(\mathbf{x}) = \text{E}\left[\hat{Y}\mid \mathbf{x}\right] \text{ and } \mu^*(x)=\text{E}\left[Y^*\mid \mathbf{x}\right]$
- $\text{bias}(\mathbf{x})=\mu(\mathbf{x})-\mu^*(\mathbf{x})$
- $\text{var}\left[Y^*\mid \mathbf{x}\right]$ the irreducible error that does not depend on the model. It simply originates from observations that randomly fluctuate around the true mean $\mu^*(x)$.

## Toxicogenomics example

```{r}
library(glmnet)
mRidge <- glmnet(
  x = toxData[,-1] %>%
    as.matrix,
  y = toxData %>%
    pull(BA),
  alpha = 0) # ridge: alpha = 0  

plot(mRidge, xvar="lambda")
```


The R function \textsf{glmnet} uses \textsf{lambda} to refer to the penalty parameter. In this course we use $\lambda$, because $\lambda$ is often used as eigenvalues.

The graph shows that with increasing penalty parameter, the parameter estimates are shrunken towards zero. The estimates will only reach zero for $\lambda \rightarrow \infty$. The stronger the shrinkage, the larger the bias (towards zero) and the smaller the variance of the parameter estimators (and hence also smaller variance of the predictions).

Another (informal) viewpoint is the following. By shrinking the estimates towards zero, the estimates loose some of their ``degrees of freedom'' so that the parameters become estimable with only $n<p$ data points. Even with a very small $\lambda>0$, the parameters regain their estimability. However, note that the variance of the estimator is given by
\[
  \text{var}\left[\hat{\mathbf\beta}\right] = (\mathbf{X^TX}+\lambda \mathbf{I})^{-1} \sigma^2 = \mathbf{V}(\boldsymbol{\Delta}^2+\lambda\mathbf{I})^{-1}\mathbf{V}^T\sigma^2.
\]
Hence, a small $\lambda$ will result in large variances of the parameter estimators. The larger $\lambda$, the smaller the variances become. In the limit, as $\lambda\rightarrow\infty$, the estimates are converged to zero and show no variability any longer.

# Lasso Regression

- The Lasso is another example of penalised regression.

- The lasso estimator of $\boldsymbol{\beta}$ is the solution to minimising the penalised SSE
\[
 \text{SSE}_\text{pen} = \sum_{i=1}^n (Y_i - \mathbf{x}_i^T\boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \vert \beta_j\vert.
\]


or, equivalently, minimising

\[
\text{SSE}  = \Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert_1 \leq c
\]
with

- $\Vert \mathbf\beta\Vert_1 = \sum\limits_{j=1}^p \vert \beta_j \vert$

- Despite strong similarity between ridge and lasso regression ($L_2$ versus $L_1$ norm in penalty term), there is no analytical solution of the lasso parameter estimate of $\mathbf\beta$.

- Fortunately, computational efficient algorithms have been implemented in statistical software

- The Lasso estimator of $\boldsymbol{\beta}$ is biased and generally has a smaller variance then the least-squares estimator.

- Hence, the bias-variance trade-off may here also help in finding better predictions with biased estimators.

- In contrast to ridge regression, however, the lasso estimator can give at most $\min(p,n)$ non-zero $\beta$-estimates.

- Hence, at first sight the lasso is not directly appropriate for high-dimensional settings.

- An important advantage of the lasso is that choosing an appropriate value for $\lambda$ is a kind a model building or feature selection procedure (see further).

## Graphical interpretation of Lasso vs ridge

Note that the lasso is a constrained regression problem with

\[
\Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert_1 \leq c
\]
and ridge
\[
\Vert \mathbf{Y} - \mathbf{X\beta}\Vert_2^2 \text{ subject to } \Vert \mathbf\beta\Vert^2_2 \leq c
\]

```{r echo = FALSE, warning = FALSE, message = FALSE}
pLasso <- p1 +
  geom_segment(aes(x = 0, y = 4.2 , xend = 4.2, yend = 0), color = "red") +
  geom_segment(aes(x = 0, y = 4.2 , xend = - 4.2, yend = 0), color = "red") +
  geom_segment(aes(x = 4.2, y = 0 , xend = 0, yend = -4.2), color = "red") +
  geom_segment(aes(x = 0, y = - 4.2 , xend = - 4.2, yend = 0), color = "red") +
  geom_point(aes(x = 0, y = 4.2), color = "red") +
  annotate("text", label = TeX("$(\\hat{\\beta}_1^{lasso}, \\hat{\\beta}_2^{lasso})$"), x = 7, y = 4.2, size = 6, parse = TRUE, color = "red") +
  ggtitle("Lasso") +
  geom_vline(xintercept = 0, color = "grey") +
  geom_hline(yintercept = 0, color = "grey") +
  theme_minimal()

grid.arrange(pLasso, pRidge, ncol = 2)
```

Note, that

- parameters for the lasso can never switch sign, they are set at zero! Selection!
- ridge regression can lead to parameters that switch sign.

## Toxicogenomics example

```{r}
mLasso <- glmnet(
  x = toxData[,-1] %>%
    as.matrix,
  y = toxData %>%
    pull(BA),
alpha = 1)
plot(mLasso, xvar = "lambda")
```

- The graph with the paths of the parameter estimates nicely illustrates the typical behaviour of the lasso estimates as a function of $\lambda$: when $\lambda$ increases the estimates are shrunken towards zero.

- When an estimate hits zero, it remains exactly equal to zero when $\gamma$ further increases. A parameter estimate equal to zero, say $\hat\beta_j=0$, implies that the corresponding predictor $x_j$ is no longer included in the model (i.e. $\beta_jx_j=0$).

- The model fit is known as a sparse model fit (many zeroes). Hence, choosing a appropriate value for $\gamma$ is like choosing the important predictors in the model (feature selection).


# Splines and the connection to ridge regression.

## Lidar dataset

- LIDAR (light detection and ranging) uses the reflection of laser-emitted light to detect chemical compounds in the atmosphere.
- The LIDAR technique has proven to be an efficient tool for monitoring the distribution of several atmospheric pollutants of importance; see Sigrist (1994).
- The range is the distance traveled before the light is reflected back to its source.
- The logratio is the logarithm of the ratio of received light from two laser sources.

  - One source had a frequency equal to the resonance frequency of the compound of interest, which was mercury in this study.
  - The other source had a frequency off this resonance frequency.

  - The concentration of mercury can be derived from a regression model of the logratio in function of  the range for each range x.

```{r}
library("SemiPar")
data(lidar)
pLidar <- lidar %>%
  ggplot(aes(x = range, y = logratio)) +
  geom_point() +
  xlab("range (m)")

pLidar +
  geom_smooth()
```

- The data is non-linear
- Linear regression will not work!
- The data shows a smooth relation between the logratio and the range

## Basis expansion

\[y_i=f(x_i)+\epsilon_i,\]
with
\[f(x)=\sum\limits_{k=1}^K \theta_k b_k(x)\]

-  Select set of basis functions $b_k(x)$
-  Select number of basis functions $K$
-  Examples

    -  Polynomial model: $x^k$
    -  Orthogonal series: Fourier, Legendre polynomials, Wavelets
    -  Polynomial splines: $1, x, (x-t_m)_+$ with $m=1, \ldots, K-2$ knots $t_m$
    -  ...

### Trunctated line basis

\[y_i=f(x_i)+\epsilon_i,\]

-  One of the most simple basis expansions
-  $f(x_i)=\beta_0+\beta_1x_i+\sum\limits_{m=1}^{K-2}\theta_m(x_i-t_m)_+$ with $(.)_+$ the operator that takes the positive part.
-  Note, that better basis expansions exist, which are orthogonal, computational more stable and/or continuous derivative beyond first order
-  We will use this basis for didactical purposes
- We can use OLS to fit y w.r.t. the basis.

```{r}
knots <- seq(400,700,12.5)

basis <- sapply(knots,
  function(k,y) (y-k)*(y>k),
  y= lidar %>% pull(range)
  )

basisExp <- cbind(1, range = lidar %>% pull(range), basis)

splineFitLs <- lm(logratio ~ -1 + basisExp, lidar)

pBasis <- basisExp[,-1] %>%
  data.frame %>%
  gather("basis","values",-1) %>%
  ggplot(aes(x = range, y = values, color = basis)) +
  geom_line() +
  theme(legend.position="none") +
  ylab("basis")

grid.arrange(
  pLidar +
    geom_line(aes(x = lidar$range, y = splineFitLs$fitted), lwd = 2),
  pBasis,
  ncol=1)
```

- Note, that the model is overfitting!
- The fit is very wiggly and is tuned too much to the data.
- The fit has a large variance and low bias.
- It will therefore not generalise well to predict the logratio of future observations.

#### Solution for overfitting?

- We could perform model selection on the basis to select the important basis functions to model the signal. But, this will have the undesired property that the fit will no longer be smooth.

- We can also adopt a ridge penalty!
- However, we do not want to penalise the intercept and the linear term.  
- Ridge criterion

\[\Vert\mathbf{Y}-\mathbf{X\beta}\Vert^2+\lambda\boldsymbol{\beta}^T\mathbf{D}\boldsymbol{\beta}
\]

With $\mathbf{D}$ with dimensions (K,K): $\mathbf{D}=\left[\begin{array}{cc}\mathbf{0}_{2\times2}& \mathbf{0}_{2\times K-2}\\
\mathbf{0}_{K-2\times2}&\mathbf{I}_{K-2\times K-2}\end{array}\right]$

- Here we will set the penalty at 900.

```{r}
D <- diag(ncol(basisExp))
D[1:2,1:2] <- 0
lambda <- 900
betaRidge <- solve(t(basisExp)%*%basisExp+(lambda*D))%*%t(basisExp)%*%lidar$logratio
grid.arrange(
  pLidar +
    geom_line(aes(x = lidar$range, y = c(basisExp %*% betaRidge)), lwd = 2),
  pBasis,
  ncol=1)
```

How do we choose $\lambda$?

---

# Evaluation of Prediction Models


Predictions are calculated with the fitted model
 \[
   \hat{Y}(\mathbf{x}) = \hat{m}(\mathbf{x})=\mathbf{x}^T\hat{\beta}
 \]
 when focussing on prediction, we want the prediction error to be as small as possible.

The **prediction error** for a prediction at covariate pattern $\mathbf{x}$ is given by
  \[
     \hat{Y}(\mathbf{x}) - Y^*,
  \]
where

- $\hat{Y}(\mathbf{x})=\mathbf{x}^T\hat{\boldsymbol{\beta}}$ is the prediction at $\mathbf{x}$

-  $Y^*$ is an outcome at covariate pattern $\mathbf{x}$

Prediction is typically used to predict an outcome before it is observed.

- Hence, the outcome $Y^*$ is not observed yet, and
- the prediction error cannot be computed.

---

- Recall that the prediction model $\hat{Y}(\mathbf{x})$ is estimated by using data in the training data set $(\mathbf{X},\mathbf{Y})$, and
- that the outcome $Y^*$ is an outcome at $\mathbf{x}$ which is assumed to be independent of the training data.

- Goal is to use prediction model for predicting a future observation ($Y^*$), i.e. an observation that still has to be realised/observed (otherwise prediction seems rather useless).

- Hence, $Y^*$ can never be part of the training data set.

---

Here we provide definitions and we show how the prediction performance of a prediction model can be evaluated from data.

Let ${\cal{T}}=(\mathbf{Y},\mathbf{X})$ denote the training data, from which the prediction model $\hat{Y}(\cdot)$ is build. This building process typically involves feature selection and parameter estimation.

 We will use a more general notation for the prediction model: $\hat{m}(\mathbf{x})=\hat{Y}(\mathbf{x})$.

---

## Test or Generalisation Error

 The test or generalisation error for prediction model $\hat{m}(\cdot)$ is given by
  \[
    \text{Err}_{\cal{T}} = \text{E}_{Y^*,X^*}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\mid {\cal{T}}\right]
  \]
  where $(Y^*,X^*)$ is independent of the training data.

---

- Note that the test error is conditional on the training data ${\cal{T}}$.
- Hence, the test error evaluates the performance of the single model build from the observed training data.
- This is the ultimate target of the model assessment, because it is exactly this prediction model that will be used in practice and applied to future predictors $\mathbf{X}^*$ to predict $Y^*$.
- The test error is defined as an average over all such future observations $(Y^*,\mathbf{X}^*)$.

---

## Conditional test error

Sometimes the conditional test error is used:

The conditional test error in $\mathbf{x}$ for prediction model $\hat{m}(\mathbf{x})$ is given by
 \[
   \text{Err}_{\cal{T}}(\mathbf{x}) = \text{E}_{Y^*}\left[(\hat{m}(\mathbf{x}) - Y^*)^2\mid {\cal{T}}, \mathbf{x}\right]
 \]
 where $Y^*$ is an outcome at predictor $\mathbf{x}$, independent of the training data.

 Hence,
 \[
   \text{Err}_{\cal{T}} = \text{E}_{X^*}\left[\text{Err}_{\cal{T}}(\mathbf{X}^*)\right].
 \]

A closely related error is the **insample error**.

---

## Insample Error

The insample error for prediction model $\hat{m}(\mathbf{x})$ is given by
 \[
   \text{Err}_{\text{in} \cal{T}} = \frac{1}{n}\sum_{i=1}^n \text{Err}_{\cal{T}}(\mathbf{x}_i),
 \]

i.e. the insample error is the sample average of the conditional test errors evaluated in the $n$ training dataset predictors $\mathbf{x}_i$.

Since $\text{Err}_{\cal{T}}$ is an average over all $\mathbf{X}$, even over those predictors not observed in the training dataset, it is sometimes referred to as the **outsample error**.

---

## Estimation of the insample error

We start with introducing the training error rate, which is closely related to the MSE in linear models.

### Training error

 The training error is given by
 \[
   \overline{\text{err}} = \frac{1}{n}\sum_{i=1}^n (Y_i - \hat{m}(\mathbf{x}_i))^2 ,
 \]
 where the $(Y_i,\mathbf{x}_i)$ from the training dataset which is also used for the calculation of $\hat{m}$.

- The training error is an overly optimistic estimate of the test error $\text{Err}_{\cal{T}}$.

- The training error will never increases when the model becomes more complex. $\longrightarrow$ cannot be used directly as a model selection criterion.

Indeed, model parameters are often estimated by minimising the training error (cfr. SSE).

- Hence the fitted model adapts to the training data, and
- training error will be an overly optimistic estimate of the test error $\text{Err}_{\cal{T}}$.

---

It can be shown that the training error is related to the insample test error via

\[
\text{E}_\mathbf{Y}
\left[\text{Err}_{\text{in}{\cal{T}}}\right] = \text{E}_\mathbf{Y}\left[\overline{\text{err}}\right] + \frac{2}{n}\sum_{i=1}^n \text{cov}_\mathbf{Y}\left[\hat{m}(\mathbf{x}_i),Y_i\right],
\]

Note, that for linear models
\[ \hat{m}(\mathbf{x}_i) = \mathbf{X}\hat{\boldsymbol{\beta}}= \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} = \mathbf{HY}
\]
with

- $\mathbf{H}$ the hat matrix and
- all $Y_i$ are assumed to be independently distributed  $N(\mathbf{X}\boldsymbol{\beta},\sigma^2)$  

Hence, for linear models with independent observations

\begin{eqnarray}
\text{cov}_\mathbf{Y}\left[\hat{m}(\mathbf{x}_i),Y_i)\right] &=&
\text{cov}_\mathbf{Y}\left[\mathbf{H}_{i}^T\mathbf{Y},Y_i)\right]\\
&=& \text{cov}_\mathbf{Y}\left[h_{ii} Y_i,Y_i\right]\\
&=& h_{ii} \text{cov}_\mathbf{Y}\left[Y_i,Y_i\right]\\
&=& h_{ii} \sigma^2\\
\end{eqnarray}

And we can thus estimate the insample error by Mallow's $C_p$

\begin{eqnarray}
C_p &=& \overline{\text{err}} + \frac{2\sigma^2}{n}\text{tr}(\mathbf{H})\\
&=& \overline{\text{err}} + \frac{2\sigma^2p}{n}
\end{eqnarray}

with $p$ the number of predictors.

- Mallow's $C_p$ is often used for model selection.
- Note, that we can also consider it as a kind of penalized least squares:

\[
n \times C_p = \Vert \mathbf{Y} - \mathbf{X}\boldsymbol{\beta}\Vert_2^2 + 2\sigma^2 \Vert \boldsymbol{\beta} \Vert_0
\]
with $L_0$ norm $\Vert \boldsymbol{\beta} \Vert_0 = \sum_{j=1}^p \beta_p^0 = p$.

---

## Expected test error

The test or generalisation error was defined conditionally on the training data. By averaging over the distribution of training datasets, the expected test error arises.

\begin{eqnarray*}
   \text{E}_{\cal{T}}\left[\text{Err}_{{\cal{T}}}\right]
     &=& \text{E}_{\cal{T}}\left[\text{E}_{Y^*,X^*}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\mid {\cal{T}}\right]\right] \\
     &=& \text{E}_{Y^*,X^*,{\cal{T}}}\left[(\hat{m}(\mathbf{X}^*) - Y^*)^2\right].
 \end{eqnarray*}

 - The expected test error may not be of direct interest when the goal is to assess the prediction performance of a single prediction model $\hat{m}(\cdot)$.

 - The expected test error averages the test errors of all models that can be build from all training datasets, and hence this may be less relevant when the interest is in evaluating one particular model that resulted from a single observed training dataset.

 - Also note that building a prediction model involves both parameter estimation and feature selection.

 - Hence the expected test error also evaluates the feature selection procedure (on average).

 - If the expected test error is small, it is an indication that the model building process gives good predictions for future observations $(Y^*,\mathbf{X}^*)$ on average.

### Estimating the Expected test error

The expected test error may be estimated by cross validation (CV).

#### Leave one out cross validation (LOOCV)}

The LOOCV estimator of the expected test error (or expected outsample error) is given by
  \[
     \text{CV} = \frac{1}{n} \sum_{i=1}^n \left(Y_i - \hat{m}^{-i}(\mathbf{x}_i)\right)^2 ,
  \]
where

- the $(Y_i,\mathbf{x}_i)$ form the training dataset
-   $\hat{m}^{-i}$ is the fitted model based on all training data, except observation $i$
-   $\hat{m}^{-i}(\mathbf{x}_i)$ is the prediction at $\mathbf{x}_i$, which is the observation left out the training data before building model $m$.

Some rationale as to why LOOCV offers a good estimator of the outsample error:

- the prediction error $Y^*-\hat{m}(\mathbf{x})$ is mimicked by not using one of the training outcomes $Y_i$ for the estimation of the model so that this $Y_i$ plays the role of $Y^*$, and, consequently, the fitted model $\hat{m}^{-i}$ is independent of $Y_i$

 - the sum in $CV$ is over all $\mathbf{x}_i$ in the training dataset, but each term $\mathbf{x}_i$ was left out once for the calculation of $\hat{m}^{-i}$. Hence, $\hat{m}^{-i}(\mathbf{x}_i)$ mimics an outsample prediction.

 - the sum in CV is over $n$ different training datasets (each one with a different observation removed), and hence CV is an estimator of the *expected* test error.

 - For linear models the LOOCV can be readily obtained from the fitted model: i.e.

 \[\text{CV} = \frac{1}{n}\sum\limits_{i=1}^n \frac{e_i^2}{(1-h_{ii})^2}\]

 with $e_i$ the residuals from the model that is fitted based on all training data.

---

An alternative to LOOCV is the $k$-fold cross validation procedure. It also gives an estimate of the expected outsample error.

#### $k$-fold cross validation

-  Randomly divide the training dataset into $k$ approximately equal subsets . Let $S_j$ denote the index set of the $j$th subset (referred to as a **fold**). Let $n_j$ denote the number of observations in fold $j$.

- The $k$-fold cross validation estimator of the expected outsample error is given by
 \[
     \text{CV}_k = \frac{1}{k}\sum_{j=1}^k \frac{1}{n_j} \sum_{i\in S_j} \left(Y_i - \hat{m}^{-S_j}(\mathbf{x}_i)\right)^2
 \]
 where $\hat{m}^{-S_j}$ is the model fitted using all training data, except observations in fold $j$ (i.e. observations $i \in S_j$).

---

The cross validation estimators of the expected outsample error are nearly unbiased. One argument that helps to understand where the bias comes from is the fact that e.g. in de LOOCV estimator the model is fit on only $n-1$ observations, whereas we are aiming at estimating the outsample error of a model fit on all $n$ training observations. Fortunately, the bias is often small and is in practice hardly a concern.

$k$-fold CV is computationally more complex.

Since CV and CV$_k$ are estimators, they also show sampling variability. Standard errors of the CV or CV$_k$ can be computed. We don't show the details, but in the example this is illustrated.

### Bias Variance trade-off

For the expected conditional test error in $\mathbf{x}$, it holds that
\begin{eqnarray*}
  \text{E}_{\cal{T}}\left[\text{Err}_{\cal{T}}(\mathbf{x})\right]
    &=& \text{E}_{Y^*,{\cal{T}}}\left[(\hat{m}(\mathbf{x})-Y^*)^2 \mid \mathbf{x}\right] \\
    &=&  \text{var}_{\mathbf{Y}}\left[\hat{Y}(\mathbf{x})\mid \mathbf{x}\right] +(\mu(\mathbf{x})-\mu^*(\mathbf{x}))^2+\text{var}_{Y^*}\left[Y^*\mid \mathbf{x}\right]
\end{eqnarray*}
where $\mu(\mathbf{x}) = \text{E}_{\mathbf{Y}}\left[\hat{Y}(\mathbf{x})\mid \mathbf{x}\right] \text{ and } \mu^*(\mathbf{x})=\text{E}_{Y^*}\left[Y^*\mid \mathbf{x}\right]$.

- **bias**: $\text{bias}(\mathbf{x})=\mu(\mathbf{x})-\mu^*(\mathbf{x})$

- $\text{var}_{Y^*}\left[Y^*\mid \mathbf{x}\right]$ does not depend on the model, and is referred to as the **irreducible variance**.

---

The importance of the bias-variance trade-off can be seen from a model selection perspective. When we agree that a good model is a model that has a small expected conditional test error at some point $\mathbf{x}$, then the bias-variance trade-off shows us that a model may be biased as long as it has a small variance to compensate for the bias.  It often happens that a biased model has a substantial smaller variance. When these two are combined, a small expected test error may occur.

Also note that the model $m$ which forms the basis of the prediction model $\hat{m}(\mathbf{x})$ does NOT need to satisfy $m(\mathbf{x})=\mu(\mathbf{x})$ or $m(\mathbf{x})=\mu^*(\mathbf{x})$. The model $m$ is known by the data-analyst (its the basis of the prediction model), whereas $\mu(\mathbf{x})$ and $\mu^*(\mathbf{x})$ are generally unknown to the data-analyst. We only hope that $m$ serves well as a prediction model.

---

### In practice

We use cross validation to estimate the lambda penalty for penalised regression:

- Ridge Regression
- Lasso
- Build models, e.g. select the number of PCs for PCA regression
- Splines

### Toxicogenomics example

#### Lasso

```{r}
set.seed(15)
library(glmnet)
mCvLasso <- cv.glmnet(
  x = toxData[,-1] %>%
    as.matrix,
  y = toxData %>%
    pull(BA),
  alpha = 1)  # lasso alpha=1

plot(mCvLasso)
```

Default CV procedure in \textsf{cv.glmnet} is $k=10$-fold CV.

The Graphs shows

- 10-fold CV estimates of the extra-sample error as a function of the lasso penalty parameter $\lambda$.
- estimate plus and minus once the estimated standard error of the CV estimate (grey bars)
- On top the number of non-zero regression parameter estimates are shown.

Two vertical reference lines are added to the graph. They correspond to

- the $\log(\lambda)$ that gives the smallest CV estimate of the extra-sample error, and
- the largest $\log(\lambda)$ that gives a CV estimate of the extra-sample error that is within one standard error from the smallest error estimate.
- The latter choice of $\lambda$ has no firm theoretical basis, except that it somehow accounts for the imprecision of the error estimate. One could loosely say that this $\gamma$ corresponds to the smallest model (i.e. least number of predictors) that gives an error that is within margin of error of the error of the best model.

---

```{r}
mLassoOpt <- glmnet(
  x = toxData[,-1] %>%
    as.matrix,
  y = toxData %>%
    pull(BA),
    alpha = 1,
    lambda = mCvLasso$lambda.min)

summary(coef(mLassoOpt))
```


With the optimal $\lambda$ (smallest error estimate) the output shows the `r  nrow(summary(coef(mLassoOpt)))` non-zero estimated regression coefficients (sparse solution).

---

```{r}
mLasso1se <- glmnet(
  x = toxData[,-1] %>%
    as.matrix,
    y= toxData %>%
      pull(BA),
    alpha = 1,
    lambda = mCvLasso$lambda.1se)

mLasso1se %>%
  coef %>%
  summary
```

This shows the solution for the largest $\lambda$ within one standard error of the optimal model. Now only `r  nrow(summary(coef(mLasso1se)))` non-zero estimates result.

---

#### Ridge

```{r}
mCvRidge <- cv.glmnet(
  x = toxData[,-1] %>%
    as.matrix,
    y = toxData %>%
      pull(BA),
      alpha = 0)  # ridge alpha=0

plot(mCvRidge)
```

- Ridge does not seem to have optimal solution.
- 10-fold CV is also larger than for lasso.

---

#### PCA regression

```{r fig.keep = "none", warning = FALSE}
set.seed(1264)
library(DAAG)

tox <- data.frame(
  Y = toxData %>%
    pull(BA),
  PC = Zk)

PC.seq <- 1:25
Err <- numeric(25)

mCvPca <- cv.lm(
  Y~PC.1,
  data = tox,
  m = 5,
  printit = FALSE)

Err[1]<-attr(mCvPca,"ms")

for(i in 2:25) {
  mCvPca <- cv.lm(
    as.formula(
      paste("Y ~ PC.1 + ",
        paste("PC.", 2:i, collapse = "+", sep=""),
        sep=""
      )
    ),
    data = tox,
    m = 5,
    printit = FALSE)
  Err[i]<-attr(mCvPca,"ms")
}
```

- Here we illustrate principal component regression.

- The most important PCs are selected in a forward model selection procedure.

- Within the model selection procedure the models are evaluated with 5-fold CV estimates of the outsample error.

- It is important to realise that a forward model selection procedure will not necessarily result in the best prediction model, particularly because the order of the PCs is generally not related to the importance of the PCs for predicting the outcome.

- A supervised PC would be better.

```{r}
pPCreg <- data.frame(PC.seq, Err) %>%
  ggplot(aes(x = PC.seq, y = Err)) +
  geom_line() +
  geom_point() +
  geom_hline(
    yintercept = c(
      mCvLasso$cvm[mCvLasso$lambda==mCvLasso$lambda.min],
      mCvLasso$cvm[mCvLasso$lambda==mCvLasso$lambda.1se]),
    col = "red") +
  xlim(1,26)

grid.arrange(
  pPCreg,
  pPCreg + ylim(0,5),
  ncol=2)
```

- The graph shows the CV estimate of the outsample error as a function of the number of sparse PCs included in the model.

- A very small error is obtained with the model with only the first PC. The best model with 3 PCs.

- The two vertical reference lines correspond to the error estimates obtained with lasso (optimal $\lambda$ and largest $\lambda$ within one standard error).

- Thus although there was a priori no guarantee that the first PCs are the most predictive, it seems to be the case here (we were lucky!).

- Moreover, the first PC resulted in a small outsample error.

- Note that the graph does not indicate the variability of the error estimates (no error bars).

- Also note that the graph clearly illustrates the effect of overfitting: including too many PCs causes a large outsample error.

### Lidar Example: splines

- We use the mgcv package to fit the spline model to the lidar data.
- A better basis is used than the truncated spline basis
- Thin plate splines are also linear smoothers, i.e.  
$\hat{Y} = \hat{m}(\mathbf{X}) = \mathbf{SY}$
- So their variance can be easily calculated.
- The ridge/smoothness penalty is chosen by generalized cross validation.

```{r}
library(mgcv)
gamfit <- gam(logratio ~ s(range), data = lidar)
gamfit$sp

pLidar +
  geom_line(aes(x = lidar$range, y = gamfit$fitted), lwd = 2)
```

## More general error definitions

So far we only looked at continuous outcomes $Y$ and errors defined in terms of the squared loss $(\hat{m}(\mathbf{x})-Y^*)^2$.

More generally, a **loss function** measures an discrepancy between the prediction $\hat{m}(\mathbf{x})$ and an independent outcome $Y^*$ that corresponds to $\mathbf{x}$.


Some examples for continuous $Y$:
\begin{eqnarray*}
  L(Y^*,\hat{m}(\mathbf{x}))
    &=& (\hat{m}(\mathbf{x})-Y^*)^2 \;\;\text{(squared error)} \\
  L(Y^*,\hat{m}(\mathbf{x}))
    &=& \vert\hat{m}(\mathbf{x})-Y^*\vert \;\;\text{(absolute error)} \\
   L(Y^*,\hat{m}(\mathbf{x}))
    &=& 2 \int_{\cal{Y}} f_y(y) \log\frac{f_y(y)}{f_{\hat{m}}(y)} dy \;\;\text{(deviance)}.
\end{eqnarray*}


In the expression of the deviance

- $f_y$ denotes the density function of a distribution with mean set to $y$ (cfr. perfect fit), and
- $f_{\hat{m}}$ is the density function of the same distribution but with mean set to the predicted outcome $\hat{m}(\mathbf{x})$.

---

With a given loss function, the errors are defined as follows:
- Test or generalisation or outsample error
    \[
      \text{Err}_{\cal{T}} = \text{E}_{Y^*,X^*}\left[L(Y^*,\hat{m}(\mathbf{X}^*))\right]
    \]

- Training error
  \[
    \overline{\text{err}} = \frac{1}{n}\sum_{i=1}^n L(Y_i,\hat{m}(\mathbf{x}_i))
  \]

- $\ldots$

---

When an exponential family distribution is assumed for the outcome distribution, and when the deviance loss is used, the insample error can be estimated by means of the AIC and BIC.

### Akaike's Information Criterion (AIC)

The AIC for a model $m$ is given by
\[
\text{AIC} = -2 \ln \hat{L}(m) +2p
\]
where $\hat{L}(m)$ is the maximised likelihood for model $m$.

When assuming normally distributed error terms and homoscedasticity, the AIC becomes
\[
\text{AIC} = n\ln \text{SSE}(m) +2p = n\ln(n\overline{\text{err}}(m)) + 2p
\]
with $\text{SSE}(m)$ the residual sum of squares of model $m$.

In linear models with normal error terms, Mallow's $C_p$ criterion (statistic) is a linearised version of AIC and it is an unbiased estimator of the in-sample error.

---

### Bayesian Information Criterion (BIC)}

The BIC for a model $m$ is given by
\[
\text{BIC} = -2 \ln \hat{L}(m) +p\ln(n)
\]
where $\hat{L}(m)$ is the maximised likelihood for model $m$.

When assuming normally distributed error terms and homoscedasticity, the BIC becomes
\[
\text{BIC} = n\ln \text{SSE}(m) +p\ln(n) = n\ln(n\overline{\text{err}}(m)) + p\ln(n)
\]
with $\text{SSE}(m)$ the residual sum of squares of model $m$.

When large datasets are used, the BIC will favour smaller models than the AIC.

---

## Training and test sets

Sometimes, when a large (training) dataset is available, one may decide the split the dataset randomly in a

- **training dataset**:
   data are used for model fitting and for model building or feature selection (this may require e.g. cross validation)

- **test dataset**:
   this data are used to evaluate the final model (result of model building). An unbiased estimate of the outsample error (i.e. test or generalisation error) based on this test data is
  \[
     \frac{1}{m} \sum_{i=1}^m \left(\hat{m}(\mathbf{x}_i)-Y_i\right)^2,
  \]
  where
    - $(Y_1,\mathbf{x}_1), \ldots, (Y_m,\mathbf{x}_m)$ denote the $m$ observations in the test dataset

    - $\hat{m}$ is estimated from using the training data (this may also be the result from model building, using only the training data).

---

Note that the training dataset is used for model building or feature selection. This also requires the evaluation of models. For these evaluations the methods from the previous slides can be used (e.g. cross validation, $k$-fold CV, Mallow's $C_p$). The test dataset is only used for the  evaluation of the final model (estimated and build from using only the training data). The estimate of the outsample error based on the test dataset is the best possible estimate in the sense that it is unbiased. The observations used for this estimation are independent of the observations in the training data.
However, if the number of data points in the test dataset ($m$) is small, the estimate of the outsample error may show large variance and hence is not reliable.

# Logistic Regression Analysis for High Dimensional Data

## Cancer Example

- Schmidt *et al.*, 2008, Cancer Research, {\bf 68}, 5405-5413

- Gene expression patterns in $n=200$ breast tumors were investigated ($p=22283$ genes)

- After surgery the tumors were graded by a pathologist (stage 1,2,3)

- Here the objective is to predict stage 3 from the gene expression data (prediction of binary outcome)

- If the prediction model works well, it can be used to predict the stage from a biopsy sample.

---

## Logistic regression models

Binary outcomes are often analysed with **logistic regression models**.

Let $Y$ denote the binary (1/0, case/control, positive/negative) outcome, and $\mathbf{x}$ the $p$-dimensional predictor.

Logistic regression  assumes
\[
   Y \mid \mathbf{x} \sim \text{Bernoulli}(\pi(\mathbf{x}))
\]
with $\pi(\mathbf{x}) = \text{P}\left[Y=1\mid \mathbf{x}\right]$ and
\[
   \ln \frac{\pi(\mathbf{x})}{1-\pi(\mathbf{x})}=\beta_0 + \boldsymbol{\beta}^T\mathbf{x}.
\]

The parameters are typically estimated by maximising the log-likelihood, which is denoted by $l(\mathbf{
\beta})$, i.e.
\[
   \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}).
\]

- Maximum likelihood is only applicable when $n>p$.

- When $p>n$ penalised maximum likelihood methods are applicable.

---

## Penalized maximum likelihood

Penalised estimation methods (e.g. lasso and ridge) can als be applied to maximum likelihood, resulting in the **penalised maximum likelihood estimate**.

Lasso:
\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda \Vert \boldsymbol{\beta}\Vert_1.
\]

Ridge:
\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda \Vert \boldsymbol{\beta}\Vert_2^2.
\]

Once the parameters are estimated, the model may be used to compute
\[
  \hat{\pi}(\mathbf{x}) = \hat{\text{P}}\left[Y=1\mid \mathbf{x}\right].
\]
With these estimated probabilities the prediction rule becomes
\begin{eqnarray*}
  \hat{\pi}(\mathbf{x}) &\leq c& \text{predict } Y=0 \\
  \hat{\pi}(\mathbf{x}) &>c & \text{predict } Y=1
\end{eqnarray*}
with $0<c<1$ a threshold that either is fixed (e.g. $c=1/2$), depends on prior probabilities, or is empirically determined by optimising e.g. the Area Under the ROC Curve (AUC) or by finding a good compromise between sensitivity and specificity.

Note that logistic regression directly models the **Posterior probability** that an observation belongs to class $Y=1$, given the predictor $\mathbf{x}$.

## Model evaluation

Common model evaluation criteria for binary prediction models are:

- sensitivity = true positive rate (TPR)

- specificity = true negative rate (TNR)

- misclassification error

- area under the ROC curve (AUC)

These criteria can again be estimated via cross validation or via splitting of the data into training and test/validation data.

### Sensitivity of a model $\pi$ with threshold $c$

Sensitivity is the probability to correctly predict a positive outcome:
\[
\text{sens}(\pi,c)=\text{P}_{X^*}\left[\hat\pi(\mathbf{X}^*)>c \mid Y^*=1 \mid {\cal{T}}\right].
\]

It is also known as the true positive rate (TPR).

### Specificity of a model $\pi$ with threshold $c$

Specificity is the probability to correctly predict a negative outcome:
\[
\text{spec}(\pi,c)=\text{P}_{X^*}\left[\hat\pi(\mathbf{X}^*)\leq c \mid Y^*=0 \mid {\cal{T}}\right].
\]

It is also known as the true negative rate (TNR).

---

### Misclassification error of a model $\pi$ with threshold $c$

The misclassification error is the probability to incorrectly predict an outcome:
\begin{eqnarray*}
\text{mce}(\pi,c) &=&\text{P}_{X^*,Y^*}\left[\hat\pi(\mathbf{X})\leq c \text{ and } Y^*=1 \mid {\cal{T}}\right] \\
&  & + \text{P}_{X^*,Y^*}\left[\hat\pi(\mathbf{X})> c \text{ and } Y^*=0 \mid {\cal{T}}\right].
\end{eqnarray*}

Note that in the definitions of sensitivity, specificity and the misclassification error, the probabilities refer to the distribution of the  $(\mathbf{X}^*,Y^*)$, which is independent of the training data, conditional on the training data. This is in line with the test or generalisation error. The misclassification error is actually the test error when a 0/1 loss function is used. Just as before, the sensitivity, specificity and the misclassification error can also be averaged over the distribution of the training data set, which is in line with the expected test error which has been discussed earlier.

---

### ROC curve of a model $\pi$

The Receiver Operating Characteristic (ROC) curve for model $\pi$ is given by the function

\[
\text{ROC}: [0,1] \rightarrow [0,1]\times [0,1]: c \mapsto (1-\text{spec}(\pi,c), \text{sens}(\pi,c)).
\]

For when $c$ moves from 1 to 0, the ROC function defines a curve in the plane $[0,1]\times [0,1]$, moving from $(0,0)$ for $c=1$ to $(1,1)$ for $c=0$.

The horizontal axis of the ROC curve shows 1-specificity. This is also known as the False Positive Rate (FPR).

---

### Area under the curve (AUC) of a model $\pi$

The area under the curve (AUC) for model $\pi$ is area under the ROC curve and is given by
\[
\int_0^1 \text{ROC}(c) dc.
\]

Some notes about the AUC:

- AUC=0.5 results when the ROC curve is the diagonal. This corresponds to flipping a coin, i.e. a complete random prediction.

- AUC=1 results from the perfect ROC curve, which is the ROC curve through the points $(0,0)$, $(0,1)$ and $(1,1)$. This ROC curve includes a threshold $c$ such that sensitivity and specificity are equal to one.

## Breast cancer example

### Data

```{r, message=FALSE, warning=FALSE}
library(glmnet)

#BiocManager::install("genefu")
#BiocManager::install("breastCancerMAINZ")

library(genefu)
library(breastCancerMAINZ)
data(mainz)

X <- t(exprs(mainz)) # gene expressions
n <- nrow(X)
H <- diag(n)-1/n*matrix(1,ncol=n,nrow=n)
X <- H%*%X
Y <- ifelse(pData(mainz)$grade==3,1,0)
table(Y)
```

---

From the table of the outcomes in Y we read that

- `r sum(Y==1)` tumors were graded as stage 3 and
- `r sum(Y==0)` tumors were graded as stage 1 or 2.

In this the stage 3 tumors are referred to as cases or postives and the stage 1 and 2 tumors as controls or negatives.

---

### Training and test dataset

The use of the lasso logistic regression for the prediction of stage 3 breast cancer is illustrated here by

- randomly splitting the dataset into a training dataset ($80\%$ of data = 160 tumors) and a test dataset (40 tumors)

- using the training data to select a good $\lambda$ value in the lasso logistic regression model (through 10-fold CV)

- evaluating the final model by means of the test dataset (ROC Curve, AUC).


```{r}

## Used to provide same results as in previous R version
RNGkind(sample.kind = "Rounding")
set.seed(6977326)
####

n <- nrow(X)
nTrain <- round(0.8*n)
nTrain

indTrain <- sample(1:n,nTrain)
XTrain <- X[indTrain,]
YTrain <- Y[indTrain]
XTest <- X[-indTrain,]
YTest <- Y[-indTrain]
table(YTest)
```

Note that the randomly selected test data has `r mean(YTest==1)*100`% cases of stage 3 tumors.
This is a bit higher than the `r mean(Y==1)*100`%  in the complete data.

One could also perform the random splitting among the positives and the negatives separately (stratified splitting).

### Model fitting based on training data

```{r}
mLasso <- glmnet(
  x = XTrain,
  y = YTrain,
  alpha = 1,
  family="binomial")  # lasso: alpha = 1

plot(mLasso, xvar = "lambda", xlim = c(-6,-1.5))
```

---

```{r}
mCvLasso <- cv.glmnet(
  x = XTrain,
  y = YTrain,
  alpha = 1,
  type.measure = "class",
	family = "binomial")  # lasso alpha = 1

plot(mCvLasso)
mCvLasso
```

The total misclassification error is used here to select a good value for $\lambda$.

```{r}
# BiocManager::install("plotROC")
library(plotROC)

dfLassoOpt <- data.frame(
  pi = predict(mCvLasso,
    newx = XTest,
    s = mCvLasso$lambda.min,
    type = "response") %>% c(.),
  known.truth = YTest)

roc <-
  dfLassoOpt  %>%
  ggplot(aes(d = known.truth, m = pi)) +
  geom_roc(n.cuts = 0) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)")

roc

calc_auc(roc)
```

- The ROC curve is shown for the model based on $\lambda$ with the smallest misclassification error. The model has an AUC of `r calc_auc(roc) %>% pull(AUC) %>% round(2)`.  

- Based on this ROC curve an appropriate threshold $c$ can be chosen. For example, from the ROC curve we see that it is possible to attain a specificity and a sensitivity of 75\%.

- The sensitivities and specificities in the ROC curve are unbiased (independent test dataset) for the prediction model build from the training data. The estimates of sensitivity and specificity, however, are based on only 40 observations.

---

```{r}
mLambdaOpt <- glmnet(x = XTrain,
  y = YTrain,
  alpha = 1,
  lambda = mCvLasso$lambda.min,
  family="binomial")

qplot(
  summary(coef(mLambdaOpt))[-1,1],
  summary(coef(mLambdaOpt))[-1,3]) +
  xlab("gene ID") +
  ylab("beta-hat") +
  geom_hline(yintercept = 0, color = "red")
```

- The model with the optimal $\lambda$ has only `r mLambdaOpt %>% coef %>% summary %>% nrow` non-zero parameter estimates.
- Thus only `r mLambdaOpt %>% coef %>% summary %>% nrow` genes are involved in the prediction model.
- These `r mLambdaOpt %>% coef %>% summary %>% nrow` parameter estimates are plotting in the graph.
A listing of the model output would show the names of the genes.

---

```{r}

dfLasso1se <- data.frame(
  pi = predict(mCvLasso,
    newx = XTest,
    s = mCvLasso$lambda.1se,
    type = "response") %>% c(.),
  known.truth = YTest)

roc <-
  rbind(
    dfLassoOpt %>%
      mutate(method = "min"),
    dfLasso1se %>%
      mutate(method = "1se")
  ) %>%
  ggplot(aes(d = known.truth, m = pi, color = method)) +
  geom_roc(n.cuts = 0) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)")

roc

calc_auc(roc)
```

- When using the $\lambda$ of the optimal model up to 1 standard deviation, a diagonal ROC curve is obtained and hence AUC is $0.5$.

- This prediction model is thus equivalent to flipping a coin for making the prediction.

- The reason is that with this choice of $\lambda$ (strong penalisation) almost all predictors are removed from the model.

- Therefore, do never blindly choose for the ``optimal'' $\lambda$ as defined here, but assess the performance of the model first.

```{r}
mLambda1se <- glmnet(x = XTrain,
  y = YTrain,
  alpha = 1,
  lambda = mCvLasso$lambda.1se,
  family="binomial")

mLambda1se %>%
  coef %>%
  summary
```

---

## The Elastic Net

The lasso and ridge regression have positive and negative properties.

- Lasso

   - positive: sparse solution

   - negative: at most $\min(n,p)$ predictors can be selected

   - negative: tend to select one predictor among a group of highly correlated predictors


- Ridge

    - negative: no sparse solution
    - positive: more than $\min(n,p)$ predictors can be selected

A compromise between lasso and ridge: the **elastic net**:
\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\gamma_1 \vert \boldsymbol\beta\vert -\gamma_2 \Vert \boldsymbol\beta\Vert_2^2.
\]

The elastic gives a sparse solution with potentially more than $\min(n,p)$ predictors.

---

The `glmnet` R function uses the following parameterisation,
\[
  \hat{\boldsymbol{\beta}} = \text{ArgMax}_\beta l(\boldsymbol{\beta}) -\lambda\alpha \Vert \boldsymbol\beta\Vert_0 -\lambda(1-\alpha) \Vert \boldsymbol\beta\Vert_2^2.
\]

- $\alpha$ parameter gives weight to $L_1$ penalty term (hence $\alpha=1$ gives the lasso, and $\alpha=0$ gives ridge).

- a $\lambda$ parameter to give weight to the penalisation

- Note that the combination of $\lambda$ and $\alpha$ gives the same flexibility as the combination of the parameters $\lambda_1$ and $\lambda_2$.

---

### Breast cancer example

```{r}
mElastic <- glmnet(
  x = XTrain,
  y = YTrain,
  alpha = 0.5,
  family="binomial")  # elastic net

plot(mElastic, xvar = "lambda",xlim=c(-5.5,-1))
```

```{r}
mCvElastic <- cv.glmnet(x = XTrain,
  y = YTrain,
  alpha = 0.5,
  family = "binomial",
	type.measure = "class")  # elastic net

plot(mCvElastic)
mCvElastic
```

```{r}
dfElast <- data.frame(
  pi = predict(mElastic,
    newx = XTest,
    s = mCvElastic$lambda.min,
    type = "response") %>% c(.),
  known.truth = YTest)

roc <- rbind(
  dfLassoOpt %>% mutate(method = "lasso"),
  dfElast %>% mutate(method = "elast. net")) %>%
  ggplot(aes(d = known.truth, m = pi, color = method)) +
  geom_roc(n.cuts = 0) +
  xlab("1-specificity (FPR)") +
  ylab("sensitivity (TPR)")

roc

calc_auc(roc)
```

- More parameters are used than for the lasso, but the performance does not improve.

```{r}
mElasticOpt <- glmnet(x = XTrain,
  y = YTrain,
  alpha = 0.5,
  lambda = mCvElastic$lambda.min,
  family="binomial")

qplot(
  summary(coef(mElasticOpt))[-1,1],
  summary(coef(mElasticOpt))[-1,3]) +
  xlab("gene ID") +
  ylab("beta-hat") +
  geom_hline(yintercept = 0, color = "red")
```
</div>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeSourceEmbed("prediction.Rmd");
});
</script>

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
